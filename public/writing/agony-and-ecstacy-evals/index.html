<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GPT-5 is Good, Actually: The Agony and Ecstasy of Public Benchmarks | Shane Caldwell</title>
<meta name=keywords content="llms,evals"><meta name=description content="An attempt to explain why benchmarks are either bad or secret, and why the barcharts don't matter so much."><meta name=author content="Shane Caldwell"><link rel=canonical href=http://localhost:1313/writing/agony-and-ecstacy-evals/><link crossorigin=anonymous href=/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/writing/agony-and-ecstacy-evals/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script data-goatcounter=https://sjcaldwell.goatcounter.com/count async src=//gc.zgo.at/count.js></script><style>:root{--font-mono:'SF Mono', 'Monaco', 'Inconsolata', 'Roboto Mono', 'Source Code Pro', 'Menlo', 'Consolas', monospace}body,html{font-family:var(--font-mono)!important}*,*::before,*::after{font-family:var(--font-mono)!important}.dark{--primary:#ffffff;--secondary:#e5e5e5;--tertiary:#cccccc}body:not(.dark){--primary:#000000;--secondary:#333333;--tertiary:#666666}.dark a{color:#fff!important;text-decoration:underline}.dark a:hover{color:#e5e5e5!important}code,pre{font-family:var(--font-mono)!important;font-size:.9em}h1,h2,h3,h4,h5,h6{font-weight:700!important;color:var(--primary)!important}.post-meta{color:var(--secondary)!important}button,.button{font-family:var(--font-mono)!important;font-weight:500}.paper-card,.talk-card{background:var(--theme);border:1px solid var(--border);border-radius:8px;padding:24px;margin-bottom:24px;transition:all .2s ease;box-shadow:0 2px 4px rgba(0,0,0,5%)}.paper-card:hover,.talk-card:hover{border-color:var(--secondary);box-shadow:0 4px 8px rgba(0,0,0,.1);transform:translateY(-1px)}.dark .paper-card,.dark .talk-card{background:#1a1a1a;border-color:#333;box-shadow:0 2px 4px rgba(0,0,0,.2)}.dark .paper-card:hover,.dark .talk-card:hover{border-color:#555;box-shadow:0 4px 8px rgba(0,0,0,.3)}.paper-title{margin:0 0 12px!important;font-size:1.25em;line-height:1.3}.paper-title a{color:var(--primary)!important;text-decoration:none;border-bottom:2px solid transparent;transition:border-color .2s ease}.paper-title a:hover{border-bottom-color:var(--primary)}.paper-meta{margin-bottom:16px;font-size:.9em}.paper-authors{color:var(--secondary);margin-bottom:4px;font-weight:500}.paper-date{color:var(--tertiary);font-size:.85em}.paper-abstract{color:var(--primary);line-height:1.5}.paper-abstract p{margin:0}.talk-title{margin:0 0 12px!important;font-size:1.25em;line-height:1.3}.talk-collaborators{margin-bottom:16px;font-size:.9em;color:var(--secondary)}.talk-collaborators p{margin:0}.talk-details{display:flex;flex-direction:column;gap:8px}.talk-event,.talk-recording{font-size:.9em}.talk-recording a{color:var(--primary)!important;text-decoration:underline}.talk-recording a:hover{color:var(--secondary)!important}@media(max-width:768px){.paper-card,.talk-card{padding:16px;margin-bottom:16px}}.twitter-tweet{margin:24px auto!important;max-width:550px!important}.post-content blockquote.twitter-tweet,.post-content div:has(.twitter-tweet){display:flex;justify-content:center;margin:24px 0}.post-content .twitter-tweet iframe{margin:0 auto;display:block}</style><meta property="og:url" content="http://localhost:1313/writing/agony-and-ecstacy-evals/"><meta property="og:site_name" content="Shane Caldwell"><meta property="og:title" content="GPT-5 is Good, Actually: The Agony and Ecstasy of Public Benchmarks"><meta property="og:description" content="An attempt to explain why benchmarks are either bad or secret, and why the barcharts don't matter so much."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="writing"><meta property="article:published_time" content="2025-08-17T00:00:00+00:00"><meta property="article:modified_time" content="2025-08-17T00:00:00+00:00"><meta property="article:tag" content="Llms"><meta property="article:tag" content="Evals"><meta property="og:image" content="http://localhost:1313/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/"><meta name=twitter:title content="GPT-5 is Good, Actually: The Agony and Ecstasy of Public Benchmarks"><meta name=twitter:description content="An attempt to explain why benchmarks are either bad or secret, and why the barcharts don't matter so much."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Writing","item":"http://localhost:1313/writing/"},{"@type":"ListItem","position":2,"name":"GPT-5 is Good, Actually: The Agony and Ecstasy of Public Benchmarks","item":"http://localhost:1313/writing/agony-and-ecstacy-evals/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GPT-5 is Good, Actually: The Agony and Ecstasy of Public Benchmarks","name":"GPT-5 is Good, Actually: The Agony and Ecstasy of Public Benchmarks","description":"An attempt to explain why benchmarks are either bad or secret, and why the barcharts don't matter so much.","keywords":["llms","evals"],"articleBody":"My first reaction to GPT-5 was positive. I was at Blackhat/Defcon the week of the release, and was mostly (for once) not on Twitter. Being off Twitter, I missed the much maligned livestream. In fact, before I even got on Twitter and saw everyone absolutely clowning that bar chart, the first things I heard were positive.\nMy younger brother works as an accountant. Living mostly in Excel and manually reconciling data between a lot of systems, he doesn’t use the models all that much. In our group chat, he posted a screenshot of his first GPT-5 interaction. It was an accounting question about how land depreciates (it doesn’t). GPT-5 was the first model that got his question correct. He said, basically, “Maybe they finally got me1.” Some other friends who work in data science and infrastructure also basically complimented the model. Some of them pay for the highest tier of GPT access, others pay for the lower subscriptions. Nobody was totally blown away, but the general reaction was “impressed in the way I expected to be impressed”.\nThen, I logged on to Twitter.\npic.twitter.com/OZg7qNyf5k\n— xjdr (@_xjdr) August 7, 2025 There was a lot of deserved haranging about the chart crimes. It’s a bad chart. It’s a chart so bad it’s difficult to imagine in a high school science fair, much less the much hyped release of the most significant lab on the planet. The implication was that the chart needed to be bad because it wouldn’t look good if you displayed it accurately. GPT-5 without thinking is worse than o3, and the total gains of 5 with thinking are a measly 6%. AGI is cancelled, everyone pushed their timelines back, RL has no more gains to give, etc2.\nSo, who’s right? My brother, with his one question vibe check or 80% of Twitter, with their ability to competently read benchmark bar charts and from bar charts tea leaves?\nI’m going with my brother on this one. GPT-5 is a great model. If o3 had never been released, people would be losing their minds. But it does exist, and so the result is iterative. But being iterative doesn’t prevent it from being very good. And as the week went on, some smart people I respect found the model generally more capable than its predecesors/competition.\nBTW, this is one of the most exciting bits to me- it’s not just better in benchmarks, it’s way better at hacking real, live targets https://t.co/VCM2VYXX5B pic.twitter.com/MALagN8oct\n— Brendan Dolan-Gavitt (@moyix) August 15, 2025 Tested the NSA code over night and after a few tweaks it trains. Wow, GPT5 and Opus4.1 wrote a 100% AI generated (human art directed) NSA implementation. I would not have guessed that was possible\n— xjdr (@_xjdr) August 12, 2025 I’ve found generally the same things. GPT-5 has been super useful as a research assistant in the last week. Its ability to find relevant paper results for lit review has increased dramatically, as well as its ability to proofread paper drafts and work through technical specs. I haven’t used it for code, yet, but I’m so happy with Opus I just haven’t bothered. I’m confident it’s quite good at that, too.\nSo if it’s so much better at code re: xjdr’s tweet, how come that doesn’t show up in the SWE Bench results? That’s easy, public benchmarks are basically awful and you’re better off ignoring them.\nPublic Benchmarks are Terrible I’m going to say a lot of harsh things, but before I do, I have to acknowledge:\nEvals are incredibly difficult to make. They’re getting more difficult every year. The people who manage to do it are undersung heroes and nothing I write here is to criticize them. If you want to understand why it’s so difficult, there are two salient points to understand:\nAs soon as you make a benchmark public, it is going to get totally saturated if anybody cares about it, and then it might as well not exist.\nThe models are so capable that creating an evaluation capable of distinguishing between the most capable models is expensive and painful.\nBefore we expand on those, let’s just briefly talk about the good old days with training sets and test sets.\nThe Good Old Days The ideal benchmark dataset is difficult enough that substantive progress on it requires serious breakthroughs. ImageNet, for example, was a large and broad enough dataset such that doing classification well required the creation of convolutional neural nets. When researchers refer to the ImageNet Moment they’re referring to the 2012 rendition of the ImageNet classification challenge where AlexNet won the competition with over a 10% lead to all of its competitors, and would spawn 80,000 citations and a whole slew of technical innovation in the years to follow. ImageNet itself was created in 2009. That’s four years! SWE-bench Verified came out last year and it’s cooked.\nThe rules were also very clear. Everybody had the same training data. The test set for everyone was the same. If you trained on test this was immediately clear from trying to replicate your results, and if you did that you would be sent to the gulag. You could look at both sets and have a sense of what generalization was required to perform the task, and when a method “worked” it was obvious to everybody. That’s no longer the case.\nNow the training set is R E D A C T E D. We have no idea what frontier labs are training on, but it’s as much as they can get, then as much as they can generate, and then as many worthwhile environments as they can get verifiable rewards from3. There’s pretraining, mid-training, post-training, with different teams working on different parts of the training. Let’s take a look at everything the GPT-5 model card has to say about the data and training.\nThat’s nothing! You’re actually better off hanging around their careers page to try and get a sense of what capabilities they’re trying to bring to the team (and models). And OpenAI is in no way special in this, that’s just how the labs are these days. Every piece of information is a freebie to a competitor and they’ve got enough to worry about with the way information flows around SF. Beyond that, every written admission of how anything was trained invites a potential legal challenge. It just doesn’t make sense to say anything. If you want a sense of what data is being used to train a model, you can stick to Allen and Nous, but even the leaders of those labs would agree that they’re far more resource bound than their frontier competitors and their models lag accordingly.\nSo the training set is ???, the test sets are these public benchmarks/evals, and the test-time distribution we’d like these models to cover is literally anything you might want a computer to do.\nWith that established, let’s cover those two points from earlier:\nPublic Benchmarks Will Always Be Saturated The preprint of SWE-Bench was released in October of 2023. The creators took 2,294 public issues from 12 popular Python repos. These include astropy, seaborn, pytest, flask, sphinx, requests, pytest, and others. These issues and models performance on them have essentially become the single scalar of how models are perceived at performing on software engineering.\nThis is an ingenous idea for a benchmark. You’ve got all this code data out there, and the creators had an intuition that writing one-off functions to get specific test cases to pass was missing some of the complexity of real software engineering and that these public Github issues of mature projects presented a really useful measurement of progress. They set up a harness to test models and report that the best model earns a 4.8% on their benchmark. That seems really great, and like it’s going to be useful to watch models slowly improve at it, and as they improve on these benchmarks we’ll see gradually better coding capabilities in the models.\nBut that’s not really what happened. By publishing this benchmark and it becoming the defacto measurement of model quality for what is currently the most economically valuable task LLMs can work on, it became the battleground for frontier labs to fight it out over4.\nThe ImageNet of it all falls apart almost immediately due to the incentive structures at play. Training a model is super expensive, nobody gets to see your training data, and most people who aren’t using these models at a high level are going to judge you mostly on this score. Even if the models were trained exclusively by saints, it’s not hard to figure out what’s going to happen. You can be damn sure that as they’re training these models they’re taking a look at the SWE-Bench leaderboards and figuring out if there’s a narrative where they’re a helluva lot better, or very competitive for the model size, or whatever it has to be, but there has to be a narrative that looks good or that model isn’t going out the door.\nTrain on more code? Sure. Set up RL environments that are shockingly similar to the benchmark but using different repositories? Literally why wouldn’t you? Your competitors are. The delta between evals and RL environments all comes down to whether you’re willing to write a reward function and update some weights. Schemes to generate synthetic data that is intentionally close to the test set but isn’t (legally) the test set? Please do.\nThe fear of training on the test set previously was that your model would memorize it all and then totally fail to generalize at all to the real world. Now that’s not nearly so much of a concern, you can do whatever black magic you need to in order to get the numbers where they need to be, and that’s just another item on your to-do list as you prepare for a major model release. That doesn’t mean you’re making a bad model - I’ve personally seen the capabilities of the models continue to increase at a steady rate that continues to blow my mind. It’s just that also you make sure you count the letters in strawberry correctly because you know that’s something people are looking for and you’re tasked with brand building at the same time you’re tasked with creating the most useful model possible.\nThen, having gotten the model as good as it’s going to get, it’s time to dress up those results. Need to mess with pass@k for its bar on the chart to be taller than the other guy? Fine. Need to beat a Y-axis to death with your bare hands such that it violates Euclidean geometry? Cost of doing business. Nothing about it is really surprising. You’ve all worked at places where somebody made a slide deck about your work that hyped it up more than is deserved, and if you’ve lived long enough you’ve come to accept that that’s just one of the weird perverse incentives of business. Epistemically fraught, a bit, but if everybody’s in on the game it’s not shocking or anything. It’s just what it is.\nSo as a researcher without access to a frontier labs compute, the most useful way you can steer the lever of progress is by developing large, easy-to-run benchmarks that models are currently kind of bad at for tasks you care about. This is an incredible amount of work in itself. Backbreaking amounts of quality control, one-offs to fix, and mental labor expended. If you then do the work of getting that benchmark popular and well-cited enough, it goes into the crosshairs of the labs. If your benchmark comes to matter enough to be referenced in the model card, it’s going to get saturated5, because these labs have to one up each other every time a release comes out, so you are nearly guaranteeing that those capabilities are going to increase, but also that the benchmark isn’t going to matter much anymore. Or at least, the climbing of the benchmark numbers are not going to be as aligned with the capability increases you see in real life as you hoped there would be when you made the benchmark.\nI mean, Jesus, even playing Pokemon got saturated.\nGPT-5 just finished Pokémon Red! 6,470 steps vs. 18,184 for o3! Check the stats site to compare!\nThat's a huge improvement! Well done, @OpenAI you cooked with GPT-5. What an incredible model.\nNext up: GPT-5 vs. Pokémon Crystal (16 Badges + Red). The run starts soon on Twitch. https://t.co/fV8kcAEijr pic.twitter.com/V4GbhRxtj0\n— Clad3815 (@Clad3815) August 14, 2025 So, what do you do? You accept the Dark Forest situation for evals and work from there. You keep secret benchmarks that aren’t available to frontier labs and in that way you have your own private signal of model capability increases. The downsides of this are it’s still really hard. Benchmarks are not easy to build. Creating a set of reproducible, diverse tasks that are complex enough to be worth keeping track of is just an inherently difficult thing to do. But if you get it, it’s my little brother’s accounting question on steroids. Crucially, this makes no sense if you’re a researcher. Releasing a really strong benchmark is a ticket to fame, fortune, and maybe some of that compute you currently don’t have any of. So who does this make sense for? Businesses, governments, the types of organizations where people would find it worth investing in understanding capabilities and then keeping that knowledge to themselves.\nWhat are the epistemic downsides? Well, let’s see what happens when you tell somebody about your definitely very real and intentionally secret benchmark.\nWhat a hot take based on some unverifiable “internal benchmark set”…but it totally fits into their general overhyping marketing strategy that (intentionally) mixes things up. 🤷‍♂️ https://t.co/v1U2dS7omk\n— Julien | MrTuxracer 🇪🇺 (@MrTuxracer) August 16, 2025 I get it, Mr. Tux, I really do. But if they let you verify those benchmarks (made them public and verifiable), they would lose all meaning almost immediately. How do you know how to update your beliefs based on a company’s report of a benchmark if you can’t verify it? Well, depends on your belief of the integrity of the company. So we arrive at the just trust me bro era of AI research. Blessed be the Twitter hypebeasts who show off their cool examples on Twitter, because if not for them you’d have no signals at all. This is why people who use LLMs in some vertical release cool demos and try to put out public examples of their work. They have to find some way to send you and other potential customers positive signal that can combat your basic skepticism over claimed capabilities without just releasing their benchmarks and making the entire exercise pointless.\nThe Models Are So Capable They’re Hard to Evaluate Evals are hard! They were hard “back in the day” and they’re harder now. MMLU seems like a relative cakewalk from an infrastructure perspective. If you can put out your whole benchmark on HuggingFace and it all works by downloading a dataset and running it you have it as easy as possible. The quality control required to make several thousand test cases all correct is still extremely painful and labor intensive, but at least it’s easy to run.\nBut we don’t care about question answering now. Or translation. We care about stuff like computer-use. Now that we’re evaluating agents, each of these tasks needs realistic and rich environments. Someone has make that! That’s a lot of engineering, expensive infrastructure, and domain expertise to make sure you’re not fooling yourself. When orchestrating 500 docker containers is the clean case, you know it’s going to be painful.\nAs these setups are required to get more painful in order to accurately measure the capabilities, they’re also just more expensive to run. The infrastructure needs spinning up, the token use to get to an action turn count such that you can prove your environment is sufficiently realistic and the task is sufficiently difficult is huge. Trust me, pal, you wouldn’t run those evals if they were publicly verifiable. You don’t have the stamina or the checkbook.\nThat in and of itself is one of the largest markers of progress to me. It is legitimately an intellectual exercise and engineering undertaking to get a truly useful set of scenarios where the models actually screw up. That was not the case in 2023. A lot of smart people are spending a lot of time trying to get to an empirical measurement they can trust for their particular domain. And that ability to measure capability in and of itself now becomes intellectual property, and it’s pretty likely those who invest the effort are going to keep it to themselves.\nWhat This Means For You So do I have any actual recommendations here? Sure, build your own benchmarks. If you’re an organization, this is basically a must. It’s hard and requires a lot of effort but if you’ve got a business case around models reaching a certain capability level, it’s basically table stakes to be able to measure those in a mature and repeatable format. Nobody wants to write evals, nobody wants to run evals, but if you’re not participating you’re left looking at benchmark screenshots. This is, essentially, irresponsible and ensures that when the capabilities get to that point you were waiting for you’ll find out about them via tweet.\nIf you’re an individual? Well, the least you can do is get your private test set together. This could be questions, this could be engineering requests or code you’d like to see, it could be a harness you expect to be able to accomplish some challenge agentically when the models get good enough. You don’t have to tell anybody about it, but you should have them. They’ll tell you more than the bar charts of those publicly available evals you’ve never examined. And you’ll be able to comfortably skip the livestream and decide for yourself if GPT-6 is any good.\nHis language was a bit more severe and quite a bit funnier, but it’s bad form to directly quote Signal GCs. ↩︎\nIn fairness this probably also has a lot to do with the model routing, which was apparently broken on day one. ↩︎\nAnd then whatever the universal verifier (judge) tells them is good, and so on, and so on. ↩︎\nAnd usurped MMLU as the bar chart people look at before they tweet whether the model is good or not. ↩︎\nThe only area where this is spiky in my personal experiences is cybersecurity evals, where the incentives seem to shift to desiring to look non-threatening and not worth legislating. Sometimes I look at results on stuff I run and the output of frontier labs and assume they’re tying the model’s hands behind their back and leaving them a python 2 interpreter, bash, and some duct-tape so they can report the models are still only kind-of-okay at CTFs. Trust me, they’re really quite remarkable. ↩︎\n","wordCount":"3170","inLanguage":"en","image":"http://localhost:1313/","datePublished":"2025-08-17T00:00:00Z","dateModified":"2025-08-17T00:00:00Z","author":{"@type":"Person","name":"Shane Caldwell"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/writing/agony-and-ecstacy-evals/"},"publisher":{"@type":"Organization","name":"Shane Caldwell","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Shane Caldwell (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Shane Caldwell</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Main><span>Main</span></a></li><li><a href=http://localhost:1313/papers/ title=Papers><span>Papers</span></a></li><li><a href=http://localhost:1313/talks/ title=Talks><span>Talks</span></a></li><li><a href=http://localhost:1313/writing/ title=Writing><span>Writing</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">GPT-5 is Good, Actually: The Agony and Ecstasy of Public Benchmarks</h1><div class=post-description>An attempt to explain why benchmarks are either bad or secret, and why the barcharts don't matter so much.</div><div class=post-meta><span title='2025-08-17 00:00:00 +0000 UTC'>August 17, 2025</span>&nbsp;·&nbsp;15 min&nbsp;·&nbsp;3170 words&nbsp;·&nbsp;Shane Caldwell</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#public-benchmarks-are-terrible>Public Benchmarks are Terrible</a><ul><li><a href=#the-good-old-days>The Good Old Days</a></li><li><a href=#public-benchmarks-will-always-be-saturated>Public Benchmarks Will Always Be Saturated</a></li><li><a href=#the-models-are-so-capable-theyre-hard-to-evaluate>The Models Are So Capable They&rsquo;re Hard to Evaluate</a></li><li><a href=#what-this-means-for-you>What This Means For You</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>My first reaction to GPT-5 was positive. I was at Blackhat/Defcon the week of the release, and was mostly (for once) not on Twitter. Being off Twitter, I missed the much maligned livestream. In fact, before I even got on Twitter and saw everyone absolutely clowning <em>that</em> bar chart, the first things I heard were positive.</p><p>My younger brother works as an accountant. Living mostly in Excel and manually reconciling data between a lot of systems, he doesn&rsquo;t use the models all that much. In our group chat, he posted a screenshot of his first GPT-5 interaction. It was an accounting question about how land depreciates (it doesn&rsquo;t). GPT-5 was the first model that got his question correct. He said, basically, &ldquo;Maybe they finally got me<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.&rdquo; Some other friends who work in data science and infrastructure also basically complimented the model. Some of them pay for the highest tier of GPT access, others pay for the lower subscriptions. Nobody was totally blown away, but the general reaction was &ldquo;impressed in the way I expected to be impressed&rdquo;.</p><p>Then, I logged on to Twitter.</p><blockquote class=twitter-tweet><p lang=zxx dir=ltr><a href=https://t.co/OZg7qNyf5k>pic.twitter.com/OZg7qNyf5k</a></p>&mdash; xjdr (@_xjdr) <a href="https://twitter.com/_xjdr/status/1953504619969306981?ref_src=twsrc%5Etfw">August 7, 2025</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><p>There was a lot of deserved haranging about the chart crimes. It&rsquo;s a bad chart. It&rsquo;s a chart so bad it&rsquo;s difficult to imagine in a high school science fair, much less the much hyped release of the most significant lab on the planet. The implication was that the chart needed to be bad because it wouldn&rsquo;t look good if you displayed it accurately. GPT-5 without thinking is worse than o3, and the total gains of 5 with thinking are a measly 6%. AGI is cancelled, everyone pushed their timelines back, RL has no more gains to give, etc<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</p><p>So, who&rsquo;s right? My brother, with his one question vibe check or 80% of Twitter, with their ability to competently read benchmark bar charts and from bar charts tea leaves?</p><p>I&rsquo;m going with my brother on this one. GPT-5 is a great model. If o3 had never been released, people would be losing their minds. But it does exist, and so the result is iterative. But being iterative doesn&rsquo;t prevent it from being <em>very good</em>. And as the week went on, some smart people I respect found the model generally more capable than its predecesors/competition.</p><blockquote class=twitter-tweet><p lang=en dir=ltr>BTW, this is one of the most exciting bits to me- it’s not just better in benchmarks, it’s way better at hacking real, live targets <a href=https://t.co/VCM2VYXX5B>https://t.co/VCM2VYXX5B</a> <a href=https://t.co/MALagN8oct>pic.twitter.com/MALagN8oct</a></p>&mdash; Brendan Dolan-Gavitt (@moyix) <a href="https://twitter.com/moyix/status/1956423766768247032?ref_src=twsrc%5Etfw">August 15, 2025</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><blockquote class=twitter-tweet><p lang=en dir=ltr>Tested the NSA code over night and after a few tweaks it trains. Wow, GPT5 and Opus4.1 wrote a 100% AI generated (human art directed) NSA implementation. I would not have guessed that was possible</p>&mdash; xjdr (@_xjdr) <a href="https://twitter.com/_xjdr/status/1955286758700056927?ref_src=twsrc%5Etfw">August 12, 2025</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><p>I&rsquo;ve found generally the same things. GPT-5 has been super useful as a research assistant in the last week. Its ability to find relevant paper results for lit review has increased dramatically, as well as its ability to proofread paper drafts and work through technical specs. I haven&rsquo;t used it for code, yet, but I&rsquo;m so happy with Opus I just haven&rsquo;t bothered. I&rsquo;m confident it&rsquo;s quite good at that, too.</p><p>So if it&rsquo;s so much better at code re: xjdr&rsquo;s tweet, how come that doesn&rsquo;t show up in the SWE Bench results? That&rsquo;s easy, public benchmarks are basically awful and you&rsquo;re better off ignoring them.</p><h2 id=public-benchmarks-are-terrible>Public Benchmarks are Terrible<a hidden class=anchor aria-hidden=true href=#public-benchmarks-are-terrible>#</a></h2><p>I&rsquo;m going to say a lot of harsh things, but before I do, I have to acknowledge:</p><p><img alt="Sad but true" loading=lazy src=/writing/agony-and-ecstacy-evals/evals_suck.jpeg></p><p>Evals are incredibly difficult to make. They&rsquo;re getting more difficult every year. The people who manage to do it are undersung heroes and nothing I write here is to criticize them. If you want to understand why it&rsquo;s so difficult, there are two salient points to understand:</p><ol><li><p>As soon as you make a benchmark public, it is going to get totally saturated if anybody cares about it, and then it might as well not exist.</p></li><li><p>The models are so capable that creating an evaluation capable of distinguishing between the most capable models is expensive and painful.</p></li></ol><p>Before we expand on those, let&rsquo;s just briefly talk about the good old days with training sets and test sets.</p><h3 id=the-good-old-days>The Good Old Days<a hidden class=anchor aria-hidden=true href=#the-good-old-days>#</a></h3><p>The ideal benchmark dataset is difficult enough that substantive progress on it requires serious breakthroughs. ImageNet, for example, was a large and broad enough dataset such that doing classification well required the creation of convolutional neural nets. When researchers refer to the <a href=https://image-net.org/challenges/LSVRC/2012/>ImageNet Moment</a> they&rsquo;re referring to the 2012 rendition of the ImageNet classification challenge where <a href=https://en.wikipedia.org/wiki/AlexNet>AlexNet</a> won the competition with over a 10% lead to all of its competitors, and would spawn 80,000 citations and a whole slew of technical innovation in the years to follow. ImageNet itself was created in 2009. That&rsquo;s four years! SWE-bench Verified came out <a href=https://openai.com/index/introducing-swe-bench-verified/>last year</a> and it&rsquo;s cooked.</p><p>The rules were also very clear. Everybody had the same training data. The test set for everyone was the same. If you trained on test this was immediately clear from trying to replicate your results, and if you did that you would be sent to the gulag. You could look at both sets and have a sense of what generalization was required to perform the task, and when a method &ldquo;worked&rdquo; it was obvious to everybody. That&rsquo;s no longer the case.</p><p>Now the training set is R E D A C T E D. We have no idea what frontier labs are training on, but it&rsquo;s as much as they can get, then as much as they can generate, and then as many worthwhile environments as they can get verifiable rewards from<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>. There&rsquo;s pretraining, mid-training, post-training, with different teams working on different parts of the training. Let&rsquo;s take a look at everything the <a href=https://cdn.openai.com/pdf/8124a3ce-ab78-4f06-96eb-49ea29ffb52f/gpt5-system-card-aug7.pdf>GPT-5 model card</a> has to say about the data and training.</p><p><img alt="Thank god it all fits in a screenshot" loading=lazy src=/writing/agony-and-ecstacy-evals/gpt5_model_training.png></p><p>That&rsquo;s nothing! You&rsquo;re actually better off hanging around their <a href=https://openai.com/careers/>careers page</a> to try and get a sense of what capabilities they&rsquo;re trying to bring to the team (and models). And OpenAI is in no way special in this, that&rsquo;s just how the labs are these days. Every piece of information is a freebie to a competitor and they&rsquo;ve got enough to worry about with the way information flows around SF. Beyond that, every written admission of how anything was trained invites a potential legal challenge. It just doesn&rsquo;t make sense to say anything. If you want a sense of what data is being used to train a model, you can stick to <a href=https://allenai.org/>Allen</a> and <a href=https://nousresearch.com/>Nous</a>, but even the leaders of those labs would agree that they&rsquo;re far more resource bound than their frontier competitors and their models lag accordingly.</p><p>So the training set is ???, the test sets are these public benchmarks/evals, and the test-time distribution we&rsquo;d like these models to cover is <em>literally anything you might want a computer to do</em>.</p><p>With that established, let&rsquo;s cover those two points from earlier:</p><h3 id=public-benchmarks-will-always-be-saturated>Public Benchmarks Will Always Be Saturated<a hidden class=anchor aria-hidden=true href=#public-benchmarks-will-always-be-saturated>#</a></h3><p>The preprint of <a href=https://arxiv.org/pdf/2310.06770v1>SWE-Bench</a> was released in October of 2023. The creators took 2,294 public issues from 12 popular Python repos. These include astropy, seaborn, pytest, flask, sphinx, requests, pytest, and others. These issues and models performance on them have essentially become the single scalar of how models are perceived at performing on software engineering.</p><p>This is an ingenous idea for a benchmark. You&rsquo;ve got all this code data out there, and the creators had an intuition that writing one-off functions to get specific test cases to pass was missing some of the complexity of real software engineering and that these public Github issues of mature projects presented a really useful measurement of progress. They set up a harness to test models and report that the <em>best</em> model earns a 4.8% on their benchmark. That seems really great, and like it&rsquo;s going to be useful to watch models slowly improve at it, and as they improve on these benchmarks we&rsquo;ll see gradually better coding capabilities in the models.</p><p>But that&rsquo;s not really what happened. By publishing this benchmark and it becoming the defacto measurement of model quality for what is currently the most economically valuable task LLMs can work on, it became the battleground for frontier labs to fight it out over<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>.</p><p>The ImageNet of it all falls apart almost immediately due to the incentive structures at play. Training a model is super expensive, nobody gets to see your training data, and most people who aren&rsquo;t using these models at a high level are going to judge you mostly on this score. Even if the models were trained exclusively by saints, it&rsquo;s not hard to figure out what&rsquo;s going to happen. You can be damn sure that as they&rsquo;re training these models they&rsquo;re taking a look at the SWE-Bench leaderboards and figuring out if there&rsquo;s a narrative where they&rsquo;re a helluva lot better, or very competitive for the model size, or <em>whatever it has to be</em>, but there has to be a narrative that looks good or that model isn&rsquo;t going out the door.</p><p>Train on more code? Sure. Set up RL environments that are shockingly similar to the benchmark but using different repositories? Literally why wouldn&rsquo;t you? Your competitors are. The delta between evals and RL environments all comes down to whether you&rsquo;re willing to write a reward function and update some weights. Schemes to generate synthetic data that is intentionally close to the test set but <em>isn&rsquo;t</em> (legally) the test set? Please do.</p><p>The fear of training on the test set previously was that your model would memorize it all and then totally fail to generalize at all to the real world. Now that&rsquo;s not nearly so much of a concern, you can do whatever black magic you need to in order to get the numbers where they need to be, and that&rsquo;s just another item on your to-do list as you prepare for a major model release. That doesn&rsquo;t mean you&rsquo;re making a bad model - I&rsquo;ve personally seen the capabilities of the models continue to increase at a steady rate that continues to blow my mind. It&rsquo;s just that also you make sure you count the letters in strawberry correctly because you know that&rsquo;s something people are looking for and you&rsquo;re tasked with brand building at the same time you&rsquo;re tasked with creating the most useful model possible.</p><p>Then, having gotten the model as good as it&rsquo;s going to get, it&rsquo;s time to dress up those results. Need to mess with pass@k for its bar on the chart to be taller than the other guy? Fine. Need to beat a Y-axis to death with your bare hands such that it violates Euclidean geometry? Cost of doing business. Nothing about it is really surprising. You&rsquo;ve all worked at places where somebody made a slide deck about your work that hyped it up more than is deserved, and if you&rsquo;ve lived long enough you&rsquo;ve come to accept that that&rsquo;s just one of the weird perverse incentives of business. Epistemically fraught, a bit, but if everybody&rsquo;s in on the game it&rsquo;s not shocking or anything. It&rsquo;s just what it is.</p><p>So as a researcher without access to a frontier labs compute, the most useful way you can steer the lever of progress is by developing large, easy-to-run benchmarks that models are currently kind of bad at for tasks you care about. This is an incredible amount of work in itself. Backbreaking amounts of quality control, one-offs to fix, and mental labor expended. If you then do the work of getting that benchmark popular and well-cited enough, it goes into the crosshairs of the labs. If your benchmark comes to matter enough to be referenced in the model card, it&rsquo;s going to get saturated<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>, because these labs have to one up each other every time a release comes out, so you are nearly guaranteeing that those capabilities are going to increase, but also that the benchmark isn&rsquo;t going to matter much anymore. Or at least, the climbing of the benchmark numbers are not going to be as aligned with the capability increases you see in real life as you hoped there would be when you made the benchmark.</p><p>I mean, Jesus, even playing Pokemon got saturated.</p><blockquote class=twitter-tweet><p lang=en dir=ltr>GPT-5 just finished Pokémon Red! 6,470 steps vs. 18,184 for o3! Check the stats site to compare!<br><br>That's a huge improvement! Well done, <a href="https://twitter.com/OpenAI?ref_src=twsrc%5Etfw">@OpenAI</a> you cooked with GPT-5. What an incredible model.<br><br>Next up: GPT-5 vs. Pokémon Crystal (16 Badges + Red). The run starts soon on Twitch. <a href=https://t.co/fV8kcAEijr>https://t.co/fV8kcAEijr</a> <a href=https://t.co/V4GbhRxtj0>pic.twitter.com/V4GbhRxtj0</a></p>&mdash; Clad3815 (@Clad3815) <a href="https://twitter.com/Clad3815/status/1955980772575268897?ref_src=twsrc%5Etfw">August 14, 2025</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><p>So, what do you do? You accept the Dark Forest situation for evals and work from there. You keep secret benchmarks that aren&rsquo;t available to frontier labs and in that way you have your own private signal of model capability increases. The downsides of this are it&rsquo;s still really hard. Benchmarks are <em>not</em> easy to build. Creating a set of reproducible, diverse tasks that are complex enough to be worth keeping track of is just an inherently difficult thing to do. But if you get it, it&rsquo;s my little brother&rsquo;s accounting question on steroids. Crucially, this makes <em>no</em> sense if you&rsquo;re a researcher. Releasing a really strong benchmark is a ticket to fame, fortune, and maybe some of that compute you currently don&rsquo;t have any of. So who does this make sense for? Businesses, governments, the types of organizations where people would find it worth investing in understanding capabilities and then keeping that knowledge to themselves.</p><p>What are the epistemic downsides? Well, let&rsquo;s see what happens when you tell somebody about your definitely very real and intentionally secret benchmark.</p><blockquote class=twitter-tweet><p lang=en dir=ltr>What a hot take based on some unverifiable “internal benchmark set”…but it totally fits into their general overhyping marketing strategy that (intentionally) mixes things up. 🤷‍♂️ <a href=https://t.co/v1U2dS7omk>https://t.co/v1U2dS7omk</a></p>&mdash; Julien | MrTuxracer 🇪🇺 (@MrTuxracer) <a href="https://twitter.com/MrTuxracer/status/1956686066708070816?ref_src=twsrc%5Etfw">August 16, 2025</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><p>I get it, Mr. Tux, I really do. But if they let you verify those benchmarks (made them public and verifiable), they would lose all meaning almost immediately. How do you know how to update your beliefs based on a company&rsquo;s report of a benchmark if you can&rsquo;t verify it? Well, depends on your belief of the integrity of the company. So we arrive at the <em>just trust me bro</em> era of AI research. Blessed be the Twitter hypebeasts who show off their cool examples on Twitter, because if not for them you&rsquo;d have no signals at all. This is why people who use LLMs in some vertical release cool demos and try to put out public examples of their work. They have to find some way to send you and other potential customers positive signal that can combat your basic skepticism over claimed capabilities without just releasing their benchmarks and making the entire exercise pointless.</p><h3 id=the-models-are-so-capable-theyre-hard-to-evaluate>The Models Are So Capable They&rsquo;re Hard to Evaluate<a hidden class=anchor aria-hidden=true href=#the-models-are-so-capable-theyre-hard-to-evaluate>#</a></h3><p>Evals are hard! They were hard &ldquo;back in the day&rdquo; and they&rsquo;re harder now. MMLU seems like a relative cakewalk from an infrastructure perspective. If you can put out your <em>whole</em> benchmark on HuggingFace and it all works by downloading a dataset and running it you have it as easy as possible. The quality control required to make several thousand test cases all correct is still extremely painful and labor intensive, but at least it&rsquo;s easy to run.</p><p>But we don&rsquo;t <em>care</em> about question answering now. Or translation. We care about stuff like computer-use. Now that we&rsquo;re evaluating agents, each of these tasks needs realistic and rich environments. Someone has make that! That&rsquo;s a lot of engineering, expensive infrastructure, and domain expertise to make sure you&rsquo;re not fooling yourself. When orchestrating 500 docker containers is the clean case, you know it&rsquo;s going to be painful.</p><p>As these setups are required to get more painful in order to accurately measure the capabilities, they&rsquo;re also just more expensive to run. The infrastructure needs spinning up, the token use to get to an action turn count such that you can prove your environment is sufficiently realistic and the task is sufficiently difficult is huge. Trust me, pal, you wouldn&rsquo;t run those evals if they <em>were</em> publicly verifiable. You don&rsquo;t have the stamina or the checkbook.</p><p>That in and of itself is one of the largest markers of progress to me. It is legitimately an intellectual exercise and engineering undertaking to get a truly useful set of scenarios where the models actually screw up. That was not the case in 2023. A lot of smart people are spending a lot of time trying to get to an empirical measurement they can trust for their particular domain. And that ability to <em>measure capability</em> in and of itself now becomes intellectual property, and it&rsquo;s pretty likely those who invest the effort are going to keep it to themselves.</p><h3 id=what-this-means-for-you>What This Means For You<a hidden class=anchor aria-hidden=true href=#what-this-means-for-you>#</a></h3><p>So do I have any actual recommendations here? Sure, build your own benchmarks. If you&rsquo;re an organization, this is basically a must. It&rsquo;s hard and requires a lot of effort but if you&rsquo;ve got a business case around models reaching a certain capability level, it&rsquo;s basically table stakes to be able to measure those in a mature and repeatable format. Nobody wants to write evals, nobody wants to run evals, but if you&rsquo;re not participating you&rsquo;re left looking at benchmark screenshots. This is, essentially, irresponsible and ensures that when the capabilities get to that point you were waiting for you&rsquo;ll find out about them via tweet.</p><p>If you&rsquo;re an individual? Well, the least you can do is get your private test set together. This could be questions, this could be engineering requests or code you&rsquo;d like to see, it could be a harness you expect to be able to accomplish some challenge agentically when the models get good enough. You don&rsquo;t have to tell anybody about it, but you should have them. They&rsquo;ll tell you more than the bar charts of those publicly available evals you&rsquo;ve never examined. And you&rsquo;ll be able to comfortably skip the livestream and decide for yourself if GPT-6 is any good.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>His language was a bit more severe and quite a bit funnier, but it&rsquo;s bad form to directly quote Signal GCs.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>In fairness this probably also has a lot to do with the model routing, which was apparently <a href=https://x.com/tszzl/status/1954325217087754278>broken</a> on day one.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>And then whatever the universal verifier (judge) tells them is good, and so on, and so on.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>And usurped MMLU as the bar chart people look at before they tweet whether the model is good or not.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>The only area where this is spiky in my personal experiences is cybersecurity evals, where the incentives seem to shift to desiring to look non-threatening and not worth legislating. Sometimes I look at results on stuff I run and the output of frontier labs and assume they&rsquo;re tying the model&rsquo;s hands behind their back and leaving them a python 2 interpreter, bash, and some duct-tape so they can report the models are still only kind-of-okay at CTFs. Trust me, they&rsquo;re really quite remarkable.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/llms/>Llms</a></li><li><a href=http://localhost:1313/tags/evals/>Evals</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/writing/haskell-empathy/><span class=title>Next »</span><br><span>The Religious Devotion of Haskell</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Shane Caldwell</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><style>.copy-code{display:inline-flex;align-items:center;justify-content:center;width:32px;height:32px;background:var(--tertiary);border:1px solid var(--border);border-radius:6px;color:var(--secondary);cursor:pointer;transition:all .2s ease;position:absolute;top:8px;right:8px;z-index:10}.copy-code:hover{background:var(--secondary);color:var(--theme)}.copy-code svg{width:16px;height:16px}.copy,.highlight .copy{display:none!important}pre{position:relative}</style><script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll('.copy, [class*="copy"]').forEach(e=>{e.classList.contains("copy-code")||e.remove()});const e=document.querySelectorAll("pre");e.forEach(e=>{const t=e.cloneNode(!0);e.parentNode.replaceChild(t,e)}),document.querySelectorAll("pre code").forEach(e=>{const n=e.parentElement;if(n.querySelector(".copy-code"))return;const t=document.createElement("button");t.classList.add("copy-code"),t.setAttribute("aria-label","Copy code"),t.setAttribute("type","button");const s=`<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"/><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"/></svg>`,a=`<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6L9 17l-5-5"/></svg>`;t.innerHTML=s;function o(){t.innerHTML=a,t.style.color="#10b981",setTimeout(()=>{t.innerHTML=s,t.style.color=""},2e3)}t.addEventListener("click",function(t){t.preventDefault(),t.stopPropagation();const n=e.textContent||e.innerText;navigator.clipboard?navigator.clipboard.writeText(n).then(()=>{o()}).catch(()=>{i(n)}):i(n)});function i(e){const t=document.createElement("textarea");t.value=e,t.style.position="fixed",t.style.opacity="0",document.body.appendChild(t),t.select();try{document.execCommand("copy"),o()}catch(e){console.error("Copy failed:",e)}document.body.removeChild(t)}n.appendChild(t)})})</script><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>.footnote-popup{position:absolute;background:var(--theme);border:1px solid var(--border);border-radius:6px;padding:12px 16px;max-width:300px;font-size:.85em;line-height:1.4;z-index:1000;box-shadow:0 4px 12px rgba(0,0,0,.15);font-family:var(--font-mono);color:var(--primary);display:none;pointer-events:auto;word-wrap:break-word}.dark .footnote-popup{background:#2d2d2d;box-shadow:0 4px 12px rgba(0,0,0,.3)}.footnote-popup::before{content:'';position:absolute;top:-6px;left:50%;transform:translateX(-50%);width:12px;height:12px;background:var(--theme);border:1px solid var(--border);border-bottom:none;border-right:none;rotate:45deg}.dark .footnote-popup::before{background:#2d2d2d}.footnote-ref{text-decoration:none!important;font-weight:700;padding:2px 7px;border-radius:4px;background:var(--primary);color:var(--theme)!important;transition:all .2s ease;position:relative;border:1px solid var(--border);font-size:.85em;line-height:1;display:inline-block;min-width:16px;text-align:center}.footnote-ref:hover{background:var(--secondary);color:var(--theme)!important;transform:translateY(-1px);box-shadow:0 2px 6px rgba(0,0,0,.2)}.dark .footnote-ref{background:#fff;color:#000!important;border:1px solid #666}.dark .footnote-ref:hover{background:#e5e5e5;color:#000!important;box-shadow:0 2px 6px rgba(0,0,0,.4)}</style><script>document.addEventListener("DOMContentLoaded",function(){let e=null,t=null;function s(e,t){const n=document.createElement("div");return n.className="footnote-popup",n.innerHTML=t,document.body.appendChild(n),n}function o(n,s){t&&(clearTimeout(t),t=null);const i=n.getBoundingClientRect(),r=s.getBoundingClientRect();let o=i.left+i.width/2-s.offsetWidth/2,a=i.top-s.offsetHeight-10;o<10&&(o=10),o+s.offsetWidth>window.innerWidth-10&&(o=window.innerWidth-s.offsetWidth-10),a<10&&(a=i.bottom+10),s.style.left=o+window.scrollX+"px",s.style.top=a+window.scrollY+"px",s.style.display="block",e=s}function n(){t=setTimeout(()=>{e&&(e.style.display="none",e=null)},150)}document.querySelectorAll("a.footnote-ref").forEach(e=>{const a=e.getAttribute("href"),r=document.querySelector(a);if(!r)return;const c=r.innerHTML.replace(/<a[^>]*href="#fnref[^"]*"[^>]*>.*?<\/a>/g,"").trim();if(!c)return;const i=s(a,c);e.addEventListener("mouseenter",()=>{o(e,i)}),e.addEventListener("mouseleave",n),i.addEventListener("mouseenter",()=>{t&&(clearTimeout(t),t=null)}),i.addEventListener("mouseleave",n)}),window.addEventListener("scroll",()=>{e&&(e.style.display="none",e=null)})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>