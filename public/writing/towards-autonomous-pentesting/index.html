<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Deep Reinforcement Learning for Security: Toward an Autonomous Pentesting Agent | Shane Caldwell</title>
<meta name=keywords content="reinforcement-learning,penetration-testing,autonomous-agents,deep-learning"><meta name=description content="An exploration of using deep reinforcement learning to create autonomous penetration testing agents, examining the challenges and potential solutions for automating cybersecurity assessments."><meta name=author content="Shane Caldwell"><link rel=canonical href=http://localhost:1313/writing/towards-autonomous-pentesting/><link crossorigin=anonymous href=/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/writing/towards-autonomous-pentesting/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script data-goatcounter=https://sjcaldwell.goatcounter.com/count async src=//gc.zgo.at/count.js></script><style>:root{--font-mono:'SF Mono', 'Monaco', 'Inconsolata', 'Roboto Mono', 'Source Code Pro', 'Menlo', 'Consolas', monospace}body,html{font-family:var(--font-mono)!important}*,*::before,*::after{font-family:var(--font-mono)!important}.dark{--primary:#ffffff;--secondary:#e5e5e5;--tertiary:#cccccc}body:not(.dark){--primary:#000000;--secondary:#333333;--tertiary:#666666}.dark a{color:#fff!important;text-decoration:underline}.dark a:hover{color:#e5e5e5!important}code,pre{font-family:var(--font-mono)!important;font-size:.9em}h1,h2,h3,h4,h5,h6{font-weight:700!important;color:var(--primary)!important}.post-meta{color:var(--secondary)!important}button,.button{font-family:var(--font-mono)!important;font-weight:500}.paper-card,.talk-card{background:var(--theme);border:1px solid var(--border);border-radius:8px;padding:24px;margin-bottom:24px;transition:all .2s ease;box-shadow:0 2px 4px rgba(0,0,0,5%)}.paper-card:hover,.talk-card:hover{border-color:var(--secondary);box-shadow:0 4px 8px rgba(0,0,0,.1);transform:translateY(-1px)}.dark .paper-card,.dark .talk-card{background:#1a1a1a;border-color:#333;box-shadow:0 2px 4px rgba(0,0,0,.2)}.dark .paper-card:hover,.dark .talk-card:hover{border-color:#555;box-shadow:0 4px 8px rgba(0,0,0,.3)}.paper-title{margin:0 0 12px!important;font-size:1.25em;line-height:1.3}.paper-title a{color:var(--primary)!important;text-decoration:none;border-bottom:2px solid transparent;transition:border-color .2s ease}.paper-title a:hover{border-bottom-color:var(--primary)}.paper-meta{margin-bottom:16px;font-size:.9em}.paper-authors{color:var(--secondary);margin-bottom:4px;font-weight:500}.paper-date{color:var(--tertiary);font-size:.85em}.paper-abstract{color:var(--primary);line-height:1.5}.paper-abstract p{margin:0}.talk-title{margin:0 0 12px!important;font-size:1.25em;line-height:1.3}.talk-collaborators{margin-bottom:16px;font-size:.9em;color:var(--secondary)}.talk-collaborators p{margin:0}.talk-details{display:flex;flex-direction:column;gap:8px}.talk-event,.talk-recording{font-size:.9em}.talk-recording a{color:var(--primary)!important;text-decoration:underline}.talk-recording a:hover{color:var(--secondary)!important}@media(max-width:768px){.paper-card,.talk-card{padding:16px;margin-bottom:16px}}</style><meta property="og:url" content="http://localhost:1313/writing/towards-autonomous-pentesting/"><meta property="og:site_name" content="Shane Caldwell"><meta property="og:title" content="Deep Reinforcement Learning for Security: Toward an Autonomous Pentesting Agent"><meta property="og:description" content="An exploration of using deep reinforcement learning to create autonomous penetration testing agents, examining the challenges and potential solutions for automating cybersecurity assessments."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="writing"><meta property="article:published_time" content="2020-04-28T00:00:00+00:00"><meta property="article:modified_time" content="2020-04-28T00:00:00+00:00"><meta property="article:tag" content="Reinforcement-Learning"><meta property="article:tag" content="Penetration-Testing"><meta property="article:tag" content="Autonomous-Agents"><meta property="article:tag" content="Deep-Learning"><meta property="og:image" content="http://localhost:1313/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/"><meta name=twitter:title content="Deep Reinforcement Learning for Security: Toward an Autonomous Pentesting Agent"><meta name=twitter:description content="An exploration of using deep reinforcement learning to create autonomous penetration testing agents, examining the challenges and potential solutions for automating cybersecurity assessments."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Writing","item":"http://localhost:1313/writing/"},{"@type":"ListItem","position":2,"name":"Deep Reinforcement Learning for Security: Toward an Autonomous Pentesting Agent","item":"http://localhost:1313/writing/towards-autonomous-pentesting/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Deep Reinforcement Learning for Security: Toward an Autonomous Pentesting Agent","name":"Deep Reinforcement Learning for Security: Toward an Autonomous Pentesting Agent","description":"An exploration of using deep reinforcement learning to create autonomous penetration testing agents, examining the challenges and potential solutions for automating cybersecurity assessments.","keywords":["reinforcement-learning","penetration-testing","autonomous-agents","deep-learning"],"articleBody":"Introduction I’ve found myself very interested in reinforcement learning recently. As you do deep learning work, you can sometimes feel limited in the problems you can solve by the paradigms you have available. To paraphrase Andrej Karpathy, the APIs to deep learning can seem constraining, despite their power. We start with a fixed size input and fixed size output for problems like classification routinely solved by CNNs. To deal with text, we have RNNs and the more intricate LSTM models that can deal intelligently with long sequences with a kind of memory. There’s an incredible array of kinds of problems that can be formulated to be solved by those approaches. We’ve seen generated artwork with GANs, object detectors used for medical diagnostics, and CNNs applied to sound classification. It will be a long time before we’re out of runway applying these techniques with novel variations to different fields with a lot of success. There are careers to be made for clever folks to use domain knowledge in a subject to reformulate their problem into one of these “solved problems”.\nWhen I started studying machine learning, I actually had a specific domain in mind I wanted to apply it to. I’d been a penetration tester for almost two years and recently earned my OSCP when I was offered a position in a Masters in Data Science program. Pentesting was super fun, but I found myself daydreaming on the problem of whether it was possible to develop intelligent tools to aid in penetration testing. What would a tool like that be like? Specifically, I wanted to know whether it was possible to create an autonomous pentesting agent, like the kind of sentient hacking AI that make up the endlessly readable William Gibson novels.\nIt was also partially born out of a desire to make a useful tool in a competitive field. There are really wonderful tools out there for the would-be attacker. For web application pentesting, Burp Suite is an incredibly comprehensive exploitation tool. It’s a proxy that sits between your HTTP requests coming from your client browser heading to the server, allowing you to freely edit the content going to the server. Through this, all sorts of interesting attacks are possible. Using the tool is easy, as well! After browsing the site normally for awhile, it logs all the routes you can send requests to, and all the types of requests you’ve sent and received while interacting with the tool. From there, you can run a scan. The scan can reliably find everything from cross-site scripting to SQL injection mostly with the power of regular expressions and a handy list of strings that are usually used to exploit these sorts of attacks.\nFrom the network side of things, Metasploit is even more compelling. It’s a tool and framework all in one. From within the metasploit tool you can keep track of almost everything you need to run a penetration test successfully. You can run scans, store information about target hosts, customize and launch exploits, and select payloads all from within that tool. Even more incredible - it’s open source! Once a proof of concept for an exploit has been discovered, there’s an easy to use API that allows you to write a little Ruby and produce your own exploit that you can share with others.\nThose tools are remarkably solid and being produced by a community of talented security professionals. Better yet, they’re frameworks that allow a developer to add new functionality for anything they find lacking and share it with the world. Still, I couldn’t help but think it should be possible to perform the work automatically. I don’t mean ‘script recurring tasks’ automatic, I mean ‘set it, perform pentest, let me know how to patch the holes you found’ automatically. That’s not to say I want the work to go away. The most exciting aspects of the work are this rare 15% of it that requires an insane amount of creativity and knowledge. You can read writeups from folks who have found seemingly invisible bugs that you would think don’t have any impact at all, and used them to completely compromise applications and plunder their databases. If you don’t believe me, the popularization of bug bounties have made it incredibly easy to see what kind of hacks are out there in the wild. Bug bounties allow hackers to make money for security bugs found within their applications or networks, and many organizations running the programs allow for writeups to be published after the fact. It’s humbling to read them.\nThat other 85% or so can be a bit of a slog, though. There are several well known security issues that crop up time and time again. Finding them is always exciting in the way that all hacking is - you broke a thing that’s not supposed to break! You have access to stuff you’re not supposed to have! But it’s not challenging or engaging, really. Is it possible to build tools that make all of security the fun part? And of course, the holy grail - is it possible to make an agent even better at penetration testing than humans?\nBut before we plot the future, let’s see where we stand. How is ML being applied to security today?\nThe state of ML in Defense Most machine learning naturally lends itself to defense, more than attack. There’s actually been a pretty good amount of defensive tooling developed. And why not? The paradigms fit like a glove. As a defender your biggest problem is probably that you have too much information. Networks are just happening all the time, generating all sorts of traffic on all sorts of services. You’re a human being with two eyes and a limited amount of caffeine to throw at the problem of perceiving incredibly granular logs. If you knew something bad was happening, you’re probably educated enough to take an action, but how can you know? Frequently some scripted logic and a regular expression list can alert you of some well described dangers - imagine your database administrator logged in from an IP belonging to a country they don’t live in and then changed their password - but not all dangerous situations are that well-described. What about stuff that’s just weird?\nThese fall under the general bucket of anomaly detection as a problem. First, you gather a lot of data and group it into some sort of observation at a fidelity a model can interpret. Then, you run the observation through the model and get a boolean output. Either it’s bad, and you alert a person, or it’s good, and nothing happens. Think about it as a “weird/not weird” classifier. The intuition behind the perceptual task is stored within the dataset, and the algorithm transforms it into something that’s augmenting a human’s capabilities by taking cognitive load off of them.\nIf you’re looking for something with a similar principle but more automated, all sorts of “smart firewalls” can be made this way. You learn what looks normal, train a network to recognize normal, and then if you’re not normal you’re an anomaly. The upside is big - if you detect an attack, you can take an action. The downside of a false alarm can be bad depending on the tooling, but as long as you’re not overwhelmed with anomalies to look at a false positive is fine. At least in theory whatever you’re looking at should be anomalous and therefore interesting.\nIn practice, this is challenging to pull off. What’s normal for a network is a living, breathing thing. New people come in, they leave. New servers come on site. If configured poorly, all of these things can be anomalous. Training a network in a custom way is also challenging - you want to learn a good distribution of normal but for that to be legitimate you would need to know within a shadow of a doubt that your network is currently not compromised as you’re training. Obviously, you have no idea whether that’s the case or not and there’s really no way to prove otherwise. So you have this sort of ontological problem for these types of detectors that’s challenging to solve, at least at the network level.\nCylance claims to do this on the endpoint level, using AI to find malware processes on desktops and phones. There’s not really a clear whitepaper that breaks down how, but it sounds pretty cool. The approach for an endpoint anomaly detector seems equally sound to others in the anomaly detection paradigm - in each you find this distribution of process behavior that’s normal or acceptable, and if you fall outside of that you can flag it and allow a user to make the call to override detection if it’s a false positive.\nYou couldn’t really call any of these tools autonomous defenders though. You don’t have agents on the environment watching network traffic and taking actions in response to them. You might automatically put someone on a block list, or filter bad traffic (I too have scraped websites aggressively enough that I was hit with a captcha) but none of those tools are giving the Security Operations Center the day off to play golf. We don’t have ourselves an “autonomous defender”, we have a fire alarm.\nThe state of ML in Offense The state of things over on the offensive side is actually starting to catch up to defense, at least over the last couple of years. Attackers do a lot of enumerating resources, which is its own form of data collection (though it pales in comparison to the sheer volume of the defensive side).\nThey follow a very similar paradigm as well, actually. Except now anomaly means something different. On the offensive side it’s “Hey bud, that’s a whole lotta attack surface to look at there. Want me to check it out and see if any tires seem worth kicking”?\nBishopFox’s eyeballer is actually a really cool example of one of these. Many security tools sniff HTTP endpoints of a target and screenshot them for you to review. Eyeballer goes that extra step forward and lets you apply classification to the problem. Run them through the classifier to find out if they’re login pages, or they look like old custom code, whatever. It’s a great example of taking a domain specific pentesting problem and making it fit into the classification paradigm.\nThere’s been similar work done with text. I even found a language model used to do reconnaissance on a target’s twitter and then use text models to customize messages with phishing links catered to them. This is a BlackHat talk from ZeroFox. As you might’ve noticed, there are a lot of foxes in security consulting. But also, this is very much in line with what I was thinking of - an automated, intelligent tool to assist with security testing.\nFor the record, I think all of the tools I’ve listed above are insanely cool and I would’ve been proud to have worked on any of them. It is not a critique that none of them seem to fit the paradigm I’m looking for: how would you go about developing an agent that could act autonomously? To be specific, the ‘hello world’ of such an agent might look as follows:\nHow could you develop a system that had never seen Metasploitable or similar vulnerable-by-design single hosts that could be placed on the same network as them, automatically enumerate information about, exploit, and extract data from them? If such a system was robust enough to handle many different intentionally vulnerable systems, it would be an autonomous pentesting agent.\nReinforcement Learning If you’re interested in AI, you’ve probably heard of reinforcement learning. Even if you haven’t heard it by that name, it’s definitely been in the news. It’s the paradigm that made AlphaGo possible, and is the same paradigm that’s helped OpenAI crush Atari scores for game after game. It’s also made a bot that can play Smash Bros pretty dang well. But what is it? And how might it help us develop a system that can hack autonomously?\nBroadly, reinforcement learning is the study of agents that learn by trial and error. Agents learn policies that direct them to take actions and then observe the change in environments and the reward they receive to inform their next action.\nMulti-Armed Bandits The classical non-deep example, the one a reader is most likely to have come across in the past, is the multi-armed bandit. The problem is a simple one: you find yourself in a casino. You stand in front of a slot machine with three arms. You’re told that each of the arms has a different probability of success - some are luckier than others. Your goal is to find the best strategy to achieve the highest reward you can in a given number of arm pulls.\nA naive approach might be to play with each arm many times. In fact, play each arm so many rounds you can eventually estimate the true probability of reward on the machine when the law of large numbers kicks in. Once you’ve done this for each machine, you merely need to hang out on the machine that ended up with the highest reward probability, right? Easy peasy.\nThose of you who have gone to a casino would surely retort that this is an inefficient and expensive strategy. Fine, then: let’s introduce some definitions and try to use math to be a little more than lucky.\nWe have $n$ arms on the machine, and $t$ number of time steps to play the game. Each arm represents an action $a$ we can take. Our goal is to approximate the true success probability of each of the arms or $q(a)$ and then exploit that knowledge for reward.\nWe’ve established we can’t know the true reward, so we’ll call our approximation $Q(a)$. Because this is an approximation based on our current understanding of the environment, and we’re an intelligent agent that updates our beliefs based on our observations, it makes most sense to think about $Q_t(a)$, or our estimate valued of a given action at a given time step, $t$.\nFirst, we know nothing about the environment, so we pull an arm at random. Let’s say it gives us a reward! For one pull of the arm you’ve gotten exactly one reward. What do you think about that machine’s odds of success now?\nWell, it makes the most sense to basically just keep a running list of how many times we’ve tried the action, and what our total reward has been with the action. That’s our estimated probability. Something like:\n$$ Q_t(a) = \\frac{R_1 + R_2 + … + R_{N_t(a)}}{N_t(a)} $$\nWith this, we could keep a running best guess of the reward for each action.\nBut that’s a lot of information to record. For a computer program, that means the memory needed for the program scales up linearly with the amount of time steps considered. In practice, we use something called a q table to keep the memory constant. I won’t go into it too much here but you’ll see it below in my python implementation. The idea is the same, which is to update $Q_t(a)$ at each timestep allowing it to become slowly more accurate.\nSo what is our strategy? A greedy strategy is just to read the action from the Q table that maximizes your reward:\n$$ A_t = \\arg\\max Q_t(a) $$\nRemember, we already pulled a lever once and it yielded a reward. So that action is the only one in the Q table with a value over 0.0. So does that just mean we select that action over and over again, without ever trying the other arms? How do we know the other actions wouldn’t give us even greater rewards?\nThis is the essence of the multi-armed bandit problem. To exploit our current knowledge of the environment to the best of our ability or explore to learn more about an action we don’t currently understand very well.\nTo do this, we introduce $\\epsilon$. Every $\\epsilon%$ of the time, we will choose a random action instead of the action we know will yield us the most gain, observe our success or failure, and update our $Q_t(a)$ for that action.\nGiven a reasonable choice of $\\epsilon$ and enough time steps, this allows us to converge on the best solution, even if our initial solution is not optimal.\nWe can examine this in code, as below:\nimport numpy as np class Environment: def __init__(self, p): ''' p is the probability of success for each casino arm ''' self.p = p def step(self, action): ''' The agent pulls an arm and selects an action. The reward is stochastic - you only get anything with the probability given in self.p for a given arm. action - the index of the arm you choose to pull ''' result_prob = np.random.random() # Samples from continuous uniform distribution if result_prob \u003c self.p[action]: return 1 else: return 0 class Agent: def __init__(self, actions, eps): ''' actions - The number of actions (arms to pull) eps - The frequency with which the agent will explore, rather than selecting the highest reward action ''' self.eps = eps self.num_acts = actions self.actions_count = [0 for action in range(actions)] self.Q = [0 for action in range(actions)] def act(self): if np.random.random() \u003c self.eps: #we explore action = np.random.randint(self.num_acts) else: #we exploit action = np.argmax(self.Q) return action def update_q_table(self, action, reward): self.actions_count[action] += 1 step_size = 1.0 / self.actions_count[action] self.Q[action] = self.Q[action] + (1 - step_size) * self.Q[action] + step_size * reward def experiment(p, time_steps, eps): ''' p is probabilities of success for arms time_steps - number of time steps to run experiment for epsilon to choose for agent ''' env = Environment(p) agent = Agent(len(p), eps) for time_step in range(time_steps): action = agent.act() # get action from agent reward = env.step(action) # take action in env agent.update_q_table(action, reward) #update with reward return agent.Q q_table = experiment([0.24, 0.33, 0.41], 1_000_000, 0.1) The final q_table appears as [0.2397833283177857, 0.3332216502695646, 0.41020130865076515], indicating we were pretty successful in estimating $q(a)$ with $Q_t(a)$.\nSo it’s a simplistic example, but illustrates the power of reinforcement learning. Unlike a supervised learning example, we never told the system what the right answer was - the third level, with $q(a_3) = 0.41$. We enabled the agent to observe the effects of its actions to update its policy, and change its behavior.\nIf you want to read more about classic reinforcement learning, I highly recommend the extremely pleasant to read and extremely free Reinforcement Learning: An Introduction. Hopefully this gentle introduction has convinced you there’s an interesting power here, different from supervised or unsupervised learning methods you may have known in the past.\nThe Successes (and Caveats) of Deep Reinforcement Learning Reinforcement learning allows for self-directed optimization. Deep learning allows for function approximation. By combining the two we’re able to map environment state and action pairs into expected rewards.\nSuccesses I won’t go too long here, because there’s already plenty of hype. AlphaZero can play Go better than anyone has ever played Go, and through self-play eventually invented novel openings that human beings are now studying. Hard to overstate how mind-blowing that is. I think this was a pretty epoch defining event for anyone interested in AI in any field.\nCaveats Before I get into the weeds of the challenges deep reinforcement learning faces as a field, I’d be remiss to not advise anyone interested to read Alex Irpan’s Deep Reinforcement Learning Doesn’t Work Yet. I’ll be summarizing some of these points below, but the whole article is a sobering but ultimately optimistic read for those looking to cut their teeth on deep RL.\nI’ll be looking at each of these as challenges to be overcome for my own research: developing an autonomous pentesting agent.\nSample Inefficiency One of the key problems in deep RL is sample inefficiency: that is, you need a whole lot of data to get good performance. The ratio of environment complexity to data required for strong performance can seem frighteningly high. For many environments, particularly real life ones, you’re almost out of luck.\nEven in my multi-armed bandit scenario, I ran 1,000,000 episodes. This was a pretty simple environment to learn from. Imagine training an agent against Metasploitable. You allow the agent to take action until the completion of the episode. Then you restart the virtual machine in a clean state, and begin again. Parallelizing this requires multiple virtual machines, and the time between episodes is as long as it takes to load up a fresh disk image - and that’s for a single host! Full environments representing entire networks would be even harder to generate adequate experience for. Think about how long it takes you to spin up a fleet of boxes in Amazon, much less configure all the network policies. Brutal. For a single host, resetting metasploitable to a clean state a million times would take, optimistically, two minutes a pop. Doing that one million times? That would take about 4 years.\nSo even if the method could work in principle, generating the data to overcome sample inefficiency is going to be tough.\nReward Function Design is Tough Designing reward for Go is kinda easy. Collecting territory and winning? These things are good. Giving up territory and losing the game? This is very bad. Atari is pretty straightforward as well. Each of these games provide a score - if you make the score go up, you’re doing well! If the score goes down, or you die, you’re doing poorly.\nExpressing those sorts of reward functions in simple environments mathematically is not extraordinarily difficult.\nHow about more subtle goals though? Take our goal of pentesting:\nHow do you define good pentesting? To do that, you’d need to ask a good pentester what their goals are on an assessment. Since I don’t have any on hand, my personal experience will have to suffice: good pentesting is about careful thoroughness.\nFor a real life attacker, your only goal is to find a single exploitable hole good enough to weasel your way into the network, find high-value information, and take off with it. Ideally without letting anyone know you were there. Sort of a depth-first search kinda deal.\nPentesting needs to be wide and deep. You want to present the client with evidence you looked over their network to the best of your ability, found as many chinks in their armor as possible at all levels of access you were able to achieve. And while doing this, you’re under certain constraints. You can’t break their network to discover a high value target. Some things are off limits, also known as out-of-scope. Also you have a fixed amount of time. So you can’t explore everything. You have to provide breadth, and use your intuition to decide where to spend time going deep that will provide the biggest bang for the client’s buck. That’s good pentesting.\nThere are two kinds of rewards we might try: sparse rewards only provide reward at the end of the episode if the policy resulted in a ‘success’. The agent “won” the game. We’re having a hard time defining success for pentesting if we use the above definition, but even if the answer was just ‘got root access on a specific machine’ that likely wouldn’t be enough. With so little to go off of, you can imagine a pentesting agent firing off some random scans, maybe trying some random exploits against random machines, and never receiving even a drop of reward for its trouble. The policy network has no valuable information to backprop on, and you’re essentially dead stuck unless by some miracle the network chooses random actions that lead to success. As a former pentester, I can attest that I have tried that strategy and been very disappointed in it.\nIn this case, we need something more complicated. Shaped reward provides increasing rewards for states as they become closer to the end goal, rewarding actions that are useful. This sounds like a better fit for our problem. For example, scanning a potential target is not getting root on a high value target, but it’s a useful step on the way, so we should give some reward there.\nHow would you express that as a reward function? Exploits are good! Discovering hosts, and information about hosts is also good. But we want to ensure we’re not just brute-forcing throwing exploits at hosts to see if they work, so maybe we impose noisiness cost per action to encourage strategic exploits and scanning. How do we weigh the reward of exploit vs scanning? When it comes to information exfiltration, how do we teach an agent to understand what high-value vs low-value information is? We want the agent to understand high-value targets that deserve more intensive study, but how do we communicate that? In fact, we don’t want to do that at all - we want the agent to discover that. Now how do you say that with math? When you try to piece these ideas into a singular reward function it gets hard quick.\nReward Functions like to Blow Up in Your Face Agents do not care about your problems. They only care about the reward their actions can give them. Despite the elegant expressiveness of mathematics and your best personal efforts, there will probably be a gap between your intentions. In these gaps, the agent will attempt to find whatever action in the environment gives them the quick fix of reward without all the challenge of discovering a really useful policy.\nOpenAI provides an infamous example in one of their experiments: in a boat racing game, they used a shaped reward. The agent got the most reward for winning, but they got partial reward for picking up powerups (useful for winning!) and passing checkpoints.\nThe agent quickly discovers you can get the most reward by just collecting the powerups, since they regenerate quickly. It finds itself stuck in a really elegant loop as its opponents whiz by. The agent will never win the race this way, and still get an incredible amount of reward. This is called reward hacking.\nThink about our previously proposed hodge-podge of actions that would give our hypothetical agent reward. It’s easy to imagine an agent that had not yet penetrated the network finding a successful exploit that got it access to another machine. Great place to farm! The agent would likely just fire off that exploit again and again, and each success would give it more reward. The same could be said about a scan enumerating a host, or any number of activities. Without a carefully crafted reward, our proposed shaped reward could be easily “hacked”, with plenty of reward gained and our task undone.\nThe Environment Challenge State Space Another thing deep reinforcement learning requires is an environment. For a game like chess or shogi, this is just the board. It’s pretty easy to gracefully represent as a matrix.\nDefining a board for pentesting is kind of hard. You kind of start with a fog of war situation where you know about the perimeter of a network early on, but you really don’t know the full size of the environment in terms of number of hosts until you find one. So it’s an environment that starts small and gets bigger over time, with each new host found having different properties.\nMost game environments are pretty fixed, so that’s tough. It could be seen as a blessing, though. You’re encouraged to overfit like crazy in reinforcement learning when generating experience in the game, often these learned skills don’t transfer to a new environment. For penetration testing each “game” starts on a new network, or a new sized “board”. There’s a general pattern of penetration testing that should stay consistent, but the shape of the network and hosts on it will define what your optimal actions are. Hopefully that keeps overfitting to a minimum.\nAction Space Your action space, the actions available to an agent that can be taken, also need to be provided. Chess, for example, this might be the legal moves your agent can take for any input board state.\nThere are continuous and discrete action spaces. Discrete action spaces basically just means a countable number of actions. The chess example applies here. Continuous action spaces might be found when you’re using RL to set the specific value of a sensor, for example. Where the value of the sensor can take on any real-numbered value between a lower and upper bound. To be honest, I haven’t totally wrapped my head around methods for continuous action spaces but I have seen a lot of clever problem formulation to make the action space discrete instead.\nFor example, take that sensor problem - pretty continuous. But what if we assume there’s a minimum amount you can tune the sensor up or down that’s meaningful? Call it $x$. Now, after taking an observation from our environment, let’s say we only have two options - up or down by $x$. Well golly gee, sir, up or down? I ain’t no mathematician but that’s a pretty discrete space if I do say so myself.\nThis sort of judo is on display whenever the problem allows for it. When OpenAI tackled Dota 2, they easily could have considered the action space continuous - but they didn’t. They discretized the action space on a per-hero basis, arriving at a model choosing among 8,000 to 80,000 discrete actions depending on their hero. A discrete action space will be pried from their cold, dead hands.\nThat’s a lot of moves. OpenAI had access to the game engine’s API, so these actions were probably read rather than hand-coded. For our pentesting problem, how do we handle that? You’re sitting in front of a terminal, where you can enter any text. A very minuscule part of the distribution of all text you can type into a terminal is going to be valuable for accessing your hacking tools. Within those tools, there’s very specific syntax that will be valuable. That’s a pretty big action space, and I’m not sure we can specify reward that will make that valuable, even shaped. So what’s the play?\nMetasploit API: The ‘game engine’ of pentesting I puzzled over this for a long time before I did some literature review and found Jonathan Schwartz’s thesis Autonomous Penetration Testing using Reinforcement Learning. In it, he creates a pretty convincing partially observable Markov decision process to form a model of penetration testing. It’s one of the few real attempts I’ve seen to tackle the formulation of the problem. One line in particular really inspired me to take a serious look at the problem again. While justifying some simplifications to his network model, Jonathan says:\nThe specific details of performing each action, for example which port to communicate with, are details that can be handled by application specific implementations when moving towards higher fidelity systems. Penetration testing is already moving in this direction with frameworks such as metasploit which abstract away exactly how an exploit is performed and simply provide a way to find if the exploit is applicable and launch it, taking care of all the lower level details of the exploit\nFirst, this struck me as an oversimplification. How many times had I loaded up an exploit in metasploit only to have it not work? Then I had to dig into the specifics of the Ruby code and twiddle with things. Many exploits also have a pretty large number of required arguments to set that require some domain/target specific knowledge. Then I decided this was totally genius. That insanely large action space of the open terminal now starts to more resemble a game board. Metasploit stores information about hosts it knows about, their open services and distribution information. Exploits apply to specific distributions and services. Metasploit even provides tools for information gathering once you’ve compromised your host. It’s not always enough - often you need to break out of their laundry list of commands and use an honest-to-god terminal. But there’s a lot you can do restricting the action space to the Metasploit level. I haven’t done the back of the envelope math, but that feels like Dota 2 size action space to me, maybe smaller.\nThe actions you can take with Metasploit, and the information it chooses to store reduces the complications in considering both the action space and the state space of penetration testing.\nRelated Safety Problems Solving penetration testing would also involve, as a sub-problem, solving a variety of safety problems. Not safety as in “paper clip AGI destroys humanity” but several of the problems described in OpenAI’s Concrete Problems in AI Safety. It’s essentially a review of practical research problems that can be broadly categorized around AI safety. Some of them are practically necessary to solve before you can design an agent that could be truly an autonomous attacker or defender.\nIn the paper safe exploration is broadly defined as ensuring the “exploration” side of exploitation vs exploration is sensitive to how it explores such that it doesn’t take extremely risky ’exploratory’ actions. This is part of being subtle in penetration testing engagements. If you explore loudly (a super fast, full network scan) you’ll probably get lots of information quickly, but you’re also likely to set off the SOC’s alarms and are liable to get your IP blocked. Every pentester has experienced the walk of shame, having tripped the alarms before the engagement has scarcely begun and writing a tepid email about how it would be oh-so kind of the SOC to unblock an IP range, yes, thank you, sorry.\nAvoiding negative side effects is defined as not disturbing the environment in negative ways while pursuing its goals. This is absolutely huge on an engagement. More than once I’ve been in a situation where I’m fairly certain I have an exploit that could gain me access to the server. Trouble is, while this exploit will give me a shell, it will also crash the service. Not only will this definitely trip some alarms, but if the service isn’t set to auto-start you could cause an outage. Disturbing the day-to-day work of the client during a penetration test is a sure fire way to make sure you never work with them again. The question of whether we can teach an agent this sort of discretion without manually specifying all the things that it shouldn’t disturb is a challenging question with no immediate answers.\nBoth of these are generally problems that exist in more complex environments than the ones RL has succeeded in so far, and would certainly need to be solved before any of these agents with any influence over the real-world environment could be released in the wild.\nSimulation as a path forward If you’ve read this far, you might be under the impression I have a pretty negative view of the odds of solving penetration testing with RL. Nothing could be further from the truth! I’m just being honest about the many, potentially very thorny, sub-problems on the way to that solution.\nTo me, the immediate work to be done is in the simulation space. One has to choose a subset of Metasploit actions directly from their API and map them to actions an agent can take.\nThere’s still the problem of sample inefficiency - how do you generate enough experience?\nThe answer has to be simulation. Instead of interacting with a full virtual machine environment, you need a simulated environment that makes it easy for an agent to quickly test a policy against an environment. The way the network is composed needs to be, to my mind, similar to a rogue-like game. We want procedurally generated vulnerable networks at a just realistic enough fidelity for policies learned to apply to a real network. These could be spun up and down quickly and easily parallelized to achieve the kind of massive experience generation achieved by OpenAI with Dota 2.\nThe aforementioned Jonathan Schwartz has already developed a simulator that I believe steps in that direction, and extending it would certainly make a good environment for the metasploit-driven agent I’m picturing.\nFor now, I need to consider the design of the subset of metasploit actions that would make an acceptable action space for solving non-trivial vulnerable networks. Achieving an acceptable fidelity for the simulation is also key - but to me it’s just the minimum viable environment that allows the metasploit action APIs to be meaningful.\nIn a future post, I’ll take my first steps using the OpenAI Gym framework to develop a simple environment I can train one of their prewritten models on. Whatever the final shape of the simulator, I believe making sure it fits within the OpenAI gym framework popularized by researchers at the forefront of RL is the best way to get new eyes onto the project. It’s also a good way for me to get some experience with DRL tooling.\n","wordCount":"6178","inLanguage":"en","image":"http://localhost:1313/","datePublished":"2020-04-28T00:00:00Z","dateModified":"2020-04-28T00:00:00Z","author":{"@type":"Person","name":"Shane Caldwell"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/writing/towards-autonomous-pentesting/"},"publisher":{"@type":"Organization","name":"Shane Caldwell","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Shane Caldwell (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Shane Caldwell</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Main><span>Main</span></a></li><li><a href=http://localhost:1313/papers/ title=Papers><span>Papers</span></a></li><li><a href=http://localhost:1313/talks/ title=Talks><span>Talks</span></a></li><li><a href=http://localhost:1313/writing/ title=Writing><span>Writing</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Deep Reinforcement Learning for Security: Toward an Autonomous Pentesting Agent</h1><div class=post-description>An exploration of using deep reinforcement learning to create autonomous penetration testing agents, examining the challenges and potential solutions for automating cybersecurity assessments.</div><div class=post-meta><span title='2020-04-28 00:00:00 +0000 UTC'>April 28, 2020</span>&nbsp;·&nbsp;30 min&nbsp;·&nbsp;6178 words&nbsp;·&nbsp;Shane Caldwell</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#the-state-of-ml-in-defense>The state of ML in Defense</a></li><li><a href=#the-state-of-ml-in-offense>The state of ML in Offense</a></li><li><a href=#reinforcement-learning>Reinforcement Learning</a><ul><li><a href=#multi-armed-bandits>Multi-Armed Bandits</a></li></ul></li><li><a href=#the-successes-and-caveats-of-deep-reinforcement-learning>The Successes (and Caveats) of Deep Reinforcement Learning</a><ul><li><a href=#successes>Successes</a></li><li><a href=#caveats>Caveats</a></li></ul></li><li><a href=#the-environment-challenge>The Environment Challenge</a><ul><li><a href=#state-space>State Space</a></li><li><a href=#action-space>Action Space</a></li><li><a href=#metasploit-api-the-game-engine-of-pentesting>Metasploit API: The &lsquo;game engine&rsquo; of pentesting</a></li></ul></li><li><a href=#related-safety-problems>Related Safety Problems</a></li><li><a href=#simulation-as-a-path-forward>Simulation as a path forward</a></li></ul></nav></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>I&rsquo;ve found myself very interested in reinforcement learning recently. As you do deep learning work, you can sometimes feel limited in the problems you can solve by the paradigms you have available. To paraphrase <a href=http://karpathy.github.io/2015/05/21/rnn-effectiveness/>Andrej Karpathy</a>, the APIs to deep learning can seem constraining, despite their power. We start with a fixed size input and fixed size output for problems like classification routinely solved by CNNs. To deal with text, we have RNNs and the more intricate LSTM models that can deal intelligently with long sequences with a kind of memory. There&rsquo;s an incredible array of kinds of problems that can be formulated to be solved by those approaches. We&rsquo;ve seen generated artwork with <a href=https://www.artbreeder.com/>GANs</a>, object detectors used for medical diagnostics, and CNNs applied to <a href=https://medium.com/x8-the-ai-community/audio-classification-using-cnn-coding-example-f9cbd272269e>sound classification</a>. It will be a long time before we&rsquo;re out of runway applying these techniques with novel variations to different fields with a lot of success. There are careers to be made for clever folks to use domain knowledge in a subject to reformulate their problem into one of these &ldquo;solved problems&rdquo;.</p><p>When I started studying machine learning, I actually had a specific domain in mind I wanted to apply it to. I&rsquo;d been a penetration tester for almost two years and recently earned my <a href=https://sjcaldwell.github.io/writing/oscp-review/>OSCP</a> when I was offered a position in a Masters in Data Science program. Pentesting was super fun, but I found myself daydreaming on the problem of whether it was possible to develop intelligent tools to aid in penetration testing. What would a tool like that be like? Specifically, I wanted to know whether it was possible to create an autonomous pentesting agent, like the kind of sentient hacking AI that make up the endlessly readable <a href=https://en.wikipedia.org/wiki/Neuromancer>William Gibson</a> novels.</p><p>It was also partially born out of a desire to make a useful tool in a competitive field. There are really wonderful tools out there for the would-be attacker. For web application pentesting, <a href=https://portswigger.net/burp>Burp Suite</a> is an incredibly comprehensive exploitation tool. It&rsquo;s a proxy that sits between your HTTP requests coming from your client browser heading to the server, allowing you to freely edit the content going to the server. Through this, all sorts of interesting attacks are possible. Using the tool is easy, as well! After browsing the site normally for awhile, it logs all the routes you can send requests to, and all the types of requests you&rsquo;ve sent and received while interacting with the tool. From there, you can run a scan. The scan can reliably find everything from cross-site scripting to SQL injection mostly with the power of regular expressions and a handy list of strings that are usually used to exploit these sorts of attacks.</p><p>From the network side of things, Metasploit is even more compelling. It&rsquo;s a tool and framework all in one. From within the metasploit tool you can keep track of almost everything you need to run a penetration test successfully. You can run scans, store information about target hosts, customize and launch exploits, and select payloads all from within that tool. Even more incredible - it&rsquo;s open source! Once a proof of concept for an exploit has been discovered, there&rsquo;s an easy to use API that allows you to write a little Ruby and produce your own exploit that you can share with others.</p><p>Those tools are remarkably solid and being produced by a community of talented security professionals. Better yet, they&rsquo;re frameworks that allow a developer to add new functionality for anything they find lacking and share it with the world. Still, I couldn&rsquo;t help but think it should be possible to perform the work automatically. I don&rsquo;t mean &lsquo;script recurring tasks&rsquo; automatic, I mean &lsquo;set it, perform pentest, let me know how to patch the holes you found&rsquo; automatically. That&rsquo;s not to say I want the work to go away. The most exciting aspects of the work are this rare 15% of it that requires an insane amount of creativity and knowledge. You can read writeups from folks who have found seemingly invisible bugs that you would think don&rsquo;t have any impact at all, and used them to completely compromise applications and plunder their databases. If you don&rsquo;t believe me, the popularization of bug bounties have made it incredibly easy to see what kind of hacks are out there in the wild. Bug bounties allow hackers to make money for security bugs found within their applications or networks, and many organizations running the programs allow for writeups to be published after the fact. <a href=https://pentester.land/list-of-bug-bounty-writeups.html>It&rsquo;s humbling to read them</a>.</p><p>That other 85% or so can be a bit of a slog, though. There are several well known security issues that crop up time and time again. Finding them is always exciting in the way that all hacking is - you broke a thing that&rsquo;s not supposed to break! You have access to stuff you&rsquo;re not supposed to have! But it&rsquo;s not challenging or engaging, really. Is it possible to build tools that make all of security the fun part? And of course, the holy grail - is it possible to make an agent even better at penetration testing than humans?</p><p>But before we plot the future, let&rsquo;s see where we stand. How is ML being applied to security today?</p><h2 id=the-state-of-ml-in-defense>The state of ML in Defense<a hidden class=anchor aria-hidden=true href=#the-state-of-ml-in-defense>#</a></h2><p>Most machine learning naturally lends itself to defense, more than attack. There&rsquo;s actually been a pretty good amount of defensive tooling developed. And why not? The paradigms fit like a glove. As a defender your biggest problem is probably that you have too much information. Networks are just <em>happening</em> all the time, generating all sorts of traffic on all sorts of services. You&rsquo;re a human being with two eyes and a limited amount of caffeine to throw at the problem of perceiving incredibly granular logs. If you knew something bad was happening, you&rsquo;re probably educated enough to take an action, but how can you know? Frequently some scripted logic and a regular expression list can alert you of some well described dangers - imagine your database administrator logged in from an IP belonging to a country they don&rsquo;t live in and then changed their password - but not all dangerous situations are that well-described. What about stuff that&rsquo;s just <em>weird</em>?</p><p>These fall under the general bucket of <em>anomaly detection</em> as a problem. First, you gather a lot of data and group it into some sort of <em>observation</em> at a fidelity a model can interpret. Then, you run the observation through the model and get a boolean output. Either it&rsquo;s bad, and you alert a person, or it&rsquo;s good, and nothing happens. Think about it as a &ldquo;weird/not weird&rdquo; classifier. The intuition behind the perceptual task is stored within the dataset, and the algorithm transforms it into something that&rsquo;s augmenting a human&rsquo;s capabilities by taking cognitive load off of them.</p><p>If you&rsquo;re looking for something with a similar principle but more automated, all sorts of &ldquo;smart firewalls&rdquo; can be made this way. You learn what looks normal, train a network to recognize normal, and then if you&rsquo;re not normal you&rsquo;re an anomaly. The upside is big - if you detect an attack, you can take an action. The downside of a false alarm can be bad depending on the tooling, but as long as you&rsquo;re not overwhelmed with anomalies to look at a false positive is fine. At least in theory whatever you&rsquo;re looking at should be <em>anomalous</em> and therefore <em>interesting</em>.</p><p>In practice, this is challenging to pull off. What&rsquo;s normal for a network is a living, breathing thing. New people come in, they leave. New servers come on site. If configured poorly, all of these things can be anomalous. Training a network in a custom way is also challenging - you want to learn a good distribution of <em>normal</em> but for that to be legitimate you would need to know within a shadow of a doubt that your network is currently not compromised as you&rsquo;re training. Obviously, you have no idea whether that&rsquo;s the case or not and there&rsquo;s really no way to prove otherwise. So you have this sort of ontological problem for these types of detectors that&rsquo;s challenging to solve, at least at the network level.</p><p><a href=https://www.cylance.com/en-us/index.html>Cylance</a> claims to do this on the endpoint level, using AI to find malware processes on desktops and phones. There&rsquo;s not really a clear whitepaper that breaks down how, but it sounds <a href=https://s7d2.scene7.com/is/content/cylance/prod/cylance-web/en-us/resources/knowledge-center/resource-library/white-papers/Not_All_AI_Is_Created_Equal_White_Paper.pdf>pretty cool</a>. The approach for an endpoint anomaly detector seems equally sound to others in the anomaly detection paradigm - in each you find this distribution of process behavior that&rsquo;s normal or acceptable, and if you fall outside of that you can flag it and allow a user to make the call to override detection if it&rsquo;s a false positive.</p><p>You couldn&rsquo;t really call any of these tools <em>autonomous defenders</em> though. You don&rsquo;t have agents on the environment watching network traffic and taking actions in response to them. You might automatically put someone on a block list, or filter bad traffic (I too have scraped websites aggressively enough that I was hit with a captcha) but none of those tools are giving the Security Operations Center the day off to play golf. We don&rsquo;t have ourselves an &ldquo;autonomous defender&rdquo;, we have a fire alarm.</p><h2 id=the-state-of-ml-in-offense>The state of ML in Offense<a hidden class=anchor aria-hidden=true href=#the-state-of-ml-in-offense>#</a></h2><p>The state of things over on the offensive side is actually starting to catch up to defense, at least over the last couple of years. Attackers do a lot of enumerating resources, which is its own form of data collection (though it pales in comparison to the sheer volume of the defensive side).</p><p>They follow a very similar paradigm as well, actually. Except now <em>anomaly</em> means something different. On the offensive side it&rsquo;s &ldquo;Hey bud, that&rsquo;s a whole lotta attack surface to look at there. Want me to check it out and see if any tires seem worth kicking&rdquo;?</p><p>BishopFox&rsquo;s <a href=https://github.com/BishopFox/eyeballer>eyeballer</a> is actually a really cool example of one of these. Many security tools sniff HTTP endpoints of a target and screenshot them for you to review. Eyeballer goes that extra step forward and lets you apply classification to the problem. Run them through the classifier to find out if they&rsquo;re login pages, or they look like old custom code, whatever. It&rsquo;s a great example of taking a domain specific pentesting problem and making it fit into the classification paradigm.</p><p>There&rsquo;s been similar work done with text. I even found a language model used to do reconnaissance on a target&rsquo;s twitter and then use text models to customize messages with phishing links catered to them. This is a BlackHat talk from <a href=https://www.blackhat.com/docs/us-16/materials/us-16-Seymour-Tully-Weaponizing-Data-Science-For-Social-Engineering-Automated-E2E-Spear-Phishing-On-Twitter.pdf>ZeroFox</a>. As you might&rsquo;ve noticed, there are a lot of foxes in security consulting. But also, this is very much in line with what I was thinking of - an automated, intelligent tool to assist with security testing.</p><p>For the record, I think all of the tools I&rsquo;ve listed above are insanely cool and I would&rsquo;ve been proud to have worked on any of them. It is not a critique that none of them seem to fit the paradigm I&rsquo;m looking for: how would you go about developing an agent that could act autonomously? To be specific, the &lsquo;hello world&rsquo; of such an agent might look as follows:</p><p>How could you develop a system that had never seen <a href=https://information.rapid7.com/download-metasploitable-2017.html>Metasploitable</a> or similar vulnerable-by-design single hosts that could be placed on the same network as them, automatically enumerate information about, exploit, and extract data from them? If such a system was robust enough to handle many different intentionally vulnerable systems, it would be an <em>autonomous pentesting agent</em>.</p><h2 id=reinforcement-learning>Reinforcement Learning<a hidden class=anchor aria-hidden=true href=#reinforcement-learning>#</a></h2><p>If you&rsquo;re interested in AI, you&rsquo;ve probably heard of reinforcement learning. Even if you haven&rsquo;t heard it by that name, it&rsquo;s definitely been in the news. It&rsquo;s the paradigm that made <a href=https://deepmind.com/research/case-studies/alphago-the-story-so-far>AlphaGo</a> possible, and is the same paradigm that&rsquo;s helped OpenAI crush Atari scores for game after game. It&rsquo;s also made a bot that can play Smash Bros pretty <a href="https://www.youtube.com/watch?v=dXJUlqBsZtE">dang well</a>. But what is it? And how might it help us develop a system that can hack autonomously?</p><p>Broadly, reinforcement learning is the study of agents that learn by trial and error. Agents learn policies that direct them to take <em>actions</em> and then observe the change in <em>environments</em> and the <em>reward</em> they receive to inform their next action.</p><h3 id=multi-armed-bandits>Multi-Armed Bandits<a hidden class=anchor aria-hidden=true href=#multi-armed-bandits>#</a></h3><p>The classical non-deep example, the one a reader is most likely to have come across in the past, is the multi-armed bandit. The problem is a simple one: you find yourself in a casino. You stand in front of a slot machine with three arms. You&rsquo;re told that each of the arms has a different probability of success - some are luckier than others. Your goal is to find the best strategy to achieve the highest reward you can in a given number of arm pulls.</p><p>A naive approach might be to play with each arm many times. In fact, play each arm so many rounds you can eventually estimate the true probability of reward on the machine when the law of large numbers kicks in. Once you&rsquo;ve done this for each machine, you merely need to hang out on the machine that ended up with the highest reward probability, right? Easy peasy.</p><p>Those of you who have gone to a casino would surely retort that this is an inefficient and expensive strategy. Fine, then: let&rsquo;s introduce some definitions and try to use math to be a little more than lucky.</p><p>We have $n$ arms on the machine, and $t$ number of time steps to play the game. Each arm represents an action $a$ we can take. Our goal is to approximate the true success probability of each of the arms or $q(a)$ and then exploit that knowledge for reward.</p><p>We&rsquo;ve established we can&rsquo;t know the true reward, so we&rsquo;ll call our approximation $Q(a)$. Because this is an approximation based on our current understanding of the environment, and we&rsquo;re an intelligent agent that updates our beliefs based on our observations, it makes most sense to think about $Q_t(a)$, or our estimate valued of a given action at a given time step, $t$.</p><p>First, we know nothing about the environment, so we pull an arm at random. Let&rsquo;s say it gives us a reward! For one pull of the arm you&rsquo;ve gotten exactly one reward. What do you think about that machine&rsquo;s odds of success now?</p><p>Well, it makes the most sense to basically just keep a running list of how many times we&rsquo;ve tried the action, and what our total reward has been with the action. That&rsquo;s our estimated probability. Something like:</p><p>$$
Q_t(a) = \frac{R_1 + R_2 + &mldr; + R_{N_t(a)}}{N_t(a)}
$$</p><p>With this, we could keep a running best guess of the reward for each action.</p><p>But that&rsquo;s a lot of information to record. For a computer program, that means the memory needed for the program scales up linearly with the amount of time steps considered. In practice, we use something called a <em>q table</em> to keep the memory constant. I won&rsquo;t go into it too much here but you&rsquo;ll see it below in my python implementation. The idea is the same, which is to update $Q_t(a)$ at each timestep allowing it to become slowly more accurate.</p><p>So what is our strategy? A <em>greedy</em> strategy is just to read the action from the Q table that maximizes your reward:</p><p>$$
A_t = \arg\max Q_t(a)
$$</p><p>Remember, we already pulled a lever once and it yielded a reward. So that action is the only one in the Q table with a value over 0.0. So does that just mean we select that action over and over again, without ever trying the other arms? How do we know the other actions wouldn&rsquo;t give us even greater rewards?</p><p>This is the essence of the multi-armed bandit problem. To <em>exploit</em> our current knowledge of the environment to the best of our ability or <em>explore</em> to learn more about an action we don&rsquo;t currently understand very well.</p><p>To do this, we introduce $\epsilon$. Every $\epsilon%$ of the time, we will choose a random action instead of the action we know will yield us the most gain, observe our success or failure, and update our $Q_t(a)$ for that action.</p><p>Given a reasonable choice of $\epsilon$ and enough time steps, this allows us to converge on the best solution, even if our initial solution is not optimal.</p><p>We can examine this in code, as below:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Environment</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>p</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        p is the probability of success for each 
</span></span></span><span class=line><span class=cl><span class=s1>        casino arm
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>p</span> <span class=o>=</span> <span class=n>p</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>action</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        The agent pulls an arm and selects an action.
</span></span></span><span class=line><span class=cl><span class=s1>        The reward is stochastic - you only get anything 
</span></span></span><span class=line><span class=cl><span class=s1>        with the probability given in self.p for a given arm.
</span></span></span><span class=line><span class=cl><span class=s1>        
</span></span></span><span class=line><span class=cl><span class=s1>        action - the index of the arm you choose to pull
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=n>result_prob</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>()</span> <span class=c1># Samples from continuous uniform distribution</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>result_prob</span> <span class=o>&lt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>p</span><span class=p>[</span><span class=n>action</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Agent</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>actions</span><span class=p>,</span> <span class=n>eps</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        actions - The number of actions (arms to pull)
</span></span></span><span class=line><span class=cl><span class=s1>        
</span></span></span><span class=line><span class=cl><span class=s1>        eps - The frequency with which the agent will explore,
</span></span></span><span class=line><span class=cl><span class=s1>              rather than selecting the highest reward action
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>eps</span> <span class=o>=</span> <span class=n>eps</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_acts</span> <span class=o>=</span> <span class=n>actions</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>actions_count</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span> <span class=k>for</span> <span class=n>action</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>actions</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>Q</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span> <span class=k>for</span> <span class=n>action</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>actions</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>act</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>()</span> <span class=o>&lt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>eps</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1>#we explore</span>
</span></span><span class=line><span class=cl>            <span class=n>action</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>num_acts</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1>#we exploit</span>
</span></span><span class=line><span class=cl>            <span class=n>action</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>Q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>action</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>update_q_table</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>action</span><span class=p>,</span> <span class=n>reward</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>actions_count</span><span class=p>[</span><span class=n>action</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=n>step_size</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>actions_count</span><span class=p>[</span><span class=n>action</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>Q</span><span class=p>[</span><span class=n>action</span><span class=p>]</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>Q</span><span class=p>[</span><span class=n>action</span><span class=p>]</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>step_size</span><span class=p>)</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>Q</span><span class=p>[</span><span class=n>action</span><span class=p>]</span> <span class=o>+</span> <span class=n>step_size</span> <span class=o>*</span> <span class=n>reward</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>experiment</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>time_steps</span><span class=p>,</span> <span class=n>eps</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>    p is probabilities of success for arms
</span></span></span><span class=line><span class=cl><span class=s1>    time_steps - number of time steps to run experiment for
</span></span></span><span class=line><span class=cl><span class=s1>    epsilon to choose for agent
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>env</span> <span class=o>=</span> <span class=n>Environment</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>agent</span> <span class=o>=</span> <span class=n>Agent</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>p</span><span class=p>),</span> <span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>time_step</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>time_steps</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>action</span> <span class=o>=</span> <span class=n>agent</span><span class=o>.</span><span class=n>act</span><span class=p>()</span> <span class=c1># get action from agent</span>
</span></span><span class=line><span class=cl>        <span class=n>reward</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>action</span><span class=p>)</span> <span class=c1># take action in env</span>
</span></span><span class=line><span class=cl>        <span class=n>agent</span><span class=o>.</span><span class=n>update_q_table</span><span class=p>(</span><span class=n>action</span><span class=p>,</span> <span class=n>reward</span><span class=p>)</span> <span class=c1>#update with reward</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>agent</span><span class=o>.</span><span class=n>Q</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>q_table</span> <span class=o>=</span> <span class=n>experiment</span><span class=p>([</span><span class=mf>0.24</span><span class=p>,</span> <span class=mf>0.33</span><span class=p>,</span> <span class=mf>0.41</span><span class=p>],</span> <span class=mi>1_000_000</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>)</span>
</span></span></code></pre></div><p>The final q_table appears as <code>[0.2397833283177857, 0.3332216502695646, 0.41020130865076515]</code>, indicating we were pretty successful in estimating $q(a)$ with $Q_t(a)$.</p><p>So it&rsquo;s a simplistic example, but illustrates the power of reinforcement learning. Unlike a supervised learning example, we never told the system what the right answer was - the third level, with $q(a_3) = 0.41$. We enabled the agent to observe the effects of its actions to update its policy, and change its behavior.</p><p>If you want to read more about classic reinforcement learning, I highly recommend the extremely pleasant to read and extremely free <a href=https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf>Reinforcement Learning: An Introduction</a>. Hopefully this gentle introduction has convinced you there&rsquo;s an interesting power here, different from supervised or unsupervised learning methods you may have known in the past.</p><h2 id=the-successes-and-caveats-of-deep-reinforcement-learning>The Successes (and Caveats) of Deep Reinforcement Learning<a hidden class=anchor aria-hidden=true href=#the-successes-and-caveats-of-deep-reinforcement-learning>#</a></h2><p>Reinforcement learning allows for self-directed optimization. Deep learning allows for function approximation. By combining the two we&rsquo;re able to map environment state and action pairs into expected rewards.</p><h3 id=successes>Successes<a hidden class=anchor aria-hidden=true href=#successes>#</a></h3><p>I won&rsquo;t go too long here, because there&rsquo;s already plenty of hype. AlphaZero can play Go better than anyone has ever played Go, and through self-play eventually invented novel openings that human beings are now studying. Hard to overstate how mind-blowing that is. I think this was a pretty epoch defining event for anyone interested in AI in any field.</p><h3 id=caveats>Caveats<a hidden class=anchor aria-hidden=true href=#caveats>#</a></h3><p>Before I get into the weeds of the challenges deep reinforcement learning faces as a field, I&rsquo;d be remiss to not advise anyone interested to read Alex Irpan&rsquo;s <a href=https://www.alexirpan.com/2018/02/14/rl-hard.html>Deep Reinforcement Learning Doesn&rsquo;t Work Yet</a>. I&rsquo;ll be summarizing some of these points below, but the whole article is a sobering but ultimately optimistic read for those looking to cut their teeth on deep RL.</p><p>I&rsquo;ll be looking at each of these as challenges to be overcome for my own research: developing an autonomous pentesting agent.</p><h4 id=sample-inefficiency>Sample Inefficiency<a hidden class=anchor aria-hidden=true href=#sample-inefficiency>#</a></h4><p>One of the key problems in deep RL is sample inefficiency: that is, you need a <em>whole lot</em> of data to get good performance. The ratio of environment complexity to data required for strong performance can seem frighteningly high. For many environments, particularly real life ones, you&rsquo;re almost out of luck.</p><p>Even in my multi-armed bandit scenario, I ran 1,000,000 episodes. This was a pretty simple environment to learn from. Imagine training an agent against Metasploitable. You allow the agent to take action until the completion of the episode. Then you restart the virtual machine in a clean state, and begin again. Parallelizing this requires multiple virtual machines, and the time between episodes is as long as it takes to load up a fresh disk image - and that&rsquo;s for a single host! Full environments representing entire networks would be even harder to generate adequate experience for. Think about how long it takes you to spin up a fleet of boxes in Amazon, much less configure all the network policies. Brutal. For a single host, resetting metasploitable to a clean state a million times would take, optimistically, two minutes a pop. Doing that one million times? That would take about <em>4 years</em>.</p><p>So even if the method <em>could</em> work in principle, generating the data to overcome sample inefficiency is going to be tough.</p><h4 id=reward-function-design-is-tough>Reward Function Design is Tough<a hidden class=anchor aria-hidden=true href=#reward-function-design-is-tough>#</a></h4><p>Designing reward for Go is kinda easy. Collecting territory and winning? These things are good. Giving up territory and losing the game? This is very bad. Atari is pretty straightforward as well. Each of these games provide a score - if you make the score go up, you&rsquo;re doing well! If the score goes down, or you die, you&rsquo;re doing poorly.</p><p>Expressing those sorts of reward functions in simple environments mathematically is not extraordinarily difficult.</p><p>How about more subtle goals though? Take our goal of pentesting:</p><p>How do you define good pentesting? To do that, you&rsquo;d need to ask a good pentester what their goals are on an assessment. Since I don&rsquo;t have any on hand, my personal experience will have to suffice: good pentesting is about <em>careful thoroughness</em>.</p><p>For a real life attacker, your only goal is to find a single exploitable hole good enough to weasel your way into the network, find high-value information, and take off with it. Ideally without letting anyone know you were there. Sort of a depth-first search kinda deal.</p><p>Pentesting needs to be wide <em>and</em> deep. You want to present the client with evidence you looked over their network to the best of your ability, found as many chinks in their armor as possible at all levels of access you were able to achieve. And while doing this, you&rsquo;re under certain constraints. You can&rsquo;t break their network to discover a high value target. Some things are off limits, also known as out-of-scope. Also you have a fixed amount of time. So you can&rsquo;t explore <em>everything</em>. You have to provide breadth, and use your intuition to decide where to spend time going deep that will provide the biggest bang for the client&rsquo;s buck. That&rsquo;s good pentesting.</p><p>There are two kinds of rewards we might try: <strong>sparse rewards</strong> only provide reward at the end of the episode if the policy resulted in a &lsquo;success&rsquo;. The agent &ldquo;won&rdquo; the game. We&rsquo;re having a hard time defining success for pentesting if we use the above definition, but even if the answer was just &lsquo;got root access on a specific machine&rsquo; that likely wouldn&rsquo;t be enough. With so little to go off of, you can imagine a pentesting agent firing off some random scans, maybe trying some random exploits against random machines, and never receiving even a drop of reward for its trouble. The policy network has no valuable information to backprop on, and you&rsquo;re essentially dead stuck unless by some miracle the network chooses random actions that lead to success. As a former pentester, I can attest that I have tried that strategy and been very disappointed in it.</p><p>In this case, we need something more complicated. <strong>Shaped reward</strong> provides increasing rewards for states as they become closer to the end goal, rewarding actions that are useful. This sounds like a better fit for our problem. For example, scanning a potential target is not getting root on a high value target, but it&rsquo;s a useful step on the way, so we should give some reward there.</p><p>How would you express that as a reward function? Exploits are good! Discovering hosts, and information about hosts is also good. But we want to ensure we&rsquo;re not just brute-forcing throwing exploits at hosts to see if they work, so maybe we impose <em>noisiness cost</em> per action to encourage strategic exploits and scanning. How do we weigh the reward of exploit vs scanning? When it comes to information exfiltration, how do we teach an agent to understand what high-value vs low-value information is? We want the agent to understand <em>high-value</em> targets that deserve more intensive study, but how do we communicate that? In fact, we don&rsquo;t want to do that at all - we want the agent to discover that. Now how do you say that with math? When you try to piece these ideas into a singular reward function it gets hard quick.</p><h4 id=reward-functions-like-to-blow-up-in-your-face>Reward Functions like to Blow Up in Your Face<a hidden class=anchor aria-hidden=true href=#reward-functions-like-to-blow-up-in-your-face>#</a></h4><p>Agents do not care about your problems. They only care about the reward their actions can give them. Despite the elegant expressiveness of mathematics and your best personal efforts, there will probably be a gap between your intentions. In these gaps, the agent will attempt to find whatever action in the environment gives them the quick fix of reward without all the challenge of discovering a really useful policy.</p><p>OpenAI provides an infamous example in one of their experiments: in a boat racing game, they used a shaped reward. The agent got the most reward for winning, but they got partial reward for picking up powerups (useful for winning!) and passing checkpoints.</p><p>The agent quickly discovers you can get the most reward by just collecting the powerups, since they regenerate quickly. It finds itself stuck in a really elegant loop as its opponents whiz by. The agent will never win the race this way, and still get an incredible amount of reward. This is called <em>reward hacking</em>.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/tlOIHko8ySg?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="YouTube video"></iframe></div><p>Think about our previously proposed hodge-podge of actions that would give our hypothetical agent reward. It&rsquo;s easy to imagine an agent that had not yet penetrated the network finding a successful exploit that got it access to another machine. Great place to farm! The agent would likely just fire off that exploit again and again, and each success would give it more reward. The same could be said about a scan enumerating a host, or any number of activities. Without a carefully crafted reward, our proposed shaped reward could be easily &ldquo;hacked&rdquo;, with plenty of reward gained and our task undone.</p><h2 id=the-environment-challenge>The Environment Challenge<a hidden class=anchor aria-hidden=true href=#the-environment-challenge>#</a></h2><h3 id=state-space>State Space<a hidden class=anchor aria-hidden=true href=#state-space>#</a></h3><p>Another thing deep reinforcement learning requires is an environment. For a game like chess or shogi, this is just the board. It&rsquo;s pretty easy to gracefully represent as a matrix.</p><p>Defining a board for pentesting is kind of hard. You kind of start with a <em>fog of war</em> situation where you know about the perimeter of a network early on, but you really don&rsquo;t know the full size of the environment in terms of number of hosts until you find one. So it&rsquo;s an environment that starts small and gets bigger over time, with each new host found having different properties.</p><p>Most game environments are pretty fixed, so that&rsquo;s tough. It could be seen as a blessing, though. You&rsquo;re encouraged to overfit like crazy in reinforcement learning when generating experience in the game, often these learned skills don&rsquo;t transfer to a new environment. For penetration testing each &ldquo;game&rdquo; starts on a new network, or a new sized &ldquo;board&rdquo;. There&rsquo;s a general pattern of penetration testing that should stay consistent, but the shape of the network and hosts on it will define what your optimal actions are. Hopefully that keeps overfitting to a minimum.</p><h3 id=action-space>Action Space<a hidden class=anchor aria-hidden=true href=#action-space>#</a></h3><p>Your action space, the actions available to an agent that can be taken, also need to be provided. Chess, for example, this might be the legal moves your agent can take for any input board state.</p><p>There are <em>continuous</em> and <em>discrete</em> action spaces. Discrete action spaces basically just means a countable number of actions. The chess example applies here. Continuous action spaces might be found when you&rsquo;re using RL to set the specific value of a sensor, for example. Where the value of the sensor can take on any real-numbered value between a lower and upper bound. To be honest, I haven&rsquo;t totally wrapped my head around methods for continuous action spaces but I have seen a lot of clever problem formulation to make the action space <em>discrete</em> instead.</p><p>For example, take that sensor problem - pretty continuous. But what if we assume there&rsquo;s a minimum amount you can tune the sensor up or down that&rsquo;s meaningful? Call it $x$. Now, after taking an observation from our environment, let&rsquo;s say we only have two options - up or down by $x$. Well golly gee, sir, up or down? I ain&rsquo;t no mathematician but that&rsquo;s a pretty discrete space if I do say so myself.</p><p>This sort of judo is on display whenever the problem allows for it. When OpenAI tackled Dota 2, they easily could have considered the action space continuous - but they didn&rsquo;t. <a href=https://cdn.openai.com/dota-2.pdf>They discretized the action space on a per-hero basis</a>, arriving at a model choosing among 8,000 to 80,000 discrete actions depending on their hero. A discrete action space will be pried from their cold, dead hands.</p><p>That&rsquo;s a lot of moves. OpenAI had access to the game engine&rsquo;s API, so these actions were probably <em>read</em> rather than hand-coded. For our pentesting problem, how do we handle that? You&rsquo;re sitting in front of a terminal, where you can enter any text. A very minuscule part of the distribution of all text you can type into a terminal is going to be valuable for accessing your hacking tools. Within those tools, there&rsquo;s very specific syntax that will be valuable. That&rsquo;s a pretty big action space, and I&rsquo;m not sure we can specify reward that will make that valuable, even shaped. So what&rsquo;s the play?</p><h3 id=metasploit-api-the-game-engine-of-pentesting>Metasploit API: The &lsquo;game engine&rsquo; of pentesting<a hidden class=anchor aria-hidden=true href=#metasploit-api-the-game-engine-of-pentesting>#</a></h3><p>I puzzled over this for a long time before I did some literature review and found Jonathan Schwartz&rsquo;s thesis <a href=https://jjschwartz.github.io/files/2018_CS_honours_thesis.pdf>Autonomous Penetration Testing using Reinforcement Learning</a>. In it, he creates a pretty convincing partially observable Markov decision process to form a model of penetration testing. It&rsquo;s one of the few real attempts I&rsquo;ve seen to tackle the formulation of the problem. One line in particular really inspired me to take a serious look at the problem again. While justifying some simplifications to his network model, Jonathan says:</p><blockquote><p>The specific details of performing each action, for example which port to communicate with, are details that can be handled by application specific implementations when moving towards higher fidelity systems. Penetration testing is already moving in this direction with frameworks such as metasploit which abstract away exactly how an exploit is performed and simply provide a way to find if the exploit is applicable and launch it, taking care of all the lower level details of the exploit</p></blockquote><p>First, this struck me as an oversimplification. How many times had I loaded up an exploit in metasploit only to have it not work? Then I had to dig into the specifics of the Ruby code and twiddle with things. Many exploits also have a pretty large number of required arguments to set that require some domain/target specific knowledge. Then I decided this was totally genius. That insanely large action space of the open terminal now starts to more resemble a game board. Metasploit stores information about hosts it knows about, their open services and distribution information. Exploits apply to specific distributions and services. Metasploit even provides tools for information gathering once you&rsquo;ve compromised your host. It&rsquo;s not always enough - often you need to break out of their laundry list of commands and use an honest-to-god terminal. But there&rsquo;s a <em>lot</em> you can do restricting the action space to the Metasploit level. I haven&rsquo;t done the back of the envelope math, but that feels like Dota 2 size action space to me, maybe smaller.</p><p>The actions you can take with Metasploit, and the information it chooses to store reduces the complications in considering both the <em>action space</em> and the <em>state space</em> of penetration testing.</p><h2 id=related-safety-problems>Related Safety Problems<a hidden class=anchor aria-hidden=true href=#related-safety-problems>#</a></h2><p>Solving penetration testing would also involve, as a sub-problem, solving a variety of safety problems. Not safety as in &ldquo;paper clip AGI destroys humanity&rdquo; but several of the problems described in OpenAI&rsquo;s <a href=https://arxiv.org/pdf/1606.06565.pdf>Concrete Problems in AI Safety</a>. It&rsquo;s essentially a review of practical research problems that can be broadly categorized around AI safety. Some of them are practically necessary to solve before you can design an agent that could be truly an autonomous attacker or defender.</p><p>In the paper <em>safe exploration</em> is broadly defined as ensuring the &ldquo;exploration&rdquo; side of exploitation vs exploration is sensitive to how it explores such that it doesn&rsquo;t take extremely risky &rsquo;exploratory&rsquo; actions. This is part of being subtle in penetration testing engagements. If you explore loudly (a super fast, full network scan) you&rsquo;ll probably get lots of information quickly, but you&rsquo;re also likely to set off the SOC&rsquo;s alarms and are liable to get your IP blocked. Every pentester has experienced the walk of shame, having tripped the alarms before the engagement has scarcely begun and writing a tepid email about how it would be oh-so kind of the SOC to unblock an IP range, yes, thank you, sorry.</p><p><em>Avoiding negative side effects</em> is defined as not disturbing the environment in negative ways while pursuing its goals. This is absolutely huge on an engagement. More than once I&rsquo;ve been in a situation where I&rsquo;m fairly certain I have an exploit that could gain me access to the server. Trouble is, while this exploit will give me a shell, it will also crash the service. Not only will this definitely trip some alarms, but if the service isn&rsquo;t set to auto-start you could cause an outage. Disturbing the day-to-day work of the client during a penetration test is a sure fire way to make sure you never work with them again. The question of whether we can teach an agent this sort of discretion without manually specifying all the things that it shouldn&rsquo;t disturb is a challenging question with no immediate answers.</p><p>Both of these are generally problems that exist in more complex environments than the ones RL has succeeded in so far, and would certainly need to be solved before any of these agents with any influence over the real-world environment could be released in the wild.</p><h2 id=simulation-as-a-path-forward>Simulation as a path forward<a hidden class=anchor aria-hidden=true href=#simulation-as-a-path-forward>#</a></h2><p>If you&rsquo;ve read this far, you might be under the impression I have a pretty negative view of the odds of solving penetration testing with RL. Nothing could be further from the truth! I&rsquo;m just being honest about the many, potentially very thorny, sub-problems on the way to that solution.</p><p>To me, the immediate work to be done is in the simulation space. One has to choose a subset of Metasploit actions directly from their API and map them to actions an agent can take.</p><p>There&rsquo;s still the problem of sample inefficiency - how do you generate enough experience?</p><p>The answer has to be simulation. Instead of interacting with a full virtual machine environment, you need a simulated environment that makes it easy for an agent to quickly test a policy against an environment. The way the network is composed needs to be, to my mind, similar to a rogue-like game. We want procedurally generated vulnerable networks at a just realistic enough fidelity for policies learned to apply to a real network. These could be spun up and down quickly and easily parallelized to achieve the kind of massive experience generation achieved by OpenAI with Dota 2.</p><p>The aforementioned Jonathan Schwartz has already developed a simulator that I believe steps in that direction, and extending it would certainly make a good environment for the metasploit-driven agent I&rsquo;m picturing.</p><p>For now, I need to consider the design of the subset of metasploit actions that would make an acceptable action space for solving non-trivial vulnerable networks. Achieving an acceptable fidelity for the simulation is also key - but to me it&rsquo;s just the minimum viable environment that allows the metasploit action APIs to be meaningful.</p><p>In a future post, I&rsquo;ll take my first steps using the OpenAI Gym framework to develop a simple environment I can train one of their prewritten models on. Whatever the final shape of the simulator, I believe making sure it fits within the OpenAI gym framework popularized by researchers at the forefront of RL is the best way to get new eyes onto the project. It&rsquo;s also a good way for me to get some experience with DRL tooling.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/reinforcement-learning/>Reinforcement-Learning</a></li><li><a href=http://localhost:1313/tags/penetration-testing/>Penetration-Testing</a></li><li><a href=http://localhost:1313/tags/autonomous-agents/>Autonomous-Agents</a></li><li><a href=http://localhost:1313/tags/deep-learning/>Deep-Learning</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/writing/infosecs-data-problem/><span class=title>« Prev</span><br><span>Infosec's Data Problem</span>
</a><a class=next href=http://localhost:1313/writing/oscp-review/><span class=title>Next »</span><br><span>An ML Eng's Review of OSCP</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Shane Caldwell</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><style>.copy-code{display:inline-flex;align-items:center;justify-content:center;width:32px;height:32px;background:var(--tertiary);border:1px solid var(--border);border-radius:6px;color:var(--secondary);cursor:pointer;transition:all .2s ease;position:absolute;top:8px;right:8px;z-index:10}.copy-code:hover{background:var(--secondary);color:var(--theme)}.copy-code svg{width:16px;height:16px}.copy,.highlight .copy{display:none!important}pre{position:relative}</style><script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll('.copy, [class*="copy"]').forEach(e=>{e.classList.contains("copy-code")||e.remove()});const e=document.querySelectorAll("pre");e.forEach(e=>{const t=e.cloneNode(!0);e.parentNode.replaceChild(t,e)}),document.querySelectorAll("pre code").forEach(e=>{const n=e.parentElement;if(n.querySelector(".copy-code"))return;const t=document.createElement("button");t.classList.add("copy-code"),t.setAttribute("aria-label","Copy code"),t.setAttribute("type","button");const s=`<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"/><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"/></svg>`,a=`<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6L9 17l-5-5"/></svg>`;t.innerHTML=s;function o(){t.innerHTML=a,t.style.color="#10b981",setTimeout(()=>{t.innerHTML=s,t.style.color=""},2e3)}t.addEventListener("click",function(t){t.preventDefault(),t.stopPropagation();const n=e.textContent||e.innerText;navigator.clipboard?navigator.clipboard.writeText(n).then(()=>{o()}).catch(()=>{i(n)}):i(n)});function i(e){const t=document.createElement("textarea");t.value=e,t.style.position="fixed",t.style.opacity="0",document.body.appendChild(t),t.select();try{document.execCommand("copy"),o()}catch(e){console.error("Copy failed:",e)}document.body.removeChild(t)}n.appendChild(t)})})</script><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>.footnote-popup{position:absolute;background:var(--theme);border:1px solid var(--border);border-radius:6px;padding:12px 16px;max-width:300px;font-size:.85em;line-height:1.4;z-index:1000;box-shadow:0 4px 12px rgba(0,0,0,.15);font-family:var(--font-mono);color:var(--primary);display:none;pointer-events:auto;word-wrap:break-word}.dark .footnote-popup{background:#2d2d2d;box-shadow:0 4px 12px rgba(0,0,0,.3)}.footnote-popup::before{content:'';position:absolute;top:-6px;left:50%;transform:translateX(-50%);width:12px;height:12px;background:var(--theme);border:1px solid var(--border);border-bottom:none;border-right:none;rotate:45deg}.dark .footnote-popup::before{background:#2d2d2d}.footnote-ref{text-decoration:none!important;font-weight:600;padding:2px 6px;border-radius:4px;background:var(--tertiary);transition:all .2s ease;position:relative;color:var(--primary)!important}.footnote-ref:hover{background:var(--secondary);transform:translateY(-1px);box-shadow:0 2px 4px rgba(0,0,0,.1)}.dark .footnote-ref:hover{box-shadow:0 2px 4px rgba(0,0,0,.3)}</style><script>document.addEventListener("DOMContentLoaded",function(){let e=null,t=null;function s(e,t){const n=document.createElement("div");return n.className="footnote-popup",n.innerHTML=t,document.body.appendChild(n),n}function o(n,s){t&&(clearTimeout(t),t=null);const i=n.getBoundingClientRect(),r=s.getBoundingClientRect();let o=i.left+i.width/2-s.offsetWidth/2,a=i.top-s.offsetHeight-10;o<10&&(o=10),o+s.offsetWidth>window.innerWidth-10&&(o=window.innerWidth-s.offsetWidth-10),a<10&&(a=i.bottom+10),s.style.left=o+window.scrollX+"px",s.style.top=a+window.scrollY+"px",s.style.display="block",e=s}function n(){t=setTimeout(()=>{e&&(e.style.display="none",e=null)},150)}document.querySelectorAll("a.footnote-ref").forEach(e=>{const a=e.getAttribute("href"),r=document.querySelector(a);if(!r)return;const c=r.innerHTML.replace(/<a[^>]*href="#fnref[^"]*"[^>]*>.*?<\/a>/g,"").trim();if(!c)return;const i=s(a,c);e.addEventListener("mouseenter",()=>{o(e,i)}),e.addEventListener("mouseleave",n),i.addEventListener("mouseenter",()=>{t&&(clearTimeout(t),t=null)}),i.addEventListener("mouseleave",n)}),window.addEventListener("scroll",()=>{e&&(e.style.display="none",e=null)})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>