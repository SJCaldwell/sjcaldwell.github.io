<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>All Reduce Across the Atlantic: Bandwidth in Decentralized Training | Shane Caldwell</title>
<meta name=keywords content="llms,training,research,decentralized"><meta name=description content="The practical realities of devestatingly high communication cost in training."><meta name=author content="Shane Caldwell"><link rel=canonical href=http://localhost:1313/writing/all-reduce-across-atlantic/><link crossorigin=anonymous href=/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon-180x180.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/writing/all-reduce-across-atlantic/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=icon type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon-180x180.png><link rel=apple-touch-icon href=/apple-touch-icon.png><link rel="shortcut icon" href=/favicon.ico><meta name=msapplication-TileColor content="#000000"><meta name=theme-color content="#000000"><script data-goatcounter=https://sjcaldwell.goatcounter.com/count async src=//gc.zgo.at/count.js></script><style>:root{--font-mono:'SF Mono', 'Monaco', 'Inconsolata', 'Roboto Mono', 'Source Code Pro', 'Menlo', 'Consolas', monospace}body,html{font-family:var(--font-mono)!important}*:not(mjx-container):not(mjx-container *),*:not(mjx-container):not(mjx-container *)::before,*:not(mjx-container):not(mjx-container *)::after{font-family:var(--font-mono)!important}mjx-container{overflow-x:visible!important;overflow-y:visible!important;overflow:visible!important}mjx-container[display=true]{display:block!important;text-align:center!important;margin:1em auto!important;max-width:100%!important}mjx-container[display=true] mjx-math{display:inline-block!important;text-align:left!important}mjx-container mjx-mtable{display:table!important;margin:0 auto!important;width:auto!important}mjx-container mjx-mtr{display:table-row!important;height:auto!important}mjx-container mjx-mtd{display:table-cell!important;padding:.3em .8em!important;vertical-align:middle!important}mjx-container[display=true] mjx-mtable mjx-mtr{display:table-row!important;white-space:nowrap!important}mjx-container[display=true] mjx-mtable{display:table!important;border-collapse:separate!important;border-spacing:0 .3em!important}mjx-container::-webkit-scrollbar{display:none!important}mjx-container{-ms-overflow-style:none!important;scrollbar-width:none!important}.dark{--primary:#ffffff;--secondary:#e5e5e5;--tertiary:#cccccc}body:not(.dark){--primary:#000000;--secondary:#333333;--tertiary:#666666}.dark a{color:#fff!important;text-decoration:underline}.dark a:hover{color:#e5e5e5!important}code,pre{font-family:var(--font-mono)!important;font-size:.9em}h1,h2,h3,h4,h5,h6{font-weight:700!important;color:var(--primary)!important}.post-meta{color:var(--secondary)!important}button,.button{font-family:var(--font-mono)!important;font-weight:500}.paper-card,.talk-card{background:var(--theme);border:1px solid var(--border);border-radius:8px;padding:24px;margin-bottom:24px;transition:all .2s ease;box-shadow:0 2px 4px rgba(0,0,0,5%)}.paper-card:hover,.talk-card:hover{border-color:var(--secondary);box-shadow:0 4px 8px rgba(0,0,0,.1);transform:translateY(-1px)}.dark .paper-card,.dark .talk-card{background:#1a1a1a;border-color:#333;box-shadow:0 2px 4px rgba(0,0,0,.2)}.dark .paper-card:hover,.dark .talk-card:hover{border-color:#555;box-shadow:0 4px 8px rgba(0,0,0,.3)}.paper-title{margin:0 0 12px!important;font-size:1.25em;line-height:1.3}.paper-title a{color:var(--primary)!important;text-decoration:none;border-bottom:2px solid transparent;transition:border-color .2s ease}.paper-title a:hover{border-bottom-color:var(--primary)}.paper-meta{margin-bottom:16px;font-size:.9em}.paper-authors{color:var(--secondary);margin-bottom:4px;font-weight:500}.paper-date{color:var(--tertiary);font-size:.85em}.paper-abstract{color:var(--primary);line-height:1.5}.paper-abstract p{margin:0}.talk-title{margin:0 0 12px!important;font-size:1.25em;line-height:1.3}.talk-collaborators{margin-bottom:16px;font-size:.9em;color:var(--secondary)}.talk-collaborators p{margin:0}.talk-details{display:flex;flex-direction:column;gap:8px}.talk-event,.talk-recording{font-size:.9em}.talk-recording a{color:var(--primary)!important;text-decoration:underline}.talk-recording a:hover{color:var(--secondary)!important}@media(max-width:768px){.paper-card,.talk-card{padding:16px;margin-bottom:16px}}.twitter-tweet{margin:24px auto!important;max-width:550px!important}.post-content blockquote.twitter-tweet,.post-content div:has(.twitter-tweet){display:flex;justify-content:center;margin:24px 0}.post-content .twitter-tweet iframe{margin:0 auto;display:block}.post-content figure{margin:24px 0;text-align:center}.post-content figure img{margin-bottom:12px;border-radius:6px;box-shadow:0 4px 8px rgba(0,0,0,.1);max-width:100%;height:auto}.dark .post-content figure img{box-shadow:0 4px 8px rgba(0,0,0,.3)}.post-content figcaption{font-family:var(--font-mono)!important;font-size:.85em;color:var(--secondary);font-style:italic;line-height:1.4;margin-top:8px;padding:0 16px}.post-content figcaption p{margin:0;text-align:center}@media(max-width:768px){.post-content figcaption{font-size:.8em;padding:0 8px}.post-content ol{padding-left:24px!important;margin-left:4px!important}.post-content .footnote-ref{margin-left:2px!important;margin-right:3px!important}.post-content{padding-left:20px!important;padding-right:20px!important}.main{padding-left:8px!important;padding-right:8px!important}}</style><meta property="og:url" content="http://localhost:1313/writing/all-reduce-across-atlantic/"><meta property="og:site_name" content="Shane Caldwell"><meta property="og:title" content="All Reduce Across the Atlantic: Bandwidth in Decentralized Training "><meta property="og:description" content="The practical realities of devestatingly high communication cost in training."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="writing"><meta property="article:published_time" content="2025-12-12T00:00:00+00:00"><meta property="article:modified_time" content="2025-12-12T00:00:00+00:00"><meta property="article:tag" content="Llms"><meta property="article:tag" content="Training"><meta property="article:tag" content="Research"><meta property="article:tag" content="Decentralized"><meta property="og:image" content="http://localhost:1313/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/"><meta name=twitter:title content="All Reduce Across the Atlantic: Bandwidth in Decentralized Training "><meta name=twitter:description content="The practical realities of devestatingly high communication cost in training."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Writing","item":"http://localhost:1313/writing/"},{"@type":"ListItem","position":2,"name":"All Reduce Across the Atlantic: Bandwidth in Decentralized Training ","item":"http://localhost:1313/writing/all-reduce-across-atlantic/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"All Reduce Across the Atlantic: Bandwidth in Decentralized Training ","name":"All Reduce Across the Atlantic: Bandwidth in Decentralized Training ","description":"The practical realities of devestatingly high communication cost in training.","keywords":["llms","training","research","decentralized"],"articleBody":"Since implementing DiLoCo I’ve been really interested in algorithms for decentralized training. The more LLMs I train in a “normal” setting, the more interesting to me the decentralized case is. Mostly because seemingly nothing wants to work. So much of the software you can find off the shelf for training is optimized for centralized compute. For good reason, this is the default case. Most people implementing high performance algorithms to train large models are trying to optimize FLOPs, and are doing so in some kind of tightly connected datacenter. It is extremely difficult to keep FLOPs high when you’re spending time communicating data. As it is on a single-node, so it is in many.\nI implemented DiLoCo, but I tested the algorithm with a cluster of two nodes on Modal. This effectively tests the algorithmic implementation and speaks a little bit to the communication costs of the all-reduce, but fails to capture a real global training scenario.\nWhat can go wrong in a decentralized training scenario? Let’s consider just the effects decentralized training has on data parallelism, where each island of compute has access to one model replica. Compute island bandwidth can be variable between nodes. The actual compute can be heterogenous, so the same pipeline and parallelism strategies may not work on each island, requiring a custom setup fit for it, which besides just having different FLOPs may have different memory constraints leading to different minibatch sizes. This would lead certain islands to run ahead of others, so what do you do at the synchronization step? You could wait, but then your faster and probably more expensive compute is hanging waiting for the stragglers. You could try to move ahead, but now the lagging worker is training off of a stale replica and won’t catch up anytime soon, and your currently training models are missing information from the dataset you sent to the laggers. Even if the compute was homogenous, there’s latency of the islands to each other.\nThere’s a lot of complicated questions to dig into there, but I’ll start by looking at an extremely basic one: Given two nodes on opposite sides of the Atlantic, what’s the theoretical bandwidth ceiling, and how close can we get to it for gradient synchronization?\nBefore we get started, it’s worth noting some numbers, bandwidth-wise, for doing data parallel training in a centralized setting.\nBig boy datacenter numbers NVLink for GPUs on the same node: 1,800 GB/s\nPCIe on the same node: 32 GB/s\nNVSwitch for nodes on the same rack: 1,800 GB/s\nInfiniBand on different racks: 50-100 GB/s\nIndie numbers High-end home fiber: 1.25 GB/s\nTypical US Home Internet: 0.026 GB/s\nAnd to understand the size of the data we’d be moving around by default in data parallel training, we can consider the “default” case of passing the weight gradients around in FP32.\n10B parameters x 32 bits = 40 GB1.\nSo if we were fully saturating the bandwidth, our time to transfer (latency disregarded) would be:\nNVLink: 22ms Infiniband: 800ms PCIe 4.0: 1.25 seconds High-end home fiber: 32 seconds Typical US Home Internet: 26 minutes We’re bandwidth bottlenecked every time our communication time exceeds compute time. If we assume a training step finishes in something like 500ms, we’re communication bound by our theoretical max bandwidth as soon as we’re going over anything with a smaller pipe than NVLink, and that’s before we consider latency.\nMost importantly, we can think of this as primarily effecting MFU, our main measure of training efficiency. Our theoretical FLOPs are defined by the GPU. Any time blocking would essentially be overhead, so our actual achieved FLOPs go down and MFU tanks.\nLet’s briefly look at the all-reduce operation we’re doing:\nWhat’s All-Reduce for? Let’s look at the dumb version.\nimport torch.distributed as dist def sync_gradients(self): \"\"\" Call before optimizer step \"\"\" if not self.sync_grads: return for param in self.model.parameters(): if param.grad is not None: dist.all_reduce(param.grad, op=dist.ReduceOp.AVG) torch.cuda.synchronize() When we’re training, each replica is working on different data. If they weren’t, they would have no novel information to exchange with each other and we’d just be training the same model on each. We’re accumulating gradients over minibatches, and building up a local gradient state. Before we run our optimizer step, we want to capture global information from the rest of the replicas about their local gradients.\nAll-reduce is a standard distributed computing primitive that lets us take as input some data that is different for each node running the same program (our local gradients) apply some function to them (in this case, averaging) and return the result (the global gradients) back to each program.\nThis means that the value of param.grad for each param now contains the global average of the local gradients, and each program now has the same understanding of the gradient. Now when you take your gradient step, each of them will now be synced, and they will continue to work through their unique batch of data from that new starting point.\nSo, in that one line, dist.all_reduce(param.grad, op=dist.ReduceOp.AVG) we’re doing all of that. That’s a lot of invisible networking and computation happening in a one liner. What’s doing that? Where is the computation occurring? How are the nodes connecting to each other?\nWell, there are several possible libraries that implement those ‘collective communication’ operations.\nNCCL If you remember nothing else about NCCL, remember that it’s pronounced “nickel”2. It stands for “NVIDIA Collective Communications Library”. The big advantage with NCCL is that it supports GPU to GPU communication and it’s optimized for it. That is, when you do all-reduce, the data does not have to be written to CPU. Intra-node, the data can move literally just between GPU memory buffers. Inter-node, you can write from the GPU to the network interface card to the network, the other nodes networking card, and then its GPU. In either case, no CPU.\nNCCL itself doesn’t handle discovery. torchrun/torch.distributed handles that instead. When Pytorch runs init_process_group() it gets all the processes to check in. Once each rank (process) has joined, they exchange information, and then NCCL establishes direct communication channels to be used when an all-reduce or other collective communication operation is invoked.\nAnother question - how do those messages actually get passed? It turns out this is dependent on what’s available. There’s a series of fallbacks implemented to make sure you get the most efficient possible available communication.\nFor the same node - If you’ve got NVLink, you can do GPU-to-GPU interconnect.\nBetween nodes, you’ll use Infiband with GPUDirect RDMA (Remote Direct Access Memory), so you sort of treat your GPUs as one big GPU for the purposes of communication.\nIf you don’t have Infiniband, you’ll do RDMA over ethernet.\nThis is all, for decreasing amounts of bandwidth, hyper-efficient and assumes a lossless networking setup. And if all that fails (which it would, in the transcontinental case) you fall back to TCP/IP sockets, which have a lot of algorithms for handling the fact that data is lossy and needs to arrive in specific order. NCCL still technically works, but you’ll pretty much never hear about this case because the overhead is high and it’s kind of a crazy thing to do.\nGloo Gloo is not an acronym. I think it’s pronounced “glue”3.\nGloo is, by default, over TCP/IP. The rendezvous mechanism expectation is the same as with NCCL (handled by torchrun) and requires all participating processes to be reachable via a bidirectional network interface.\nGloo is generally known as being intended for CPU-based models. The advice you’ll get from reasonable sources is that if you’re doing GPU training, you should be using NCCL, and if you’re doing CPU training, you should be using Gloo. The Pytorch Docs get slightly more specific, reading:\n[For GPU hosts with Ethernet interconnect…] - Use NCCL, since it currently provides the best distributed GPU training performance, especially for multiprocess single-node or multi-node distributed training. If you encounter any problem with NCCL, use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.)\nINTELLECT-1 LLM training is obviously a GPU activity, so just based on these references it seems like NCCL should still be the right call. But, when we read the INTELLECT-1 Technical Report, where Prime Intellect trained a 10B parameter model using decentralized compute, we read the following:\nWe utilize a VPN because our inter-node collective communication library, Gloo, requires all participating processes to be reachable via a private network interface. Additionally, the VPN ensures secure, encrypted communication between nodes.\nIntra-node, they use NCCL as you would expect.\nFyi it's still use nccl for intra GPU communication. Gloo is used for the all reduce across the world\n— samsja (@samsja19) October 19, 2024 So what’s the deal?\nThe answer is actually really obvious, and specific to DiLoCo: Prime is legitimately interested in CPU, not GPU, operations during DiLoCo. As I discussed in my DiLoCo blog, the algorithm requires essentially two copies of your model.\nThe model being optimized on your local dataset, being run through the “inner optimizer”.\nA replica of the last time you did a global sync, being run through the “outer optimizer.”\nStoring both of those on GPU would be unnecessarily expensive, and you’re really not doing anything with the reference model (not pushing tokens or anything), so you offload it that one to CPU.\nMeanwhile, your inner optimizer step is plowing away, updating weights anywhere from 100 to 500 times, for some tunable parameter $H$.\nNow we get to the interesting part, which is after $H$ steps have been completed. Now you want each replica to share data. You essentially create a pseudo gradient, by looking at the difference between the weights reference model parameters you’ve got stored in CPU against the parameters you’ve ended up with after $H$ steps.\nThe detail I missed in my last blog was this: I assumed you would perform the all-reduce operation on the GPU. That is, you would calculate the gradients on GPU, and then call all-reduce, so you’d use NCCL. The INTELLECT-1 repo doesn’t do this, instead it keeps all of the outer model stuff on CPU. The optimizer lives there, and the gradients live there.\nThey note this is primarily to avoid VRAM overhead, which I don’t entirely understand. It makes sense to not want two copies of the model on GPU, but the all-reduce seems like it could happen on either GPU or CPU without complaint. In-particular when training LLMs, your memory is dominated by the activations moreso than the weights or gradients, so when you’re going to block the GPU anyway until the all-reduce is complete and you’re ready to start training again, you should have plenty of available VRAM. My current assumption is this is attractive/advantageous because recovering from failure with gloo is easier than handling it on the GPU, but I’m just making that up. It would also be reasonable to me if the sharding from FSDP2 made the VRAM situation more tenuous. If anyone knows, tell me!\nSo, after a lot of waffling, it seems that it is as simple as your GPU all-reduce backend should be NCCL, and your CPU all-reduce backend should be Gloo. It’s just that you might choose to have some surprising computations on CPU depending on your use-case.\nSo, we’ve got the reported numbers and we understand why doing the all-reduce over CPU is the right choice, making Gloo a natural fit. Let’s see what our bandwidth is between American and Europe and whether Gloo can saturate it during an all-reduce.\nIntercontinental Bandwidth Using Prime Intellect I picked up two CPU nodes - one in the United States and one in France. After establishing their Public IPs and connection between them, I wanted to take a look at the bandwidth.\nTo do that, I used iperf3, a tool for determining the maximum achievable bandwidth on IP networks. There are several tunable parameters, but for our part we were most interested in -P the number of parallel streams we run at once.\nI started with one stream.\n[ 5] local 192.168.0.118 port 39636 connected to 72.46.85.115 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 6.51 MBytes 54.6 Mbits/sec 0 3.33 MBytes [ 5] 1.00-2.00 sec 12.5 MBytes 105 Mbits/sec 1315 112 KBytes [ 5] 2.00-3.00 sec 11.2 MBytes 94.4 Mbits/sec 1017 1.55 MBytes [ 5] 3.00-4.00 sec 13.8 MBytes 115 Mbits/sec 0 1.64 MBytes [ 5] 4.00-5.00 sec 13.8 MBytes 115 Mbits/sec 0 1.71 MBytes [ 5] 5.00-6.00 sec 16.2 MBytes 136 Mbits/sec 0 1.76 MBytes [ 5] 6.00-7.00 sec 15.0 MBytes 126 Mbits/sec 0 1.79 MBytes [ 5] 7.00-8.00 sec 15.0 MBytes 126 Mbits/sec 0 1.81 MBytes [ 5] 8.00-9.00 sec 16.2 MBytes 136 Mbits/sec 0 1.82 MBytes [ 5] 9.00-10.00 sec 16.2 MBytes 136 Mbits/sec 0 1.83 MBytes [ 5] 10.00-11.00 sec 16.2 MBytes 136 Mbits/sec 0 1.83 MBytes [ 5] 11.00-12.00 sec 15.0 MBytes 126 Mbits/sec 0 1.83 MBytes [ 5] 12.00-13.00 sec 16.2 MBytes 136 Mbits/sec 0 1.83 MBytes [ 5] 13.00-14.00 sec 16.2 MBytes 136 Mbits/sec 0 1.84 MBytes [ 5] 14.00-15.00 sec 16.2 MBytes 136 Mbits/sec 0 1.86 MBytes [ 5] 15.00-16.00 sec 15.0 MBytes 126 Mbits/sec 0 1.89 MBytes [ 5] 16.00-17.00 sec 17.5 MBytes 147 Mbits/sec 0 1.94 MBytes [ 5] 17.00-18.00 sec 17.5 MBytes 147 Mbits/sec 0 2.01 MBytes [ 5] 18.00-19.00 sec 16.2 MBytes 136 Mbits/sec 0 2.09 MBytes [ 5] 19.00-20.00 sec 15.0 MBytes 126 Mbits/sec 226 1.56 MBytes [ 5] 20.00-21.00 sec 11.2 MBytes 94.4 Mbits/sec 204 1.18 MBytes [ 5] 21.00-22.00 sec 10.0 MBytes 83.9 Mbits/sec 0 1.25 MBytes [ 5] 22.00-23.00 sec 11.2 MBytes 94.4 Mbits/sec 0 1.31 MBytes [ 5] 23.00-24.00 sec 12.5 MBytes 105 Mbits/sec 0 1.36 MBytes [ 5] 24.00-25.00 sec 11.2 MBytes 94.4 Mbits/sec 0 1.38 MBytes [ 5] 25.00-26.00 sec 12.5 MBytes 105 Mbits/sec 0 1.40 MBytes [ 5] 26.00-27.00 sec 12.5 MBytes 105 Mbits/sec 0 1.41 MBytes [ 5] 27.00-28.00 sec 12.5 MBytes 105 Mbits/sec 0 1.41 MBytes [ 5] 28.00-29.00 sec 11.2 MBytes 94.4 Mbits/sec 0 1.41 MBytes [ 5] 29.00-30.00 sec 12.5 MBytes 105 Mbits/sec 0 1.41 MBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-30.00 sec 415 MBytes 116 Mbits/sec 2762 sender [ 5] 0.00-30.11 sec 414 MBytes 115 Mbits/sec receiver Our bitrate here, for one stream, is 116 Mbits/sec or 0.0145 GB/s For a 10GB gradient sync, we’re looking at around 689.655 seconds, or around 11.5 minutes for the transfer. Prime Intellect notes that their gradient syncs, which involved more nodes and more data for the ring all-reduce, took “around 1 to 7 minutes depending on the configuration”. They were also using tailscale, which they note gave a performance hit. So what gives? Did we just pick really bad nodes?\nThe INTELLECT-1 blog has an additional detail under “Maximising4 bandwidth utilization”:\nBy sharding our DiLoCo pseudo-gradients in a node, we can maximise network bandwidth utilization by opening multiple connections at the same time when performing the all-reduce. This yielded a transfer speed improvement of 8x on some nodes.\nSo the idea is basically that instead of calling one all-reduce with a single TCP stream prone to minor failures and retries, we might better be able to saturate our bandwidth by doing multiple smaller all-reduces in parallel. The amount of data being sent doesn’t change, but that works better with TCP dynamics over long distances, and the overhead to set up the streams is super trivial in comparison to that travel time.\nLet’s look at 4 streams:\n[ ID] Interval Transfer Bitrate Retr [ 5] 0.00-30.00 sec 596 MBytes 167 Mbits/sec 1741 sender [ 5] 0.00-30.11 sec 592 MBytes 165 Mbits/sec receiver [ 7] 0.00-30.00 sec 540 MBytes 151 Mbits/sec 2105 sender [ 7] 0.00-30.11 sec 538 MBytes 150 Mbits/sec receiver [ 9] 0.00-30.00 sec 377 MBytes 105 Mbits/sec 3048 sender [ 9] 0.00-30.11 sec 375 MBytes 104 Mbits/sec receiver [ 11] 0.00-30.00 sec 477 MBytes 133 Mbits/sec 2011 sender [ 11] 0.00-30.11 sec 475 MBytes 132 Mbits/sec receiver [SUM] 0.00-30.00 sec 1.94 GBytes 556 Mbits/sec 8905 sender [SUM] 0.00-30.11 sec 1.93 GBytes 552 Mbits/sec receiver Much better! Now at 552 Mbits/sec, we’re looking at 0.069 GB/s, so a total transfer time of 2.4 minutes.\nNow let’s look at 8 streams:\n[ ID] Interval Transfer Bitrate Retr [ 5] 0.00-30.00 sec 628 MBytes 176 Mbits/sec 3062 sender [ 5] 0.00-30.11 sec 627 MBytes 175 Mbits/sec receiver [ 7] 0.00-30.00 sec 498 MBytes 139 Mbits/sec 2120 sender [ 7] 0.00-30.11 sec 496 MBytes 138 Mbits/sec receiver [ 9] 0.00-30.00 sec 636 MBytes 178 Mbits/sec 2684 sender [ 9] 0.00-30.11 sec 636 MBytes 177 Mbits/sec receiver [ 11] 0.00-30.00 sec 597 MBytes 167 Mbits/sec 2867 sender [ 11] 0.00-30.11 sec 596 MBytes 166 Mbits/sec receiver [ 13] 0.00-30.00 sec 488 MBytes 136 Mbits/sec 2350 sender [ 13] 0.00-30.11 sec 487 MBytes 136 Mbits/sec receiver [ 15] 0.00-30.00 sec 422 MBytes 118 Mbits/sec 2930 sender [ 15] 0.00-30.11 sec 421 MBytes 117 Mbits/sec receiver [ 17] 0.00-30.00 sec 549 MBytes 153 Mbits/sec 2691 sender [ 17] 0.00-30.11 sec 548 MBytes 153 Mbits/sec receiver [ 19] 0.00-30.00 sec 630 MBytes 176 Mbits/sec 3933 sender [ 19] 0.00-30.11 sec 628 MBytes 175 Mbits/sec receiver [SUM] 0.00-30.00 sec 4.34 GBytes 1.24 Gbits/sec 22637 sender [SUM] 0.00-30.11 sec 4.34 GBytes 1.24 Gbits/sec receiver This gives us 1.24 Gb/s, which is 0.155 GB/s and has a sync time of 1.1 minutes. That GB/s number looks a lot closer to the crappy home internet number of 0.026 GB/s.\nYou can actually see in the code of the DiLoCo implementation their bucketing strategy for the tensors here. Each tensor group is handled separately, which would be ideal for opening those parallel streams and saturating the bandwidth. This implementation looks blocking to me, though, so I’m not sure if they did some magic or not, or whether the code that handles the parallelized calls isn’t in the repo.\nBeing unfamiliar with Gloo, I also ran an experiment to look at my bandwidth doing all-reduces in torch to see if there was evidence of Gloo opening multiple streams when doing the all-reduce. I used Torchrun over public IP.\ndef single_allreduce(tensor: torch.Tensor) -\u003e AllReduceResult: \"\"\" Perform a single all-reduce and measure it. This is pure CPU, matching DiLoCo's outer optimizer setup. \"\"\" size_bytes = tensor.numel() * tensor.element_size() timestamp = time.time() try: dist.barrier() start = time.perf_counter() dist.all_reduce(tensor, op=dist.ReduceOp.AVG) duration = time.perf_counter() - start return AllReduceResult( timestamp=timestamp, duration_seconds=duration, size_bytes=size_bytes, success=True ) except Exception as e: return AllReduceResult( timestamp=timestamp, duration_seconds=float('inf'), size_bytes=size_bytes, success=False, error=str(e) ) My mean results for the throughput was 0.01575 GB/s, which is closest to the bandwidth we observed with a single stream. So by default, it appears Gloo does not parallelize the streams. Mostly this suggests doing decentralized training and achieving fully saturated bandwidth means significantly modifying the operation of your default tooling. It’s not going to take care of these kind of edge cases by default. You can see why you would be motiviated to find solutions that let you fully saturate that meager bandwidth though, because that’s cutting entire minutes off your sync that is idling your GPUs. Your MFU depends on it!\nFactorio With Tiny Pipes As a researcher, my goal has always been to get the lowest loss/best test score possible. Getting into the systems perspective on ML is mostly just clarifying to me that the engineering perspective on these systems is straight up playing Factorio.\nYou’re still optimizing, just for a different number. You want to maximize (MFU) and everything from customizing GEMM kernels to optimizing inter-node bandwidth is all in service of keeping GPUs running as fast as possible. Anytime you’re paying for those GPUs and they’re not doing as many FLOPs as possible, you’re burning your most valuable resource.\nThe decentralization tax is pretty big.\nDecentralized training is doing all that with both hands tied behind your back. You’re basically taking the interconnect bandwidth and squeezing it to nearly nothing, and that additional constraint drives algorithmic improvements. While in a centralized setting you might focus all of your time on keeping GPUs saturated, in a decentralized setting your biggest gains are going to be optimizing transferring as little data as possible between nodes, because that’s what’s keeping your GPUs from doing work. It also suggests a search for different desirable algorithmic properties - for example, while DiLoCo is good for reducing the amount of times you need to sync during training, it’s still blocking when it’s time to sync. If you were using heterogenous compute and you expected each replica to process their million tokens in a vastly different time frame, you would suddenly desire algorithms that could comfortably handle transferring that data in an asynchronous way - all the better to keep GPUs going.\nWorth noting that the activation gradients do not need to be communicated between model replicas, we’re only really interested in the weight gradients. ↩︎\nIf you say en cee cee el you will be relentlessly bullied. ↩︎\nI don’t know how else you’d pronounce it. I don’t think you’re beat up for saying this one wrong, though. ↩︎\nTheir work is cool, so we forgive them the use of the S in maximizing. ↩︎\n","wordCount":"3551","inLanguage":"en","image":"http://localhost:1313/","datePublished":"2025-12-12T00:00:00Z","dateModified":"2025-12-12T00:00:00Z","author":{"@type":"Person","name":"Shane Caldwell"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/writing/all-reduce-across-atlantic/"},"publisher":{"@type":"Organization","name":"Shane Caldwell","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Shane Caldwell (Alt + H)">Shane Caldwell</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Main><span>Main</span></a></li><li><a href=http://localhost:1313/papers/ title=Papers><span>Papers</span></a></li><li><a href=http://localhost:1313/talks/ title=Talks><span>Talks</span></a></li><li><a href=http://localhost:1313/writing/ title=Writing><span>Writing</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">All Reduce Across the Atlantic: Bandwidth in Decentralized Training</h1><div class=post-description>The practical realities of devestatingly high communication cost in training.</div><div class=post-meta><span title='2025-12-12 00:00:00 +0000 UTC'>December 12, 2025</span>&nbsp;·&nbsp;17 min&nbsp;·&nbsp;3551 words&nbsp;·&nbsp;Shane Caldwell</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#big-boy-datacenter-numbers>Big boy datacenter numbers</a></li><li><a href=#indie-numbers>Indie numbers</a></li><li><a href=#whats-all-reduce-for>What&rsquo;s All-Reduce for?</a></li><li><a href=#nccl>NCCL</a></li><li><a href=#gloo>Gloo</a></li></ul></li><li><a href=#intellect-1>INTELLECT-1</a></li><li><a href=#intercontinental-bandwidth>Intercontinental Bandwidth</a></li></ul></nav></div></details></div><div class=post-content><p>Since <a href=https://hackbot.dad/writing/data-parallelism-for-the-poor/>implementing DiLoCo</a> I&rsquo;ve been really interested in algorithms for decentralized training. The more <a href=https://hackbot.dad/writing/pretraining-at-home/>LLMs I train</a> in a &ldquo;normal&rdquo; setting, the more interesting to me the decentralized case is. Mostly because seemingly nothing <em>wants</em> to work. So much of the software you can find off the shelf for training is optimized for centralized compute. For good reason, this is the <em>default</em> case. Most people implementing high performance algorithms to train large models are trying to optimize FLOPs, and are doing so in some kind of tightly connected datacenter. It is extremely difficult to keep FLOPs high when you&rsquo;re spending time communicating data. As it is on a single-node, so it is in many.</p><p>I implemented DiLoCo, but I tested the algorithm with a cluster of two nodes on <a href=http://modal.com/>Modal</a>. This effectively tests the algorithmic implementation and speaks a little bit to the communication costs of the all-reduce, but fails to capture a real global training scenario.</p><p>What can go wrong in a decentralized training scenario? Let&rsquo;s consider just the effects decentralized training has on data parallelism, where each island of compute has access to one model replica. Compute island bandwidth can be variable between nodes. The actual compute can be heterogenous, so the same pipeline and parallelism strategies may not work on each island, requiring a custom setup fit for it, which besides just having different FLOPs may have different memory constraints leading to different minibatch sizes. This would lead certain islands to run ahead of others, so what do you do at the synchronization step? You could wait, but then your faster and probably more expensive compute is hanging waiting for the stragglers. You could try to move ahead, but now the lagging worker is training off of a stale replica and won&rsquo;t catch up anytime soon, and your currently training models are missing information from the dataset you sent to the laggers. Even if the compute was homogenous, there&rsquo;s latency of the islands to each other.</p><p>There&rsquo;s a lot of complicated questions to dig into there, but I&rsquo;ll start by looking at an extremely basic one: Given two nodes on opposite sides of the Atlantic, what&rsquo;s the theoretical bandwidth ceiling, and how close can we get to it for gradient synchronization?</p><p>Before we get started, it&rsquo;s worth noting some numbers, bandwidth-wise, for doing data parallel training in a centralized setting.</p><h3 id=big-boy-datacenter-numbers>Big boy datacenter numbers<a hidden class=anchor aria-hidden=true href=#big-boy-datacenter-numbers>#</a></h3><ul><li><p>NVLink for GPUs on the same node: <strong>1,800 GB/s</strong></p></li><li><p>PCIe on the same node: <strong>32 GB/s</strong></p></li><li><p>NVSwitch for nodes on the same rack: <strong>1,800 GB/s</strong></p></li><li><p>InfiniBand on different racks: <strong>50-100 GB/s</strong></p></li></ul><h3 id=indie-numbers>Indie numbers<a hidden class=anchor aria-hidden=true href=#indie-numbers>#</a></h3><ul><li><p>High-end home fiber: 1.25 GB/s</p></li><li><p>Typical US Home Internet: 0.026 GB/s</p></li></ul><p>And to understand the size of the data we&rsquo;d be moving around by default in data parallel training, we can consider the &ldquo;default&rdquo; case of passing the weight gradients around in FP32.</p><p>10B parameters x 32 bits = 40 GB<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><p>So if we were fully saturating the bandwidth, our time to transfer (latency disregarded) would be:</p><ul><li>NVLink: 22ms</li><li>Infiniband: 800ms</li><li>PCIe 4.0: 1.25 seconds</li><li>High-end home fiber: 32 seconds</li><li>Typical US Home Internet: 26 minutes</li></ul><p>We&rsquo;re bandwidth bottlenecked every time our communication time <em>exceeds</em> compute time. If we assume a training step finishes in something like 500ms, we&rsquo;re communication bound by our theoretical max bandwidth as soon as we&rsquo;re going over anything with a smaller pipe than NVLink, and that&rsquo;s before we consider latency.</p><p>Most importantly, we can think of this as primarily effecting <a href=https://hackbot.dad/writing/pretraining-at-home/>MFU, our main measure of training efficiency</a>. Our theoretical FLOPs are defined by the GPU. Any time blocking would essentially be overhead, so our actual achieved FLOPs go down and MFU tanks.</p><p>Let&rsquo;s briefly look at the all-reduce operation we&rsquo;re doing:</p><h3 id=whats-all-reduce-for>What&rsquo;s All-Reduce for?<a hidden class=anchor aria-hidden=true href=#whats-all-reduce-for>#</a></h3><p>Let&rsquo;s look at the dumb version.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.distributed</span> <span class=k>as</span> <span class=nn>dist</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>sync_gradients</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>	Call before optimizer step
</span></span></span><span class=line><span class=cl><span class=s2>	&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>	<span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>sync_grads</span><span class=p>:</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>param</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
</span></span><span class=line><span class=cl>		<span class=k>if</span> <span class=n>param</span><span class=o>.</span><span class=n>grad</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>			<span class=n>dist</span><span class=o>.</span><span class=n>all_reduce</span><span class=p>(</span><span class=n>param</span><span class=o>.</span><span class=n>grad</span><span class=p>,</span> <span class=n>op</span><span class=o>=</span><span class=n>dist</span><span class=o>.</span><span class=n>ReduceOp</span><span class=o>.</span><span class=n>AVG</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>synchronize</span><span class=p>()</span>
</span></span></code></pre></div><p>When we&rsquo;re training, each replica is working on different data. If they weren&rsquo;t, they would have no novel information to exchange with each other and we&rsquo;d just be training the same model on each. We&rsquo;re accumulating gradients over minibatches, and building up a local gradient state. Before we run our optimizer step, we want to capture global information from the rest of the replicas about their local gradients.</p><p>All-reduce is a standard distributed computing primitive that lets us take as input some data that is different for each node running the same program (our local gradients) apply some function to them (in this case, averaging) and return the result (the global gradients) back to each program.</p><p>This means that the value of <code>param.grad</code> for each param now contains the global average of the local gradients, and each program now has the same understanding of the gradient. Now when you take your gradient step, each of them will now be synced, and they will continue to work through their unique batch of data from that new starting point.</p><p>So, in that one line, <code>dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)</code> we&rsquo;re doing all of that. That&rsquo;s a lot of invisible networking and computation happening in a one liner. What&rsquo;s doing that? Where is the computation occurring? How are the nodes connecting to each other?</p><p>Well, there are several possible libraries that implement those &lsquo;collective communication&rsquo; operations.</p><h3 id=nccl>NCCL<a hidden class=anchor aria-hidden=true href=#nccl>#</a></h3><p>If you remember nothing else about NCCL, remember that it&rsquo;s pronounced &ldquo;nickel&rdquo;<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. It stands for &ldquo;NVIDIA Collective Communications Library&rdquo;. The big advantage with NCCL is that it supports GPU to GPU communication and it&rsquo;s optimized for it. That is, when you do all-reduce, the data does not have to be written to CPU. Intra-node, the data can move literally just between GPU memory buffers. Inter-node, you can write from the GPU to the network interface card to the network, the other nodes networking card, and then its GPU. In either case, no CPU.</p><p>NCCL itself doesn&rsquo;t handle discovery. <code>torchrun</code>/<code>torch.distributed</code> handles that instead. When Pytorch runs <code>init_process_group()</code> it gets all the processes to check in. Once each rank (process) has joined, they exchange information, and then NCCL establishes direct communication channels to be used when an all-reduce or other collective communication operation is invoked.</p><p>Another question - how do those messages actually get passed? It turns out this is dependent on what&rsquo;s available. There&rsquo;s a series of fallbacks implemented to make sure you get the most efficient possible available communication.</p><p>For the same node - If you&rsquo;ve got NVLink, you can do GPU-to-GPU interconnect.</p><p>Between nodes, you&rsquo;ll use Infiband with GPUDirect RDMA (Remote Direct Access Memory), so you sort of treat your GPUs as one <em>big</em> GPU for the purposes of communication.</p><p>If you don&rsquo;t have Infiniband, you&rsquo;ll do RDMA over ethernet.</p><p>This is all, for decreasing amounts of bandwidth, hyper-efficient and assumes a lossless networking setup. And if all that <em>fails</em> (which it would, in the transcontinental case) you fall back to TCP/IP sockets, which have a lot of algorithms for handling the fact that data is lossy and needs to arrive in specific order. NCCL still technically works, but you&rsquo;ll pretty much never hear about this case because the overhead is high and it&rsquo;s kind of a crazy thing to do.</p><h3 id=gloo>Gloo<a hidden class=anchor aria-hidden=true href=#gloo>#</a></h3><p>Gloo is not an acronym. I think it&rsquo;s pronounced &ldquo;glue&rdquo;<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p><p>Gloo is, by default, over TCP/IP. The rendezvous mechanism expectation is the same as with NCCL (handled by torchrun) and requires all participating processes to be reachable via a bidirectional network interface.</p><p>Gloo is generally known as being intended for CPU-based models. The advice you&rsquo;ll get from <a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=nccl">reasonable sources</a> is that if you&rsquo;re doing GPU training, you should be using NCCL, and if you&rsquo;re doing CPU training, you should be using Gloo. The <a href=https://docs.pytorch.org/docs/stable/distributed.html>Pytorch Docs</a> get slightly more specific, reading:</p><blockquote><p>[For GPU hosts with Ethernet interconnect&mldr;] - Use NCCL, since it currently provides the best distributed GPU training performance, especially for multiprocess single-node or multi-node distributed training. If you encounter any problem with NCCL, use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.)</p></blockquote><h2 id=intellect-1>INTELLECT-1<a hidden class=anchor aria-hidden=true href=#intellect-1>#</a></h2><p>LLM training is obviously a GPU activity, so just based on these references it seems like NCCL should still be the right call. But, when we read the <a href=https://arxiv.org/pdf/2412.01152>INTELLECT-1 Technical Report</a>, where Prime Intellect trained a 10B parameter model using decentralized compute, we read the following:</p><blockquote><p>We utilize a VPN because our inter-node collective communication library, <strong>Gloo</strong>, requires all participating processes to be reachable via a private network interface. Additionally, the VPN ensures secure, encrypted communication between nodes.</p></blockquote><p>Intra-node, they use NCCL as you would expect.</p><blockquote class=twitter-tweet><p lang=en dir=ltr>Fyi it's still use nccl for intra GPU communication. Gloo is used for the all reduce across the world</p>&mdash; samsja (@samsja19) <a href="https://twitter.com/samsja19/status/1847698511388983372?ref_src=twsrc%5Etfw">October 19, 2024</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><p>So what&rsquo;s the deal?</p><p>The answer is actually really obvious, and specific to DiLoCo: Prime is legitimately interested in CPU, not GPU, operations during DiLoCo. As I discussed in my <a href=https://hackbot.dad/writing/data-parallelism-for-the-poor/>DiLoCo blog</a>, the algorithm requires essentially two copies of your model.</p><ol><li><p>The model being optimized on your local dataset, being run through the &ldquo;inner optimizer&rdquo;.</p></li><li><p>A replica of the last time you did a global sync, being run through the &ldquo;outer optimizer.&rdquo;</p></li></ol><p>Storing both of those on GPU would be unnecessarily expensive, and you&rsquo;re really not doing anything with the reference model (not pushing tokens or anything), so you offload it that one to CPU.</p><p>Meanwhile, your inner optimizer step is plowing away, updating weights anywhere from 100 to 500 times, for some tunable parameter $H$.</p><p>Now we get to the interesting part, which is after $H$ steps have been completed. Now you want each replica to share data. You essentially create a pseudo gradient, by looking at the difference between the weights reference model parameters you&rsquo;ve got stored in CPU against the parameters you&rsquo;ve ended up with after $H$ steps.</p><p>The detail I missed in my last blog was this: I assumed you would perform the all-reduce operation on the GPU. That is, you would calculate the gradients on GPU, and then call all-reduce, so you&rsquo;d use NCCL. <a href=https://github.com/PrimeIntellect-ai/prime-diloco/blob/main/src/zeroband/diloco.py>The INTELLECT-1 repo</a> doesn&rsquo;t do this, instead it keeps all of the outer model stuff on CPU. The optimizer lives there, and the gradients live there.</p><p>They note this is primarily to avoid VRAM overhead, which I don&rsquo;t entirely understand. It makes sense to not want two copies of the model on GPU, but the all-reduce seems like it could happen on either GPU or CPU without complaint. In-particular when training LLMs, your memory is dominated by the activations moreso than the weights or gradients, so when you&rsquo;re going to block the GPU anyway until the all-reduce is complete and you&rsquo;re ready to start training again, you should have plenty of available VRAM. My current assumption is this is attractive/advantageous because recovering from failure with gloo is easier than handling it on the GPU, but I&rsquo;m just making that up. It would also be reasonable to me if the sharding from FSDP2 made the VRAM situation more tenuous. If anyone knows, tell me!</p><p>So, after a lot of waffling, it seems that it is as simple as your GPU all-reduce backend should be NCCL, and your CPU all-reduce backend should be Gloo. It&rsquo;s just that you might choose to have some surprising computations on CPU depending on your use-case.</p><p>So, we&rsquo;ve got the reported numbers and we understand why doing the all-reduce over CPU is the right choice, making Gloo a natural fit. Let&rsquo;s see what our bandwidth is between American and Europe and whether Gloo can saturate it during an all-reduce.</p><h2 id=intercontinental-bandwidth>Intercontinental Bandwidth<a hidden class=anchor aria-hidden=true href=#intercontinental-bandwidth>#</a></h2><p>Using <a href=https://www.primeintellect.ai/>Prime Intellect</a> I picked up two CPU nodes - one in the United States and one in France. After establishing their Public IPs and connection between them, I wanted to take a look at the bandwidth.</p><p>To do that, I used <a href=https://github.com/esnet/iperf>iperf3</a>, a tool for determining the maximum achievable bandwidth on IP networks. There are several tunable parameters, but for our part we were most interested in <code>-P</code> the number of parallel streams we run at once.</p><p>I started with one stream.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span> <span class=nb>local</span> 192.168.0.118 port <span class=m>39636</span> connected to 72.46.85.115 port <span class=m>5201</span>
</span></span><span class=line><span class=cl><span class=o>[</span> ID<span class=o>]</span> Interval           Transfer     Bitrate         Retr  Cwnd
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>   0.00-1.00   sec  6.51 MBytes  54.6 Mbits/sec    <span class=m>0</span>   3.33 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>   1.00-2.00   sec  12.5 MBytes   <span class=m>105</span> Mbits/sec  <span class=m>1315</span>    <span class=m>112</span> KBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>   2.00-3.00   sec  11.2 MBytes  94.4 Mbits/sec  <span class=m>1017</span>   1.55 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>   3.00-4.00   sec  13.8 MBytes   <span class=m>115</span> Mbits/sec    <span class=m>0</span>   1.64 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>   4.00-5.00   sec  13.8 MBytes   <span class=m>115</span> Mbits/sec    <span class=m>0</span>   1.71 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>   5.00-6.00   sec  16.2 MBytes   <span class=m>136</span> Mbits/sec    <span class=m>0</span>   1.76 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>   6.00-7.00   sec  15.0 MBytes   <span class=m>126</span> Mbits/sec    <span class=m>0</span>   1.79 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>   7.00-8.00   sec  15.0 MBytes   <span class=m>126</span> Mbits/sec    <span class=m>0</span>   1.81 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>   8.00-9.00   sec  16.2 MBytes   <span class=m>136</span> Mbits/sec    <span class=m>0</span>   1.82 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>   9.00-10.00  sec  16.2 MBytes   <span class=m>136</span> Mbits/sec    <span class=m>0</span>   1.83 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  10.00-11.00  sec  16.2 MBytes   <span class=m>136</span> Mbits/sec    <span class=m>0</span>   1.83 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  11.00-12.00  sec  15.0 MBytes   <span class=m>126</span> Mbits/sec    <span class=m>0</span>   1.83 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  12.00-13.00  sec  16.2 MBytes   <span class=m>136</span> Mbits/sec    <span class=m>0</span>   1.83 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  13.00-14.00  sec  16.2 MBytes   <span class=m>136</span> Mbits/sec    <span class=m>0</span>   1.84 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  14.00-15.00  sec  16.2 MBytes   <span class=m>136</span> Mbits/sec    <span class=m>0</span>   1.86 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  15.00-16.00  sec  15.0 MBytes   <span class=m>126</span> Mbits/sec    <span class=m>0</span>   1.89 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  16.00-17.00  sec  17.5 MBytes   <span class=m>147</span> Mbits/sec    <span class=m>0</span>   1.94 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  17.00-18.00  sec  17.5 MBytes   <span class=m>147</span> Mbits/sec    <span class=m>0</span>   2.01 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  18.00-19.00  sec  16.2 MBytes   <span class=m>136</span> Mbits/sec    <span class=m>0</span>   2.09 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  19.00-20.00  sec  15.0 MBytes   <span class=m>126</span> Mbits/sec  <span class=m>226</span>   1.56 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  20.00-21.00  sec  11.2 MBytes  94.4 Mbits/sec  <span class=m>204</span>   1.18 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  21.00-22.00  sec  10.0 MBytes  83.9 Mbits/sec    <span class=m>0</span>   1.25 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  22.00-23.00  sec  11.2 MBytes  94.4 Mbits/sec    <span class=m>0</span>   1.31 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  23.00-24.00  sec  12.5 MBytes   <span class=m>105</span> Mbits/sec    <span class=m>0</span>   1.36 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  24.00-25.00  sec  11.2 MBytes  94.4 Mbits/sec    <span class=m>0</span>   1.38 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  25.00-26.00  sec  12.5 MBytes   <span class=m>105</span> Mbits/sec    <span class=m>0</span>   1.40 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  26.00-27.00  sec  12.5 MBytes   <span class=m>105</span> Mbits/sec    <span class=m>0</span>   1.41 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  27.00-28.00  sec  12.5 MBytes   <span class=m>105</span> Mbits/sec    <span class=m>0</span>   1.41 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  28.00-29.00  sec  11.2 MBytes  94.4 Mbits/sec    <span class=m>0</span>   1.41 MBytes
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>  29.00-30.00  sec  12.5 MBytes   <span class=m>105</span> Mbits/sec    <span class=m>0</span>   1.41 MBytes
</span></span><span class=line><span class=cl>- - - - - - - - - - - - - - - - - - - - - - - - -
</span></span><span class=line><span class=cl><span class=o>[</span> ID<span class=o>]</span> Interval           Transfer     Bitrate         Retr
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>   0.00-30.00  sec   <span class=m>415</span> MBytes   <span class=m>116</span> Mbits/sec  <span class=m>2762</span>             sender
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>   0.00-30.11  sec   <span class=m>414</span> MBytes   <span class=m>115</span> Mbits/sec                  receiver
</span></span></code></pre></div><p>Our bitrate here, for one stream, is <code>116 Mbits/sec</code> or <code>0.0145 GB/s</code> For a 10GB gradient sync, we&rsquo;re looking at around <code>689.655</code> seconds, or around 11.5 minutes for the transfer. Prime Intellect notes that their gradient syncs, which involved more nodes and more data for the ring all-reduce, took &ldquo;around 1 to 7 minutes depending on the configuration&rdquo;. They were also using tailscale, which they note gave a performance hit. So what gives? Did we just pick really bad nodes?</p><p>The <a href=https://www.primeintellect.ai/blog/intellect-1>INTELLECT-1 blog</a> has an additional detail under &ldquo;Maximising<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> bandwidth utilization&rdquo;:</p><blockquote><p>By sharding our DiLoCo pseudo-gradients in a node, we can maximise network bandwidth utilization by opening multiple connections at the same time when performing the all-reduce. This yielded a transfer speed improvement of 8x on some nodes.</p></blockquote><p>So the idea is basically that instead of calling one all-reduce with a single TCP stream prone to minor failures and retries, we might better be able to saturate our bandwidth by doing multiple smaller all-reduces in parallel. The amount of data being sent doesn&rsquo;t change, but that works better with TCP dynamics over long distances, and the overhead to set up the streams is super trivial in comparison to that travel time.</p><p>Let&rsquo;s look at <strong>4 streams</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>[</span> ID<span class=o>]</span> Interval           Transfer     Bitrate         Retr
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>   0.00-30.00  sec   <span class=m>596</span> MBytes   <span class=m>167</span> Mbits/sec  <span class=m>1741</span>             sender
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>   0.00-30.11  sec   <span class=m>592</span> MBytes   <span class=m>165</span> Mbits/sec                  receiver
</span></span><span class=line><span class=cl><span class=o>[</span>  7<span class=o>]</span>   0.00-30.00  sec   <span class=m>540</span> MBytes   <span class=m>151</span> Mbits/sec  <span class=m>2105</span>             sender
</span></span><span class=line><span class=cl><span class=o>[</span>  7<span class=o>]</span>   0.00-30.11  sec   <span class=m>538</span> MBytes   <span class=m>150</span> Mbits/sec                  receiver
</span></span><span class=line><span class=cl><span class=o>[</span>  9<span class=o>]</span>   0.00-30.00  sec   <span class=m>377</span> MBytes   <span class=m>105</span> Mbits/sec  <span class=m>3048</span>             sender
</span></span><span class=line><span class=cl><span class=o>[</span>  9<span class=o>]</span>   0.00-30.11  sec   <span class=m>375</span> MBytes   <span class=m>104</span> Mbits/sec                  receiver
</span></span><span class=line><span class=cl><span class=o>[</span> 11<span class=o>]</span>   0.00-30.00  sec   <span class=m>477</span> MBytes   <span class=m>133</span> Mbits/sec  <span class=m>2011</span>             sender
</span></span><span class=line><span class=cl><span class=o>[</span> 11<span class=o>]</span>   0.00-30.11  sec   <span class=m>475</span> MBytes   <span class=m>132</span> Mbits/sec                  receiver
</span></span><span class=line><span class=cl><span class=o>[</span>SUM<span class=o>]</span>   0.00-30.00  sec  1.94 GBytes   <span class=m>556</span> Mbits/sec  <span class=m>8905</span>             sender
</span></span><span class=line><span class=cl><span class=o>[</span>SUM<span class=o>]</span>   0.00-30.11  sec  1.93 GBytes   <span class=m>552</span> Mbits/sec                  receiver
</span></span></code></pre></div><p>Much better! Now at <code>552 Mbits/sec</code>, we&rsquo;re looking at <code>0.069 GB/s</code>, so a total transfer time of 2.4 minutes.</p><p>Now let&rsquo;s look at <strong>8 streams</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>[</span> ID<span class=o>]</span> Interval           Transfer     Bitrate         Retr
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>   0.00-30.00  sec   <span class=m>628</span> MBytes   <span class=m>176</span> Mbits/sec  <span class=m>3062</span>             sender
</span></span><span class=line><span class=cl><span class=o>[</span>  5<span class=o>]</span>   0.00-30.11  sec   <span class=m>627</span> MBytes   <span class=m>175</span> Mbits/sec                  receiver
</span></span><span class=line><span class=cl><span class=o>[</span>  7<span class=o>]</span>   0.00-30.00  sec   <span class=m>498</span> MBytes   <span class=m>139</span> Mbits/sec  <span class=m>2120</span>             sender
</span></span><span class=line><span class=cl><span class=o>[</span>  7<span class=o>]</span>   0.00-30.11  sec   <span class=m>496</span> MBytes   <span class=m>138</span> Mbits/sec                  receiver
</span></span><span class=line><span class=cl><span class=o>[</span>  9<span class=o>]</span>   0.00-30.00  sec   <span class=m>636</span> MBytes   <span class=m>178</span> Mbits/sec  <span class=m>2684</span>             sender
</span></span><span class=line><span class=cl><span class=o>[</span>  9<span class=o>]</span>   0.00-30.11  sec   <span class=m>636</span> MBytes   <span class=m>177</span> Mbits/sec                  receiver
</span></span><span class=line><span class=cl><span class=o>[</span> 11<span class=o>]</span>   0.00-30.00  sec   <span class=m>597</span> MBytes   <span class=m>167</span> Mbits/sec  <span class=m>2867</span>             sender
</span></span><span class=line><span class=cl><span class=o>[</span> 11<span class=o>]</span>   0.00-30.11  sec   <span class=m>596</span> MBytes   <span class=m>166</span> Mbits/sec                  receiver
</span></span><span class=line><span class=cl><span class=o>[</span> 13<span class=o>]</span>   0.00-30.00  sec   <span class=m>488</span> MBytes   <span class=m>136</span> Mbits/sec  <span class=m>2350</span>             sender
</span></span><span class=line><span class=cl><span class=o>[</span> 13<span class=o>]</span>   0.00-30.11  sec   <span class=m>487</span> MBytes   <span class=m>136</span> Mbits/sec                  receiver
</span></span><span class=line><span class=cl><span class=o>[</span> 15<span class=o>]</span>   0.00-30.00  sec   <span class=m>422</span> MBytes   <span class=m>118</span> Mbits/sec  <span class=m>2930</span>             sender
</span></span><span class=line><span class=cl><span class=o>[</span> 15<span class=o>]</span>   0.00-30.11  sec   <span class=m>421</span> MBytes   <span class=m>117</span> Mbits/sec                  receiver
</span></span><span class=line><span class=cl><span class=o>[</span> 17<span class=o>]</span>   0.00-30.00  sec   <span class=m>549</span> MBytes   <span class=m>153</span> Mbits/sec  <span class=m>2691</span>             sender
</span></span><span class=line><span class=cl><span class=o>[</span> 17<span class=o>]</span>   0.00-30.11  sec   <span class=m>548</span> MBytes   <span class=m>153</span> Mbits/sec                  receiver
</span></span><span class=line><span class=cl><span class=o>[</span> 19<span class=o>]</span>   0.00-30.00  sec   <span class=m>630</span> MBytes   <span class=m>176</span> Mbits/sec  <span class=m>3933</span>             sender
</span></span><span class=line><span class=cl><span class=o>[</span> 19<span class=o>]</span>   0.00-30.11  sec   <span class=m>628</span> MBytes   <span class=m>175</span> Mbits/sec                  receiver
</span></span><span class=line><span class=cl><span class=o>[</span>SUM<span class=o>]</span>   0.00-30.00  sec  4.34 GBytes  1.24 Gbits/sec  <span class=m>22637</span>             sender
</span></span><span class=line><span class=cl><span class=o>[</span>SUM<span class=o>]</span>   0.00-30.11  sec  4.34 GBytes  1.24 Gbits/sec                  receiver
</span></span></code></pre></div><p>This gives us <code>1.24 Gb/s</code>, which is <code>0.155 GB/s</code> and has a sync time of 1.1 minutes. That <code>GB/s</code> number looks a lot closer to the crappy home internet number of <code>0.026 GB/s</code>.</p><p>You can actually see in the code of the DiLoCo implementation their bucketing strategy for the tensors <a href=https://github.com/PrimeIntellect-ai/prime-diloco/blob/main/src/zeroband/diloco.py#L107>here</a>. Each tensor group is handled separately, which would be ideal for opening those parallel streams and saturating the bandwidth. This implementation looks blocking to me, though, so I&rsquo;m not sure if they did some magic or not, or whether the code that handles the parallelized calls isn&rsquo;t in the repo.</p><p>Being unfamiliar with Gloo, I also ran an experiment to look at my bandwidth doing all-reduces in torch to see if there was evidence of Gloo opening multiple streams when doing the all-reduce. I used Torchrun over public IP.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>single_allreduce</span><span class=p>(</span><span class=n>tensor</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>AllReduceResult</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Perform a single all-reduce and measure it.
</span></span></span><span class=line><span class=cl><span class=s2>    
</span></span></span><span class=line><span class=cl><span class=s2>    This is pure CPU, matching DiLoCo&#39;s outer optimizer setup.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>size_bytes</span> <span class=o>=</span> <span class=n>tensor</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=o>*</span> <span class=n>tensor</span><span class=o>.</span><span class=n>element_size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>timestamp</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>dist</span><span class=o>.</span><span class=n>barrier</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>perf_counter</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>dist</span><span class=o>.</span><span class=n>all_reduce</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>op</span><span class=o>=</span><span class=n>dist</span><span class=o>.</span><span class=n>ReduceOp</span><span class=o>.</span><span class=n>AVG</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>duration</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>perf_counter</span><span class=p>()</span> <span class=o>-</span> <span class=n>start</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>AllReduceResult</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>timestamp</span><span class=o>=</span><span class=n>timestamp</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>duration_seconds</span><span class=o>=</span><span class=n>duration</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>size_bytes</span><span class=o>=</span><span class=n>size_bytes</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>success</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>AllReduceResult</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>timestamp</span><span class=o>=</span><span class=n>timestamp</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>duration_seconds</span><span class=o>=</span><span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>size_bytes</span><span class=o>=</span><span class=n>size_bytes</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>success</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>error</span><span class=o>=</span><span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span></code></pre></div><p>My mean results for the throughput was <code>0.01575 GB/s</code>, which is closest to the bandwidth we observed with a single stream. So by default, it appears Gloo does not parallelize the streams. Mostly this suggests doing decentralized training and achieving fully saturated bandwidth means significantly modifying the operation of your default tooling. It&rsquo;s not going to take care of these kind of edge cases by default. You can see why you would be motiviated to find solutions that let you fully saturate that meager bandwidth though, because that&rsquo;s cutting entire minutes off your sync that is idling your GPUs. Your MFU depends on it!</p><h1 id=factorio-with-tiny-pipes>Factorio With Tiny Pipes<a hidden class=anchor aria-hidden=true href=#factorio-with-tiny-pipes>#</a></h1><p>As a researcher, my goal has always been to get the lowest loss/best test score possible. Getting into the systems perspective on ML is mostly just clarifying to me that the engineering perspective on these systems is straight up playing Factorio.</p><p>You&rsquo;re still optimizing, just for a different number. You want to maximize (MFU) and everything from customizing GEMM kernels to optimizing inter-node bandwidth is all in service of keeping GPUs running as fast as possible. Anytime you&rsquo;re paying for those GPUs and they&rsquo;re not doing as many FLOPs as possible, you&rsquo;re burning your most valuable resource.</p><figure><img loading=lazy src=bandwidth_transfer_times.png alt="The decentralization tax is pretty big."><figcaption><p>The decentralization tax is pretty big.</p></figcaption></figure><p>Decentralized training is doing all that with both hands tied behind your back. You&rsquo;re basically taking the interconnect bandwidth and squeezing it to nearly nothing, and that additional constraint drives algorithmic improvements. While in a centralized setting you might focus all of your time on keeping GPUs saturated, in a decentralized setting your biggest gains are going to be optimizing transferring as little data as possible between nodes, because that&rsquo;s what&rsquo;s keeping your GPUs from doing work. It also suggests a search for different desirable algorithmic properties - for example, while DiLoCo is good for reducing the amount of times you need to sync during training, it&rsquo;s still blocking when it&rsquo;s time to sync. If you were using heterogenous compute and you expected each replica to process their million tokens in a vastly different time frame, you would suddenly desire algorithms that could comfortably handle transferring that data in an asynchronous way - all the better to keep GPUs going.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Worth noting that the activation gradients <em>do not</em> need to be communicated between model replicas, we&rsquo;re only really interested in the <em>weight</em> gradients.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>If you say en cee cee el you will be relentlessly bullied.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>I don&rsquo;t know how else you&rsquo;d pronounce it. I don&rsquo;t think you&rsquo;re beat up for saying this one wrong, though.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Their work is cool, so we forgive them the use of the S in maximizing.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/llms/>Llms</a></li><li><a href=http://localhost:1313/tags/training/>Training</a></li><li><a href=http://localhost:1313/tags/research/>Research</a></li><li><a href=http://localhost:1313/tags/decentralized/>Decentralized</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/writing/intro-to-gpus/><span class=title>« Prev</span><br><span>Intro to GPUs For the Research Oriented</span>
</a><a class=next href=http://localhost:1313/writing/20b-tokens-of-what/><span class=title>Next »</span><br><span>Twenty Billion Tokens of What, Exactly?</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=http://localhost:1313/>Shane Caldwell</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><style>.copy-code{display:inline-flex;align-items:center;justify-content:center;width:32px;height:32px;background:var(--tertiary);border:1px solid var(--border);border-radius:6px;color:var(--secondary);cursor:pointer;transition:all .2s ease;position:absolute;top:8px;right:8px;z-index:10}.copy-code:hover{background:var(--secondary);color:var(--theme)}.copy-code svg{width:16px;height:16px}.copy,.highlight .copy{display:none!important}pre{position:relative}</style><script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll('.copy, [class*="copy"]').forEach(e=>{e.classList.contains("copy-code")||e.remove()});const e=document.querySelectorAll("pre");e.forEach(e=>{const t=e.cloneNode(!0);e.parentNode.replaceChild(t,e)}),document.querySelectorAll("pre code").forEach(e=>{const n=e.parentElement;if(n.querySelector(".copy-code"))return;const t=document.createElement("button");t.classList.add("copy-code"),t.setAttribute("aria-label","Copy code"),t.setAttribute("type","button");const s=`<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"/><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"/></svg>`,a=`<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6L9 17l-5-5"/></svg>`;t.innerHTML=s;function o(){t.innerHTML=a,t.style.color="#10b981",setTimeout(()=>{t.innerHTML=s,t.style.color=""},2e3)}t.addEventListener("click",function(t){t.preventDefault(),t.stopPropagation();const n=e.textContent||e.innerText;navigator.clipboard?navigator.clipboard.writeText(n).then(()=>{o()}).catch(()=>{i(n)}):i(n)});function i(e){const t=document.createElement("textarea");t.value=e,t.style.position="fixed",t.style.opacity="0",document.body.appendChild(t),t.select();try{document.execCommand("copy"),o()}catch(e){console.error("Copy failed:",e)}document.body.removeChild(t)}n.appendChild(t)})})</script><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},chtml:{scale:1,mtextInheritFont:!1,matchFontHeight:!1},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]},startup:{pageReady:()=>MathJax.startup.defaultPageReady().then(()=>{document.querySelectorAll("mjx-container").forEach(e=>{e.style.overflow="visible"})})}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js></script><style>.footnote-popup{position:absolute;background:var(--theme);border:1px solid var(--border);border-radius:6px;padding:12px 16px;max-width:300px;font-size:.85em;line-height:1.4;z-index:1000;box-shadow:0 4px 12px rgba(0,0,0,.15);font-family:var(--font-mono);color:var(--primary);display:none;pointer-events:auto;word-wrap:break-word}.dark .footnote-popup{background:#2d2d2d;box-shadow:0 4px 12px rgba(0,0,0,.3)}.footnote-popup::before{content:'';position:absolute;top:-6px;left:50%;transform:translateX(-50%);width:12px;height:12px;background:var(--theme);border:1px solid var(--border);border-bottom:none;border-right:none;rotate:45deg}.dark .footnote-popup::before{background:#2d2d2d}.footnote-ref{text-decoration:none!important;font-weight:700;padding:2px 6px;border-radius:4px;background:var(--primary);color:var(--theme)!important;transition:all .2s ease;position:relative;border:1px solid var(--border);font-size:.8em;line-height:1.2;display:inline-block;min-width:18px;text-align:center;margin:0 1px;vertical-align:baseline}.footnote-ref:hover{background:var(--secondary);color:var(--theme)!important;box-shadow:0 2px 4px rgba(0,0,0,.2)}.dark .footnote-ref{background:#fff;color:#000!important;border:1px solid #666}.dark .footnote-ref:hover{background:#e5e5e5;color:#000!important;box-shadow:0 2px 6px rgba(0,0,0,.4)}</style><script>document.addEventListener("DOMContentLoaded",function(){let e=null,t=null;function s(e,t){const n=document.createElement("div");return n.className="footnote-popup",n.innerHTML=t,document.body.appendChild(n),n}function o(n,s){t&&(clearTimeout(t),t=null);const i=n.getBoundingClientRect(),r=s.getBoundingClientRect();let o=i.left+i.width/2-s.offsetWidth/2,a=i.top-s.offsetHeight-10;o<10&&(o=10),o+s.offsetWidth>window.innerWidth-10&&(o=window.innerWidth-s.offsetWidth-10),a<10&&(a=i.bottom+10),s.style.left=o+window.scrollX+"px",s.style.top=a+window.scrollY+"px",s.style.display="block",e=s}function n(){t=setTimeout(()=>{e&&(e.style.display="none",e=null)},150)}document.querySelectorAll("a.footnote-ref").forEach(e=>{const i=e.getAttribute("href");if(!i)return;const r=document.querySelector(i.replace(/:/g,"\\:"));if(!r)return;const c=r.innerHTML.replace(/<a[^>]*href="#fnref[^"]*"[^>]*>.*?<\/a>/g,"").trim();if(!c)return;const a=s(i,c);e.addEventListener("mouseenter",()=>{o(e,a)}),e.addEventListener("mouseleave",n),a.addEventListener("mouseenter",()=>{t&&(clearTimeout(t),t=null)}),a.addEventListener("mouseleave",n)}),window.addEventListener("scroll",()=>{e&&(e.style.display="none",e=null)})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>