<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Pretraining at home: 20B tokens from 222 hours to 12 | Shane Caldwell</title>
<meta name=keywords content="llms,training,research"><meta name=description content="Optimizing training a Llama 3.2 1B model so we can pretrain in a day without going broke."><meta name=author content="Shane Caldwell"><link rel=canonical href=http://localhost:1313/writing/pretraining-at-home/><link crossorigin=anonymous href=/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon-180x180.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/writing/pretraining-at-home/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=icon type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon-180x180.png><link rel=apple-touch-icon href=/apple-touch-icon.png><link rel="shortcut icon" href=/favicon.ico><meta name=msapplication-TileColor content="#000000"><meta name=theme-color content="#000000"><script data-goatcounter=https://sjcaldwell.goatcounter.com/count async src=//gc.zgo.at/count.js></script><style>:root{--font-mono:'SF Mono', 'Monaco', 'Inconsolata', 'Roboto Mono', 'Source Code Pro', 'Menlo', 'Consolas', monospace}body,html{font-family:var(--font-mono)!important}*:not(mjx-container):not(mjx-container *),*:not(mjx-container):not(mjx-container *)::before,*:not(mjx-container):not(mjx-container *)::after{font-family:var(--font-mono)!important}mjx-container{overflow-x:visible!important;overflow-y:visible!important;overflow:visible!important}mjx-container[display=true]{display:block!important;text-align:center!important;margin:1em auto!important;max-width:100%!important}mjx-container[display=true] mjx-math{display:inline-block!important;text-align:left!important}mjx-container mjx-mtable{display:table!important;margin:0 auto!important;width:auto!important}mjx-container mjx-mtr{display:table-row!important;height:auto!important}mjx-container mjx-mtd{display:table-cell!important;padding:.3em .8em!important;vertical-align:middle!important}mjx-container[display=true] mjx-mtable mjx-mtr{display:table-row!important;white-space:nowrap!important}mjx-container[display=true] mjx-mtable{display:table!important;border-collapse:separate!important;border-spacing:0 .3em!important}mjx-container::-webkit-scrollbar{display:none!important}mjx-container{-ms-overflow-style:none!important;scrollbar-width:none!important}.dark{--primary:#ffffff;--secondary:#e5e5e5;--tertiary:#cccccc}body:not(.dark){--primary:#000000;--secondary:#333333;--tertiary:#666666}.dark a{color:#fff!important;text-decoration:underline}.dark a:hover{color:#e5e5e5!important}code,pre{font-family:var(--font-mono)!important;font-size:.9em}h1,h2,h3,h4,h5,h6{font-weight:700!important;color:var(--primary)!important}.post-meta{color:var(--secondary)!important}button,.button{font-family:var(--font-mono)!important;font-weight:500}.paper-card,.talk-card{background:var(--theme);border:1px solid var(--border);border-radius:8px;padding:24px;margin-bottom:24px;transition:all .2s ease;box-shadow:0 2px 4px rgba(0,0,0,5%)}.paper-card:hover,.talk-card:hover{border-color:var(--secondary);box-shadow:0 4px 8px rgba(0,0,0,.1);transform:translateY(-1px)}.dark .paper-card,.dark .talk-card{background:#1a1a1a;border-color:#333;box-shadow:0 2px 4px rgba(0,0,0,.2)}.dark .paper-card:hover,.dark .talk-card:hover{border-color:#555;box-shadow:0 4px 8px rgba(0,0,0,.3)}.paper-title{margin:0 0 12px!important;font-size:1.25em;line-height:1.3}.paper-title a{color:var(--primary)!important;text-decoration:none;border-bottom:2px solid transparent;transition:border-color .2s ease}.paper-title a:hover{border-bottom-color:var(--primary)}.paper-meta{margin-bottom:16px;font-size:.9em}.paper-authors{color:var(--secondary);margin-bottom:4px;font-weight:500}.paper-date{color:var(--tertiary);font-size:.85em}.paper-abstract{color:var(--primary);line-height:1.5}.paper-abstract p{margin:0}.talk-title{margin:0 0 12px!important;font-size:1.25em;line-height:1.3}.talk-collaborators{margin-bottom:16px;font-size:.9em;color:var(--secondary)}.talk-collaborators p{margin:0}.talk-details{display:flex;flex-direction:column;gap:8px}.talk-event,.talk-recording{font-size:.9em}.talk-recording a{color:var(--primary)!important;text-decoration:underline}.talk-recording a:hover{color:var(--secondary)!important}@media(max-width:768px){.paper-card,.talk-card{padding:16px;margin-bottom:16px}}.twitter-tweet{margin:24px auto!important;max-width:550px!important}.post-content blockquote.twitter-tweet,.post-content div:has(.twitter-tweet){display:flex;justify-content:center;margin:24px 0}.post-content .twitter-tweet iframe{margin:0 auto;display:block}.post-content figure{margin:24px 0;text-align:center}.post-content figure img{margin-bottom:12px;border-radius:6px;box-shadow:0 4px 8px rgba(0,0,0,.1);max-width:100%;height:auto}.dark .post-content figure img{box-shadow:0 4px 8px rgba(0,0,0,.3)}.post-content figcaption{font-family:var(--font-mono)!important;font-size:.85em;color:var(--secondary);font-style:italic;line-height:1.4;margin-top:8px;padding:0 16px}.post-content figcaption p{margin:0;text-align:center}@media(max-width:768px){.post-content figcaption{font-size:.8em;padding:0 8px}.post-content ol{padding-left:24px!important;margin-left:4px!important}.post-content .footnote-ref{margin-left:2px!important;margin-right:3px!important}.post-content{padding-left:20px!important;padding-right:20px!important}.main{padding-left:8px!important;padding-right:8px!important}}</style><meta property="og:url" content="http://localhost:1313/writing/pretraining-at-home/"><meta property="og:site_name" content="Shane Caldwell"><meta property="og:title" content="Pretraining at home: 20B tokens from 222 hours to 12"><meta property="og:description" content="Optimizing training a Llama 3.2 1B model so we can pretrain in a day without going broke."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="writing"><meta property="article:published_time" content="2025-11-23T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-23T00:00:00+00:00"><meta property="article:tag" content="Llms"><meta property="article:tag" content="Training"><meta property="article:tag" content="Research"><meta property="og:image" content="http://localhost:1313/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/"><meta name=twitter:title content="Pretraining at home: 20B tokens from 222 hours to 12"><meta name=twitter:description content="Optimizing training a Llama 3.2 1B model so we can pretrain in a day without going broke."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Writing","item":"http://localhost:1313/writing/"},{"@type":"ListItem","position":2,"name":"Pretraining at home: 20B tokens from 222 hours to 12","item":"http://localhost:1313/writing/pretraining-at-home/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Pretraining at home: 20B tokens from 222 hours to 12","name":"Pretraining at home: 20B tokens from 222 hours to 12","description":"Optimizing training a Llama 3.2 1B model so we can pretrain in a day without going broke.","keywords":["llms","training","research"],"articleBody":"Recently I implemented DiLoCo as part of a class on distributed training. The implementation helped me understand data parallelism a lot better. That said, reflecting on my experience over the last month or so, I felt I was leaving a lot on the table. While I trained on a small dataset - enough to verify that DiLoCo was implemented correctly - I hadn’t actually done pretraining. I wasn’t looking at loss curves on a test set, or running any particular evals to look at the quality of the model I trained. I was just looking at the loss go down and seeing how fast data moved around.\nI had internalized that pretraining was essentially a waste of time. Plenty of labs do it, they release great models all the time, and it’s much cheaper to post-train those resulting models. That makes me sound lazy. The more reasonable answer is that pretraining experiments are fiscally irresponsible. Training an 8B or 32B model to a point where it’s “chinchilla optimal”1 is expensive. To get a sense for how expensive, we can look at the training time calculator here.\nLet’s say we want to train an 8B parameter model. Twenty tokens for each parameter in the model leaves us with a desired 160 billion tokens. We’ll assume we’re competent enough to get to 50% MFU. That means we’d be training for 22 days. At the current market rate for cloud H100s on Lambda, paid by the hour, we’re looking at 24 dollars an hour. That means out of pocket, the pretraining of that model to get to the minimum compute-optimal amount of data is $12,672. For one run. Before we talk about storage costs.\nHowever, there’s been a lot of interesting work on “small” language models recently. Take Karpathy’s recent nanochat, working on training to get the best model possible for around ~$800. There’s a certain attraction to this kind of work from an educational perspective. Just understanding every part of the process in miniature is cool. Also, the model’s yours - you can do what you want with it. I’m interested in task-specific local models. My ideal model could run on an edge-device and make 200 tool calls in a row and basically would have to look up everything it wanted to know about the world because it isn’t spending 100B parameters trying to memorize frozen knowledge irrelevant to its task.\nThere’s another attraction altogether for those of us used to “old-fashioned” deep learning work, where a significant amount of time was spent on the modeling itself. I’ve found that architectural decisions of models have started to flutter out of my brain. This new model uses MoE - this one’s got a different attention implementation - this ones got RoPE, etc. Reading the papers released with these models, you get a sense of what’s “in”, and you can even speak to it, but without having implemented it yourself and trained models with it, there’s a certain textbook2 feel to the knowledge. I find I feel less like a machine learning engineer understanding the model design, and more like a mix of a zoologist and cultural anthropologist. I can see what way the fields moving and how the collected adaptions in the resulting environment have made stronger models. They’re just dead facts.\nAccepting that certain things only appear at scale and I’m unlikely to have tens of thousands of dollars sitting around, I want that modeling intuition back. Let’s start basic and say we want to train a 1B parameter dense model to knock the rust off.\nOur goals are:\nWriting a training loop that works Getting a decent MFU Low touch configuration and good experiment tracking In particular, we would like to be able to run multiple experiments a day. So our total wall-clock time-to-train must be under 12 hours.\nModel I wanted to be simple and straightforward and start with just a “regular” dense model. I ended up choosing the architecture/tokenizer for Llama-3.2-1B, for no other reason than I mentally associate it with “normality” for dense models. We’ll be starting from freshly initialized weights.\nCompute We’ll be using Modal for these experiments. I’ve found their SDK extremely easy to use which keeps my iteration speed high. I also love that I can just submit a job and know that when it’s done, the compute will spin down. I sleep easier knowing I’m not burning credits. They also have free storage until 2026, so I’m not worrying about storage costs for at least a month and a half3.\nData For a 1B parameter model, we’d like to have twenty billion training tokens (plus some extras for a validation and test set). This is our first non-trivial endeavor.\nFineWeb is a great pretraining dataset. It’s also really, really large. At 44TB of diskspace and 15 trillion tokens, it’s overkill for what we want. We’d really like a subset of 20B tokens to reach the 20 tokens per parameter rule-of-thumb for chinchila-optimality. This question of what subset of 20B tokens is, I suspect, a really important and interesting one, but we’re mostly going to sidestep it for the moment until we accomplish our initial three objectives. A future post will cover looking at the data and determining how to validate the quality and relevance of those 20B tokens.\nI know I want high quality tokens. The first subset that seemed reasonable is fineweb-edu, which is a subset of fineweb curated to have highly educational data. Unfortunately, it’s about 65 times too large for us at 1.3 trillion tokens.\nThere are many random subsets built out of the dataset. The one that’s closest to the size we’re interested in is 100BT, a measly five times what we’re interested in.\nWhile browsing the data on HuggingFace suggests that there’s no particular order to this dataset, I’m naturally suspicious and wanted to shuffle it. However, we’re not going to download all 97.3M documents to shuffle a sample. HuggingFace allows you to stream samples in. It also provides the ability to shuffle. This provided me enough confidence I was getting random samples from the 100BT subset.\nNow I wanted to make sure I got the correct token count.\nFirst I did it the dumbest way possible and wrote a function that took in the name of the dataset, the tokenizer, and the goal number of tokens. Each sample would be processed sequentially, tokenized, and add up to a specific token count.\nFor the Llama 1B tokenizer looking for 20,000,000,000 tokens, this was going to take about 12 hours. That’s not super surprising because I wasn’t batching the tokenization, so the process was fairly laborious.\nI decided it would be smarter to get a sense of the number of tokens provided by the average document. The function get_avg_token_count_of_document here let me tokenize a sample of 100,000 documents to get a sense of the average and median number of documents in my dataset. Running it I found I got an average token count of 999.32, and a median token count of 616.\nI could now assume each document is going to give me about 999 tokens, which gave me a goal document count of about 20 million. I added another 25% buffer to account for the variance between documents, which gave me a goal of 24M documents. I also chose to shoot for validation and test token counts of 100,000,000 a piece.\nModel Implementation I kept my first implementation pretty vanilla. You can see the original version here. I didn’t do any optimizations to make it memory efficient, but it ran. In my heart, I knew this wouldn’t be the final version that would get me to a complete experiment - I wrote it with naive attention, after all.\nI’m not interested in spending whole heaps of dollars, so I went ahead and launched the job on a single H100. I shot for a sequence length of 4096 and a minibatch size of 16, used gradient accumulation so I could hit my target of one million tokens per batch, and hit an OOM error. I got the same error for 8. And 4. Eventually I realized it was only going to run with a minibatch of 1 (for now).\nWhere did those OOM errors hit?\n# apply rotary position embedding cos, sin = self.rotary_emb(value_states, seq_length) query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin) # repeat k/v heads for GQA key_states = key_states.repeat_interleave(self.num_key_value_groups, dim=1) value_states = value_states.repeat_interleave(self.num_key_value_groups, dim=1) # right here attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim) Calculating attn_weights, obviously. That’s a big matrix. On the bright side, the loss goes down.\nOnly have to wait a week and a half for this bad boy to run.\nI should note here that the val_loss was calculated off of a very small part of my initial validation set. I like getting my loss fairly frequently, and was plotting it every full batch of one million tokens. Because of my minibatch size of one required by the current attention implementation, it was just totally dominating my training time. I decided to replace it with a fixed number of samples - in this case 100, which represents a fraction of a percentage of my 125,125 validation documents. If I was GPU richer, I’d love to set up a job system that would take my model checkpoint, toss it to object storage, and run it against evals without interrupting my training job and posting the results asynchronously as training went. Ray seems to support this out of the box.\nFor now, our focus is on reducing time-to-train and fully utilizing the GPUs we’re paying for, so subsets of subsets it is.\nCalculating MFU: How much GPU are we wasting? Looking at the current state of the code, there’s a lot of optimizations I can think of that would make the run finish faster. The obvious ones that come to mind:\nPretokenizing the dataset to reduce the amount of CPU overhead between batches Moving to BF16 from FP32. Using FlashAttention so I can fit more samples in a minibatch Data parallelism over 8 GPUs gives us a larger effective global batch size. Fusing specific operations or using torch.compile. What I have less of a sense for is how much each of these optimizations actually helps, mostly because I don’t spend a lot of time in the torch debugger improving training jobs - we’ll get to that.\nBefore that, though, there’s a metric we haven’t calculated yet - Model FLOPs Utilization or MFU. Given a particular piece of hardware with a published spec for its maximum throughput, what percentage of that are we achieving? This can be read as a percentage, essentially your observed throughput over the theoretical peak throughput.\nWord on the street is that 50% MFU would be considered pretty good. With all our current inefficiencies, we’re lower than that. Let’s talk about how it’s calculated.\nFirst, we need to know what we’re actually being promised at the hardware level. We can find that from a NVIDIA datasheet.\nThat’s a lot of numbers\nFirst question: which of these columns matters to us? Looking at Modal’s website, we find:\nAll H100 GPUs on the Modal platform are of the SXM variant, as can be verified by examining the power draw in the dashboard or with nvidia-smi.\nCool. Those numbers are higher, so I like that. It does imply by the defensive tone that many other providers would attempt to fool me by randomly assigning me one or the other and charging me the same price for them. Oh well, probably nothing! Onto the rows:\nYou may, like an absolute fool, look at this and with a straight face say to me: “Shane, this is easy to read. This tells us the TF32 Tensor Core, which our model is currently using, gets us 989 TFLOps.”, to which I would say, “Hold on there, pal. There’s an asterisk.”\nThat asterisk suggests these numbers are with sparsity. This leads us to two questions: is sparsity a good thing or bad thing for TFLOP performance, and does our training job count as a sparse or dense job?\nUsing my nigh undefeated understanding of human incentives, I infer that sparsity must be the higher number, or that wouldn’t be in a spec sheet that got past marketing. Some quick googling confirms this, sparse is faster. Under some specific circumstances - that is, when two out of every four contiguous values is zero, sparse tensor cores skip the zero-value calculations, and that halves the number of operations done and makes the effective TFLOPs twice as high.\nSounds great. Does that have anything to do with our training? My similarly undefeated understanding of model architecture suggests that there is no way standard LLM training would conform to this 2:4 ratio. Our matrices are not sparse, and when they are sparse, that sparsity is not structured in such a way to take advantage of this. Some specific pruning during inference might be - if you’re willing to take some accuracy hits - but not training4.\nSo, these values are actually 2x higher than what we would expect to find. That is, TF32 would be 494 TFLOPs. For BF16 (where we’re going) it would be 989.5 TFLOPs. I confirmed this by finding the technical architecture doc, where the dense/sparse split is written out explicitly on page 20.\nPro tip: If you find a table with uglier fonts, it’s more likely to be accurate.\nNow you too can read the basics of NVIDIA specsheets. It won’t make your training faster, but at least you know what you’re paying for. It also gives us the denominator for MFU.\nNow let’s tackle the numerator. We want to know what percentage of our theoretical peak we’re achieving. The easiest way to calculate that is to know how many FLOPs are processed for a single token, and then how many tokens you’re processing.\nTo calculate the model FLOPs per token during training, the rule of thumb is 6 times the number of parameters in your model. We can break that into the forward and backward passes:\nFor the forward pass: let’s assume the general matrix multiply (GEMM) with the feed forward matrices dominates the transformer’s computation (it does). During each matrix multiply, you’re looking at two floating point operations - one multiplication per input dimension, and one add to accumulate them. This is 2 FLOPs per parameter. During the backward pass, you have more computation to do - first computing gradients with respect to activations (backprop) and then computing gradients with respect to weights (for the optimizer step). Each of these costs roughly the same as the forward pass. So $2n$ for forward, $4n$ for backward, for a total of six TFLOPs per token processed.\nFinally, we just need to know how many tokens we saw. That can be more or less complicated depending on how your sequences are designed. We’ll assume here every sample is padded to be length 4096, or is a full-sized sample.\nI’ve got an example you can check out here. Nothing fancy. Basically you define your number of tokens processed for step, and call an update function every time you do the forwards/backwards. In this case the step will refer to minibatch steps/sequence length.\nThen when it’s time to check your MFU, you’re just looking at the number of tokens you processed in your minibatch, multiplied by the TFLOPs you must have done to take the step, divided by the theoretical peak you got from the specsheet. In this case, I started at an MFU of 15%. 40% would be pretty good, 50% would make me very happy, so there’s room to grow there. Since calculating the MFU is done with several approximations, it’s very cheap, so we can just keep it in our training loop without causing problems.\nTurning on the Profiler We’d also benefit from information from the torch profiler, which essentially provides timing and percentage GPU utilization for everything we want to do.\nThe profiler is implemented as a context manager. Last time I profiled pytorch was back in my CV days probably five years ago, and I usually did it on random branches off of main or in notebooks to check my math. I really only used it for inference. It just seemed really heavy to add to the training code itself. Since then, I’ve learned a little more about context managers in python. In-particular, contextlib.nullcontext(). This lets you use a conditional to setup your context manager. You can use the torch profiler when you want to, or this no-op otherwise, meaning you can easily flip the profiler on and off without a performance penalty. Great!\nif config.enable_profiling and global_rank == 0: profiler = torch.profiler.profile( activities=[ torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA, ], schedule=torch.profiler.schedule( wait=config.profiling_wait_steps, warmup=5, active=config.profiling_active_steps, repeat=1, ), on_trace_ready=torch.profiler.tensorboard_trace_handler(config.profiling_dir), record_shapes=True, profile_memory=True, with_stack=True, ) profiler_context = profiler else: profiler_context = contextlib.nullcontext() I configured ten wait steps and five warmup steps with twenty steps for actively profiling. I figured at that point we’d be well into training and the GPU would be warmed up.\nWhat you get out is a pt.trace.json profile. It’s very information dense. You can check it out right from Chrome using chrome://tracing, and it looks like this.\nI don’t know what any of this is, and I’m scared.\nThat’s a bit intimidating for me. Also, it doesn’t give me a big, obvious number to make smaller, just a lot of little ones.\nWhat I actually wanted, it turned out, was tensorboard. It has a plugin that lets you view the torch profiler traces. You can install tensorboard and the plugin like:\nuv add --dev tensorboard uv add --dev torch-tb-profiler Then you can see this much less intimidating and much clearer visualization.\nMake big number go down? That I can do.\nNow we’re talking. I have very simple numbers I would like to make go down. For example, we can see here that 15.5% percent of the profiled time was CPU overhead. We would like that number to vanish nearly to 0. Each time we make a change to our training setup, we’ll see how it effects the MFU and how it effects that CPU overhead figure, and optimizing for those two numbers should get us where we’re really looking to go: minimum wall clock time for our training.\nTLDR: Starting Numbers So, to summarize, with our naive approach we landed at 15% MFU, 15.5% CPU overhead during profiling, and an estimated train time (via calculator) of 222.2 hours with a single H100.\nSingle GPU Optimization Let’s go through them one by one. For each, we’ll track the MFU, GPU memory utilization, and total time-to-train as predicted by the training time calculator.\nBF16 The lowest touch start is BF16. This should reduce the size of the matrices we’re multiplying, allowing us to get through them faster. From MFU’s perspective, it will also increase the peak theoretical TFLOPs as well. So we may expect this number to not move at all or go down, even. However, that should open us up some memory to play with to increase our batch size, which will help our TFLOPs.\nWhile we were in FP32, our memory utilization looked like 97.52% utilization. We’ll change dtype to a parameter of our training job, swap it to bf16 when putting the model on device and let it rip.\nThis is basically a no code change.\nmodel.to(device, dtype=dtype) Running it, our GPU memory starts to hover at around 78%-80%. MFU actually goes up by quite a bit to 40%. This is a little surprising. My best bet is that my minibatch of 1 was so close to the maximum amount the GPU could handle that I was decreasing the efficiency of interleaving writing data to the GPU and processing it. I’m kind of making that up. In the future when I’m a FLOPhead maybe that will make more sense to me. We’ll take it, though.\nTotal time to train: 83.8 hours.\nFlash Attention 2 Our memory usage is a little lower, but we’ve still got the massive bottleneck that is naive attention, which we should work through.\nI decided to go with torch.nn.functional.scaled_dot_product_attention because it’s built right into modern versions of pytorch, and uses flash attention.\nMFU went to 55%, GPU memory usage 25%.\nTotal time to train: 60.6 hours.\nBatch Size With my new available memory, I tried batch sizes 16 and 8, but those still failed. 4 worked a treat, though, and was stable for several hours.\nMFU (on single GPU, mind you) 85%, GPU memory usage 25%.\nTotal time to train: 39.2 hours.\nFunctionally, all we’ve done here is swap out a naive attention implementation for Flash Attention and played around with batch size, and we’ve cut our experiment time by almost two days.\nParallelizing There’s a lot more I could do. Flash Attention 3 and torch.compile seem most obvious, and pre-tokenizing my dataset would also give me some benefits. But the biggest thing holding us back is parallelization.\nFor small models that fit on a single card, we can do distributed training relatively easily. In distributed data parallel training, you place a copy of the model on each GPU. Each GPU gets different data. Everything plays out just about the same, with each GPU doing its own gradient accumulation. Just before the optimizer takes its step, you do an all-reduce on your gradients, averaging the gradients of each worker. Then, when you take your step, each machine will end up with the same copy of the model and get to work on the next data.\nIf this were cost-free, it would provide us a linear speedup. If our 85% MFU held over 8 GPUs, we could train in less than five and a half hours. However, that GPU communication to average the gradients is pretty expensive, and the time we spend doing that average is time we’re not processing any tokens. Our theoretical throughput, however, will rise linearly. So we can expect it to drop somewhat.\nThe first thing you’re going to want to do is use torchrun.\ndef ddp_setup() -\u003e None: # check if nccl is available dist.init_process_group(backend=\"nccl\") torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"])) With torchrun to run your job. Something like.\nfrom torch.distributed.run import parse_args, run args = [ f\"--nproc-per-node={multi_node_gpus}\", \"-m\", \"nanopt.main\", config_path ] run(parse_args(args)) This combination of incantations is going to give you access to a few environmental variables.\nlocal_rank = int(os.environ[\"LOCAL_RANK\"]) world_size = int(os.environ[\"WORLD_SIZE\"]) global_rank = int(os.environ[\"RANK\"]) Local rank is the rank of the GPU on the device. World size is how many GPUs there are, period. Global rank lets you know what GPU you are on a zero indexed list of all the GPUs, particularly if you’re running on a cluster.\nWhile in a previous blog post, I implemented DDP from scratch, we’re going for speed this time, which means making use of the tools pytorch makes available. In this case,\nmodel = LlamaForCausalLM(LlamaConfig()) model.to(device, dtype=dtype) model = torch.nn.parallel.DistributedDataParallel( model, device_ids=[local_rank], output_device=local_rank, ) It would be tedious to go over each and every change you need to make for data parallelization, so I’ll just provide a few tips based on footguns I ran into.\nWhenever you’re going to log something, check whether you’re global rank 0. If you’re going to save the state of your model, check that you’re global rank 0. If you’re printing something because you want to see it later, global rank 0. There’s no need to waste computation or storage by repeating that on every GPU. DistributedDataParallel is wrapping your model. The methods you would usually call on your model may be another layer deeper. The easiest way to get around this is to throw a model.module if hasattr(model, 'module') else model at it. This shows up when you’re checking your state dicts to log the model and that sort of thing. Forward pass still works normally. MFU tracking needs to take into account your world size. Whatever the theoretical peak is on one GPU, your theoretical peak is now linearly scaled by your number of GPUs (assuming homogeneity). I briefly was getting readouts of 120% MFU. Your batches are larger, so I’d recommend scaling your gradients. Can’t hurt. With that, I scaled this job up to 8 GPUs and let it rip.\nFinal Time-To-Train Our final MFU on a single node with eight H100s was 40%. The training time calculator shows that as taking about eleven hours to train. Compared to the 222 hours we started with, that’s pretty good!\nNot bad.\nIt’s hard to finish this blog post, because there’s so much more I know I could do. Pre-tokenize the dataset, play with CUDA buffers, call torch.compile while we warmed up, write a kernel in Triton, figure out what ‘flex attention’ is. Optimizing training jobs is a job in itself, and one I have slightly more appreciation for. I expect I’ll come back to all of the above, but ultimately these optimizations were in service of training small models I want to exist. And for that, what I really need to get into is data.\nIf you want to look at the code, you can check it out here.\nUntil next time.\nIt is also the case that most models are trained beyond chinchilla optimality and continue to see stronger performance, so the calculations that follow can be considered a “minimum non-wasteful bar to clear”. Consider LLama 3 8B being trained on 15 trillion tokens. ↩︎\nDeragatory. ↩︎\nI am confident this story ends with me waking up some day in February and realizing I forgot to delete the volumes, but that’s for another day. ↩︎\nI argued with Gemini and ChatGPT about this for about an hour. ChatGPT told me with a straight face that despite the asterisk, that was just an in-group joke that trips up newbies all the time and that the TFLOPs reported in the above table were dense. After I found a much longer 100 page PDF that showed the dense/sparse values explicitly, it relented. I propose an exciting new benchmark would be testing LLMs against NVIDIA’s marketing. ↩︎\n","wordCount":"4329","inLanguage":"en","image":"http://localhost:1313/","datePublished":"2025-11-23T00:00:00Z","dateModified":"2025-11-23T00:00:00Z","author":{"@type":"Person","name":"Shane Caldwell"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/writing/pretraining-at-home/"},"publisher":{"@type":"Organization","name":"Shane Caldwell","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Shane Caldwell (Alt + H)">Shane Caldwell</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Main><span>Main</span></a></li><li><a href=http://localhost:1313/papers/ title=Papers><span>Papers</span></a></li><li><a href=http://localhost:1313/talks/ title=Talks><span>Talks</span></a></li><li><a href=http://localhost:1313/writing/ title=Writing><span>Writing</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Pretraining at home: 20B tokens from 222 hours to 12</h1><div class=post-description>Optimizing training a Llama 3.2 1B model so we can pretrain in a day without going broke.</div><div class=post-meta><span title='2025-11-23 00:00:00 +0000 UTC'>November 23, 2025</span>&nbsp;·&nbsp;21 min&nbsp;·&nbsp;4329 words&nbsp;·&nbsp;Shane Caldwell</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#model>Model</a></li><li><a href=#compute>Compute</a></li><li><a href=#data>Data</a></li><li><a href=#model-implementation>Model Implementation</a></li><li><a href=#calculating-mfu-how-much-gpu-are-we-wasting>Calculating MFU: How much GPU are we wasting?</a></li><li><a href=#turning-on-the-profiler>Turning on the Profiler</a></li><li><a href=#tldr-starting-numbers>TLDR: Starting Numbers</a></li><li><a href=#single-gpu-optimization>Single GPU Optimization</a><ul><li><a href=#bf16>BF16</a></li><li><a href=#flash-attention-2>Flash Attention 2</a></li><li><a href=#batch-size>Batch Size</a></li><li><a href=#parallelizing>Parallelizing</a></li></ul></li><li><a href=#final-time-to-train>Final Time-To-Train</a></li></ul></nav></div></details></div><div class=post-content><p>Recently <a href=https://hackbot.dad/writing/data-parallelism-for-the-poor/>I implemented DiLoCo</a> as part of a class on <a href=https://maven.com/walk-with-code/scratch-to-scale>distributed training</a>. The implementation helped me understand data parallelism a lot better. That said, reflecting on my experience over the last month or so, I felt I was leaving a lot on the table. While I trained on a small dataset - enough to verify that DiLoCo was implemented correctly - I hadn&rsquo;t actually <em>done</em> pretraining. I wasn&rsquo;t looking at loss curves on a test set, or running any particular evals to look at the quality of the model I trained. I was just looking at the loss go down and seeing how fast data moved around.</p><p>I had internalized that pretraining was essentially a waste of time. Plenty of labs do it, they release great models all the time, and it&rsquo;s much cheaper to post-train those resulting models. That makes me sound lazy. The more reasonable answer is that pretraining experiments are <em>fiscally irresponsible</em>. Training an 8B or 32B model to a point where it&rsquo;s &ldquo;chinchilla optimal&rdquo;<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> is expensive. To get a sense for how expensive, we can look at the training time calculator <a href=https://huggingface.co/spaces/scratchtoscale/training-time-calculator>here</a>.</p><p>Let&rsquo;s say we want to train an 8B parameter model. Twenty tokens for each parameter in the model leaves us with a desired 160 billion tokens. We&rsquo;ll assume we&rsquo;re competent enough to get to 50% MFU. That means we&rsquo;d be training for 22 days. At the current market rate for cloud H100s on <a href=https://lambda.ai/>Lambda</a>, paid by the hour, we&rsquo;re looking at 24 dollars an hour. That means out of pocket, the pretraining of that model to get to the <em>minimum compute-optimal</em> amount of data is <em>$12,672</em>. For <em>one run</em>. Before we talk about storage costs.</p><p>However, there&rsquo;s been a lot of interesting work on &ldquo;small&rdquo; language models recently. Take Karpathy&rsquo;s recent <a href=https://github.com/karpathy/nanochat>nanochat</a>, working on training to get the best model possible for around ~$800. There&rsquo;s a certain attraction to this kind of work from an educational perspective. Just understanding every part of the process in miniature is cool. Also, the model&rsquo;s yours - you can do what you want with it. I&rsquo;m interested in task-specific local models. My ideal model could run on an edge-device and make 200 tool calls in a row and basically would have to look up everything it wanted to know about the world because it isn&rsquo;t spending 100B parameters trying to memorize frozen knowledge irrelevant to its task.</p><p>There&rsquo;s another attraction altogether for those of us used to &ldquo;old-fashioned&rdquo; deep learning work, where a significant amount of time was spent on the modeling itself. I&rsquo;ve found that <em>architectural decisions</em> of models have started to flutter out of my brain. This new model uses MoE - this one&rsquo;s got a different attention implementation - this ones got RoPE, etc. Reading the papers released with these models, you get a sense of what&rsquo;s &ldquo;in&rdquo;, and you can even speak to it, but without having implemented it yourself and trained models with it, there&rsquo;s a certain textbook<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> feel to the knowledge. I find I feel less like a machine learning engineer understanding the model design, and more like a mix of a zoologist and cultural anthropologist. I can see what way the fields moving and how the collected adaptions in the resulting environment have made stronger models. They&rsquo;re just dead facts.</p><p>Accepting that certain things only appear at scale and I&rsquo;m unlikely to have tens of thousands of dollars sitting around, I want that modeling intuition back. Let&rsquo;s start basic and say we want to train a 1B parameter dense model to knock the rust off.</p><p>Our goals are:</p><ol><li>Writing a training loop that works</li><li>Getting a decent MFU</li><li>Low touch configuration and good experiment tracking</li></ol><p>In particular, we would like to be able to run multiple experiments <em>a day</em>. So our total wall-clock time-to-train must be under 12 hours.</p><h2 id=model>Model<a hidden class=anchor aria-hidden=true href=#model>#</a></h2><p>I wanted to be simple and straightforward and start with just a &ldquo;regular&rdquo; dense model. I ended up choosing the architecture/tokenizer for <a href=https://huggingface.co/meta-llama/Llama-3.2-1B>Llama-3.2-1B</a>, for no other reason than I mentally associate it with &ldquo;normality&rdquo; for dense models. We&rsquo;ll be starting from freshly initialized weights.</p><h2 id=compute>Compute<a hidden class=anchor aria-hidden=true href=#compute>#</a></h2><p>We&rsquo;ll be using <a href=https://modal.com/>Modal</a> for these experiments. I&rsquo;ve found their SDK extremely easy to use which keeps my iteration speed high. I also love that I can just submit a job and know that when it&rsquo;s done, the compute will spin down. I sleep easier knowing I&rsquo;m not burning credits. They also have free storage until 2026, so I&rsquo;m not worrying about storage costs for at least a month and a half<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p><h2 id=data>Data<a hidden class=anchor aria-hidden=true href=#data>#</a></h2><p>For a 1B parameter model, we&rsquo;d like to have twenty billion training tokens (plus some extras for a validation and test set). This is our first non-trivial endeavor.</p><p><a href=https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1>FineWeb</a> is a great pretraining dataset. It&rsquo;s also really, really large. At 44TB of diskspace and 15 trillion tokens, it&rsquo;s overkill for what we want. We&rsquo;d really like a subset of 20B tokens to reach the 20 tokens per parameter rule-of-thumb for chinchila-optimality. This question of <em>what</em> subset of 20B tokens is, I suspect, a really important and interesting one, but we&rsquo;re mostly going to sidestep it for the moment until we accomplish our initial three objectives. A future post will cover looking at the data and determining how to validate the quality and relevance of those 20B tokens.</p><p>I know I want <em>high quality</em> tokens. The first subset that seemed reasonable is <a href=https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu>fineweb-edu</a>, which is a subset of fineweb curated to have highly educational data. Unfortunately, it&rsquo;s about 65 times too large for us at 1.3 trillion tokens.</p><p>There are many random subsets built out of the dataset. The one that&rsquo;s closest to the size we&rsquo;re interested in is <a href=https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/viewer/sample-100BT>100BT</a>, a measly five times what we&rsquo;re interested in.</p><p>While browsing the data on HuggingFace suggests that there&rsquo;s no particular order to this dataset, I&rsquo;m naturally suspicious and wanted to shuffle it. However, we&rsquo;re not going to download all 97.3M documents to shuffle a sample. HuggingFace allows you to stream samples in. It also provides the ability to shuffle. This provided me enough confidence I was getting <em>random</em> samples from the 100BT subset.</p><p>Now I wanted to make sure I got the correct token count.</p><p>First I did it the dumbest way possible and wrote a function that took in the name of the dataset, the tokenizer, and the goal number of tokens. Each sample would be processed sequentially, tokenized, and add up to a specific token count.</p><p>For the Llama 1B tokenizer looking for 20,000,000,000 tokens, this was going to take about 12 hours. That&rsquo;s not super surprising because I wasn&rsquo;t batching the tokenization, so the process was fairly laborious.</p><p>I decided it would be smarter to get a sense of the number of tokens provided by the average document. The function <code>get_avg_token_count_of_document</code> <a href=https://github.com/SJCaldwell/nanoPT/blob/main/scripts/01_create_data_volume.py>here</a> let me tokenize a sample of 100,000 documents to get a sense of the average and median number of documents in my dataset. Running it I found I got an average token count of 999.32, and a median token count of 616.</p><p>I could now assume each document is going to give me about 999 tokens, which gave me a goal document count of about 20 million. I added another 25% buffer to account for the variance between documents, which gave me a goal of 24M documents. I also chose to shoot for validation and test token counts of 100,000,000 a piece.</p><h2 id=model-implementation>Model Implementation<a hidden class=anchor aria-hidden=true href=#model-implementation>#</a></h2><p>I kept my first implementation pretty vanilla. You can see the original version <a href=https://github.com/SJCaldwell/nanoPT/commit/9eeac6b1038efac56275e3a0a2d8513e5ce1e737>here</a>. I didn&rsquo;t do any optimizations to make it memory efficient, but it ran. In my heart, I knew this wouldn&rsquo;t be the final version that would get me to a complete experiment - I wrote it with naive attention, after all.</p><p>I&rsquo;m not interested in spending whole heaps of dollars, so I went ahead and launched the job on a single H100. I shot for a sequence length of 4096 and a minibatch size of 16, used gradient accumulation so I could hit my target of one million tokens per batch, and hit an OOM error. I got the same error for 8. And 4. Eventually I realized it was only going to run with a minibatch of 1 (for now).</p><p>Where did those OOM errors hit?</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># apply rotary position embedding</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>cos</span><span class=p>,</span> <span class=n>sin</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>rotary_emb</span><span class=p>(</span><span class=n>value_states</span><span class=p>,</span> <span class=n>seq_length</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>query_states</span><span class=p>,</span> <span class=n>key_states</span> <span class=o>=</span> <span class=n>apply_rotary_pos_emb</span><span class=p>(</span><span class=n>query_states</span><span class=p>,</span> <span class=n>key_states</span><span class=p>,</span> <span class=n>cos</span><span class=p>,</span> <span class=n>sin</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># repeat k/v heads for GQA</span>
</span></span><span class=line><span class=cl><span class=n>key_states</span> <span class=o>=</span> <span class=n>key_states</span><span class=o>.</span><span class=n>repeat_interleave</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>num_key_value_groups</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>value_states</span> <span class=o>=</span> <span class=n>value_states</span><span class=o>.</span><span class=n>repeat_interleave</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>num_key_value_groups</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># right here</span>
</span></span><span class=line><span class=cl><span class=n>attn_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>query_states</span><span class=p>,</span> <span class=n>key_states</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span></code></pre></div><p>Calculating <code>attn_weights</code>, obviously. That&rsquo;s a big matrix. On the bright side, the loss goes down.</p><figure><img loading=lazy src=initial_training_run.png alt="Only have to wait a week and a half for this bad boy to run."><figcaption><p>Only have to wait a week and a half for this bad boy to run.</p></figcaption></figure><p>I should note here that the <code>val_loss</code> was calculated off of a very small part of my initial validation set. I like getting my loss fairly frequently, and was plotting it every full batch of one million tokens. Because of my minibatch size of one required by the current attention implementation, it was just totally dominating my training time. I decided to replace it with a fixed number of samples - in this case 100, which represents a fraction of a percentage of my 125,125 validation documents. If I was GPU richer, I&rsquo;d love to set up a job system that would take my model checkpoint, toss it to object storage, and run it against evals without interrupting my training job and posting the results asynchronously as training went. <a href=https://docs.ray.io/en/latest/train/user-guides/asynchronous-validation.html>Ray seems to support this out of the box</a>.</p><p>For now, our focus is on reducing time-to-train and fully utilizing the GPUs we&rsquo;re paying for, so subsets of subsets it is.</p><h2 id=calculating-mfu-how-much-gpu-are-we-wasting>Calculating MFU: How much GPU are we wasting?<a hidden class=anchor aria-hidden=true href=#calculating-mfu-how-much-gpu-are-we-wasting>#</a></h2><p>Looking at the current state of the code, there&rsquo;s a lot of optimizations I can think of that would make the run finish faster. The obvious ones that come to mind:</p><ol><li>Pretokenizing the dataset to reduce the amount of CPU overhead between batches</li><li>Moving to BF16 from FP32.</li><li>Using FlashAttention so I can fit more samples in a minibatch</li><li>Data parallelism over 8 GPUs gives us a larger effective global batch size.</li><li>Fusing specific operations or using <code>torch.compile</code>.</li></ol><p>What I have less of a sense for is how much each of these optimizations actually helps, mostly because I don&rsquo;t spend a lot of time in the torch debugger improving training jobs - we&rsquo;ll get to that.</p><p>Before that, though, there&rsquo;s a metric we haven&rsquo;t calculated yet - Model FLOPs Utilization or MFU. Given a particular piece of hardware with a published spec for its maximum throughput, what percentage of that are we achieving? This can be read as a percentage, essentially your observed throughput over the theoretical peak throughput.</p><p>Word on the street is that 50% MFU would be considered <em>pretty good</em>. With all our current inefficiencies, we&rsquo;re lower than that. Let&rsquo;s talk about how it&rsquo;s calculated.</p><p>First, we need to know what we&rsquo;re actually being promised at the hardware level. We can find that from a <a href=https://resources.nvidia.com/en-us-gpu-resources/h100-datasheet-24306>NVIDIA datasheet</a>.</p><figure><img loading=lazy src=nvidia_h100.png alt="That&rsquo;s a lot of numbers"><figcaption><p>That&rsquo;s a lot of numbers</p></figcaption></figure><p>First question: which of these columns matters to us? Looking at Modal&rsquo;s website, we find:</p><blockquote><p>All H100 GPUs on the Modal platform are of the SXM variant, as can be verified by examining the power draw in the dashboard or with <code>nvidia-smi</code>.</p></blockquote><p>Cool. Those numbers are higher, so I like that. It does imply by the defensive tone that many other providers would attempt to fool me by randomly assigning me one or the other and charging me the same price for them. Oh well, probably nothing! Onto the rows:</p><p>You may, like an absolute fool, look at this and with a straight face say to me: &ldquo;Shane, this is easy to read. This tells us the TF32 Tensor Core, which our model is currently using, gets us 989 TFLOps.&rdquo;, to which I would say, &ldquo;Hold on there, pal. There&rsquo;s an asterisk.&rdquo;</p><p>That asterisk suggests these numbers are <em>with sparsity</em>. This leads us to two questions: is sparsity a good thing or bad thing for TFLOP performance, and does our training job count as a sparse or dense job?</p><p>Using my nigh undefeated understanding of human incentives, I infer that sparsity <em>must</em> be the higher number, or that wouldn&rsquo;t be in a spec sheet that got past marketing. Some quick googling confirms this, sparse is faster. Under some specific circumstances - that is, when two out of every four contiguous values is zero, sparse tensor cores skip the zero-value calculations, and that halves the number of operations done and makes the effective TFLOPs twice as high.</p><p>Sounds great. Does that have anything to do with our training? My similarly undefeated understanding of model architecture suggests that there is <em>no way</em> standard LLM training would conform to this 2:4 ratio. Our matrices are not sparse, and when they <em>are</em> sparse, that sparsity is not structured in such a way to take advantage of this. Some specific pruning during inference might be - if you&rsquo;re willing to take some accuracy hits - but not training<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>.</p><p>So, these values are actually <em>2x higher</em> than what we would expect to find. That is, TF32 would be 494 TFLOPs. For BF16 (where we&rsquo;re going) it would be 989.5 TFLOPs. I confirmed this by finding the <a href=https://www.techpowerup.com/gpu-specs/docs/nvidia-gh100-architecture.pdf>technical architecture doc</a>, where the dense/sparse split is written out explicitly on page 20.</p><figure><img loading=lazy src=actual_spec_sheet.png alt="Pro tip: If you find a table with uglier fonts, it&rsquo;s more likely to be accurate."><figcaption><p>Pro tip: If you find a table with uglier fonts, it&rsquo;s more likely to be accurate.</p></figcaption></figure><p>Now you too can read the basics of NVIDIA specsheets. It won&rsquo;t make your training faster, but at least you know what you&rsquo;re paying for. It also gives us the <em>denominator</em> for MFU.</p><p>Now let&rsquo;s tackle the numerator. We want to know what percentage of our theoretical peak we&rsquo;re achieving. The easiest way to calculate that is to know how many FLOPs are processed for a single token, and then how many tokens you&rsquo;re processing.</p><p>To calculate the model FLOPs per token during training, the rule of thumb is 6 times the number of parameters in your model. We can break that into the forward and backward passes:</p><p>For the forward pass: let&rsquo;s assume the general matrix multiply (GEMM) with the feed forward matrices dominates the transformer&rsquo;s computation (it does). During each matrix multiply, you&rsquo;re looking at two floating point operations - one multiplication per input dimension, and one add to accumulate them. This is 2 FLOPs per parameter. During the backward pass, you have more computation to do - first computing gradients with respect to activations (backprop) and then computing gradients with respect to weights (for the optimizer step). Each of these costs roughly the same as the forward pass. So $2n$ for forward, $4n$ for backward, for a total of six TFLOPs per token processed.</p><p>Finally, we just need to know how many tokens we saw. That can be more or less complicated depending on how your sequences are designed. We&rsquo;ll assume here every sample is padded to be length 4096, or is a full-sized sample.</p><p>I&rsquo;ve got an example you can check out <a href=https://github.com/SJCaldwell/nanoPT/blob/main/nanopt/profiling/track_mfu.py>here</a>. Nothing fancy. Basically you define your number of tokens processed for step, and call an <code>update</code> function every time you do the forwards/backwards. In this case the step will refer to minibatch steps/sequence length.</p><p>Then when it&rsquo;s time to check your MFU, you&rsquo;re just looking at the number of tokens you processed in your minibatch, multiplied by the TFLOPs you must have done to take the step, divided by the theoretical peak you got from the specsheet. In this case, I started at an MFU of 15%. 40% would be pretty good, 50% would make me very happy, so there&rsquo;s room to grow there. Since calculating the MFU is done with several approximations, it&rsquo;s very cheap, so we can just keep it in our training loop without causing problems.</p><h2 id=turning-on-the-profiler>Turning on the Profiler<a hidden class=anchor aria-hidden=true href=#turning-on-the-profiler>#</a></h2><p>We&rsquo;d also benefit from information from the torch profiler, which essentially provides timing and percentage GPU utilization for everything we want to do.</p><p>The profiler is implemented as a context manager. Last time I profiled pytorch was back in my CV days probably five years ago, and I usually did it on random branches off of main or in notebooks to check my math. I really only used it for inference. It just seemed really heavy to add to the training code itself. Since then, I&rsquo;ve learned a little more about context managers in python. In-particular, <code>contextlib.nullcontext()</code>. This lets you use a conditional to setup your context manager. You can use the torch profiler when you want to, or this no-op otherwise, meaning you can easily flip the profiler on and off without a performance penalty. Great!</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>enable_profiling</span> <span class=ow>and</span> <span class=n>global_rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>	<span class=n>profiler</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>profiler</span><span class=o>.</span><span class=n>profile</span><span class=p>(</span>
</span></span><span class=line><span class=cl>		<span class=n>activities</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>			<span class=n>torch</span><span class=o>.</span><span class=n>profiler</span><span class=o>.</span><span class=n>ProfilerActivity</span><span class=o>.</span><span class=n>CPU</span><span class=p>,</span>
</span></span><span class=line><span class=cl>			<span class=n>torch</span><span class=o>.</span><span class=n>profiler</span><span class=o>.</span><span class=n>ProfilerActivity</span><span class=o>.</span><span class=n>CUDA</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=p>],</span>
</span></span><span class=line><span class=cl>		<span class=n>schedule</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>profiler</span><span class=o>.</span><span class=n>schedule</span><span class=p>(</span>
</span></span><span class=line><span class=cl>			<span class=n>wait</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>profiling_wait_steps</span><span class=p>,</span>
</span></span><span class=line><span class=cl>			<span class=n>warmup</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>			<span class=n>active</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>profiling_active_steps</span><span class=p>,</span>
</span></span><span class=line><span class=cl>			<span class=n>repeat</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=p>),</span>
</span></span><span class=line><span class=cl>		<span class=n>on_trace_ready</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>profiler</span><span class=o>.</span><span class=n>tensorboard_trace_handler</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>profiling_dir</span><span class=p>),</span>
</span></span><span class=line><span class=cl>		<span class=n>record_shapes</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=n>profile_memory</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=n>with_stack</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	<span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>profiler_context</span> <span class=o>=</span> <span class=n>profiler</span>
</span></span><span class=line><span class=cl><span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>	<span class=n>profiler_context</span> <span class=o>=</span> <span class=n>contextlib</span><span class=o>.</span><span class=n>nullcontext</span><span class=p>()</span>
</span></span></code></pre></div><p>I configured ten wait steps and five warmup steps with twenty steps for actively profiling. I figured at that point we&rsquo;d be well into training and the GPU would be warmed up.</p><p>What you get out is a <code>pt.trace.json</code> profile. It&rsquo;s very information dense. You can check it out right from Chrome using <code>chrome://tracing</code>, and it looks like this.</p><figure><img loading=lazy src=chrome_trace_viewer.png alt="I don&rsquo;t know what any of this is, and I&rsquo;m scared."><figcaption><p>I don&rsquo;t know what any of this is, and I&rsquo;m scared.</p></figcaption></figure><p>That&rsquo;s a bit intimidating for me. Also, it doesn&rsquo;t give me a big, obvious number to make smaller, just a lot of little ones.</p><p>What I actually wanted, it turned out, was tensorboard. It has a plugin that lets you view the torch profiler traces. You can install tensorboard and the plugin like:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>uv add --dev tensorboard
</span></span><span class=line><span class=cl>uv add --dev torch-tb-profiler
</span></span></code></pre></div><p>Then you can see this much less intimidating and much clearer visualization.</p><figure><img loading=lazy src=torch_profiler_start.png alt="Make big number go down? That I can do."><figcaption><p>Make big number go down? That I can do.</p></figcaption></figure><p><em>Now</em> we&rsquo;re talking. I have very simple numbers I would like to make go down. For example, we can see here that 15.5% percent of the profiled time was CPU overhead. We would like that number to vanish nearly to 0. Each time we make a change to our training setup, we&rsquo;ll see how it effects the MFU and how it effects that CPU overhead figure, and optimizing for those two numbers should get us where we&rsquo;re really looking to go: minimum wall clock time for our training.</p><h2 id=tldr-starting-numbers>TLDR: Starting Numbers<a hidden class=anchor aria-hidden=true href=#tldr-starting-numbers>#</a></h2><p>So, to summarize, with our naive approach we landed at <strong>15% MFU</strong>, <strong>15.5% CPU overhead</strong> during profiling, and an estimated train time (via <a href=https://huggingface.co/spaces/scratchtoscale/training-time-calculator>calculator</a>) of 222.2 hours with a single H100.</p><h2 id=single-gpu-optimization>Single GPU Optimization<a hidden class=anchor aria-hidden=true href=#single-gpu-optimization>#</a></h2><p>Let&rsquo;s go through them one by one. For each, we&rsquo;ll track the MFU, GPU memory utilization, and total time-to-train as predicted by the training time calculator.</p><h3 id=bf16>BF16<a hidden class=anchor aria-hidden=true href=#bf16>#</a></h3><p>The lowest touch start is BF16. This should reduce the size of the matrices we&rsquo;re multiplying, allowing us to get through them faster. From MFU&rsquo;s perspective, it will also <em>increase</em> the peak theoretical TFLOPs as well. So we may expect this number to not move at all or go down, even. However, that should open us up some memory to play with to increase our batch size, which <em>will</em> help our TFLOPs.</p><p>While we were in FP32, our memory utilization looked like 97.52% utilization. We&rsquo;ll change <code>dtype</code> to a parameter of our training job, swap it to bf16 when putting the model on device and let it rip.</p><p>This is basically a no code change.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span></code></pre></div><p>Running it, our GPU memory starts to hover at around 78%-80%. MFU actually goes up by quite a bit to 40%. This is a little surprising. My best bet is that my minibatch of 1 was so close to the maximum amount the GPU could handle that I was decreasing the efficiency of interleaving writing data to the GPU and processing it. I&rsquo;m kind of making that up. In the future when I&rsquo;m a FLOPhead maybe that will make more sense to me. We&rsquo;ll take it, though.</p><p>Total time to train: 83.8 hours.</p><h3 id=flash-attention-2>Flash Attention 2<a hidden class=anchor aria-hidden=true href=#flash-attention-2>#</a></h3><p>Our memory usage is a little lower, but we&rsquo;ve still got the massive bottleneck that is naive attention, which we should work through.</p><p>I decided to go with <code>torch.nn.functional.scaled_dot_product_attention</code> because it&rsquo;s built right into modern versions of pytorch, and uses flash attention.</p><p>MFU went to 55%, GPU memory usage 25%.</p><p>Total time to train: 60.6 hours.</p><h3 id=batch-size>Batch Size<a hidden class=anchor aria-hidden=true href=#batch-size>#</a></h3><p>With my new available memory, I tried batch sizes 16 and 8, but those still failed. 4 worked a treat, though, and was stable for several hours.</p><p>MFU (on single GPU, mind you) 85%, GPU memory usage 25%.</p><p>Total time to train: 39.2 hours.</p><p>Functionally, all we&rsquo;ve done here is swap out a naive attention implementation for Flash Attention and played around with batch size, and we&rsquo;ve cut our experiment time by almost two days.</p><h3 id=parallelizing>Parallelizing<a hidden class=anchor aria-hidden=true href=#parallelizing>#</a></h3><p>There&rsquo;s a lot more I could do. Flash Attention 3 and torch.compile seem most obvious, and pre-tokenizing my dataset would also give me some benefits. But the biggest thing holding us back is parallelization.</p><p>For small models that fit on a single card, we can do distributed training relatively easily. In distributed data parallel training, you place a copy of the model on each GPU. Each GPU gets different data. Everything plays out just about the same, with each GPU doing its own gradient accumulation. Just before the optimizer takes its step, you do an all-reduce on your gradients, averaging the gradients of each worker. Then, when you take your step, each machine will end up with the same copy of the model and get to work on the next data.</p><p>If this were cost-free, it would provide us a linear speedup. If our 85% MFU held over 8 GPUs, we could train in less than five and a half hours. However, that GPU communication to average the gradients is pretty expensive, and the time we spend doing that average is time we&rsquo;re not processing any tokens. Our theoretical throughput, however, <em>will</em> rise linearly. So we can expect it to drop somewhat.</p><p>The first thing you&rsquo;re going to want to do is use <code>torchrun</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>ddp_setup</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=c1># check if nccl is available</span>
</span></span><span class=line><span class=cl>	<span class=n>dist</span><span class=o>.</span><span class=n>init_process_group</span><span class=p>(</span><span class=n>backend</span><span class=o>=</span><span class=s2>&#34;nccl&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>set_device</span><span class=p>(</span><span class=nb>int</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;LOCAL_RANK&#34;</span><span class=p>]))</span>
</span></span></code></pre></div><p>With <code>torchrun</code> to run your job. Something like.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.distributed.run</span> <span class=kn>import</span> <span class=n>parse_args</span><span class=p>,</span> <span class=n>run</span>
</span></span><span class=line><span class=cl>    <span class=n>args</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=sa>f</span><span class=s2>&#34;--nproc-per-node=</span><span class=si>{</span><span class=n>multi_node_gpus</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;-m&#34;</span><span class=p>,</span> <span class=s2>&#34;nanopt.main&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>config_path</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>run</span><span class=p>(</span><span class=n>parse_args</span><span class=p>(</span><span class=n>args</span><span class=p>))</span>
</span></span></code></pre></div><p>This combination of incantations is going to give you access to a few environmental variables.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>local_rank</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;LOCAL_RANK&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>world_size</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;WORLD_SIZE&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>global_rank</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;RANK&#34;</span><span class=p>])</span>
</span></span></code></pre></div><p>Local rank is the rank of the GPU on the device. World size is how many GPUs there are, period. Global rank lets you know what GPU you are on a zero indexed list of all the GPUs, particularly if you&rsquo;re running on a cluster.</p><p>While in a <a href=https://hackbot.dad/writing/data-parallelism-for-the-poor/>previous blog post</a>, I implemented DDP from scratch, we&rsquo;re going for speed this time, which means making use of the tools pytorch makes available. In this case,</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>LlamaForCausalLM</span><span class=p>(</span><span class=n>LlamaConfig</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>parallel</span><span class=o>.</span><span class=n>DistributedDataParallel</span><span class=p>(</span>
</span></span><span class=line><span class=cl>	<span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	<span class=n>device_ids</span><span class=o>=</span><span class=p>[</span><span class=n>local_rank</span><span class=p>],</span>
</span></span><span class=line><span class=cl>	<span class=n>output_device</span><span class=o>=</span><span class=n>local_rank</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>It would be tedious to go over each and every change you need to make for data parallelization, so I&rsquo;ll just provide a few tips based on footguns I ran into.</p><ol><li>Whenever you&rsquo;re going to log something, check whether you&rsquo;re global rank 0. If you&rsquo;re going to save the state of your model, check that you&rsquo;re global rank 0. If you&rsquo;re printing something because you want to see it later, global rank 0. There&rsquo;s no need to waste computation or storage by repeating that on every GPU.</li><li><code>DistributedDataParallel</code> is wrapping your model. The methods you would usually call on your model may be another layer deeper. The easiest way to get around this is to throw a <code>model.module if hasattr(model, 'module') else model</code> at it. This shows up when you&rsquo;re checking your state dicts to log the model and that sort of thing. Forward pass still works normally.</li><li>MFU tracking needs to take into account your world size. Whatever the theoretical peak is on one GPU, your theoretical peak is now linearly scaled by your number of GPUs (assuming homogeneity). I briefly was getting readouts of 120% MFU.</li><li>Your batches are larger, so I&rsquo;d recommend scaling your gradients. Can&rsquo;t hurt.</li></ol><p>With that, I scaled this job up to 8 GPUs and let it rip.</p><h2 id=final-time-to-train>Final Time-To-Train<a hidden class=anchor aria-hidden=true href=#final-time-to-train>#</a></h2><p>Our final MFU on a single node with eight H100s was 40%. The training time calculator shows that as taking about eleven hours to train. Compared to the 222 hours we started with, that&rsquo;s pretty good!</p><figure><img loading=lazy src=full_training_run.png alt="Not bad."><figcaption><p>Not bad.</p></figcaption></figure><p>It&rsquo;s hard to finish this blog post, because there&rsquo;s so much more I know I could do. Pre-tokenize the dataset, play with CUDA buffers, call <code>torch.compile</code> while we warmed up, write a kernel in Triton, figure out what &lsquo;flex attention&rsquo; is. Optimizing training jobs is a job in itself, and one I have slightly more appreciation for. I expect I&rsquo;ll come back to all of the above, but ultimately these optimizations were in service of training small models I want to exist. And for that, what I really need to get into is <em>data</em>.</p><p>If you want to look at the code, you can check it out <a href=https://github.com/SJCaldwell/nanoPT>here</a>.</p><p>Until next time.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>It is also the case that most models are trained beyond chinchilla optimality and continue to see stronger performance, so the calculations that follow can be considered a &ldquo;minimum non-wasteful bar to clear&rdquo;. Consider LLama 3 8B being trained on <em>15 trillion</em> tokens.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Deragatory.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>I am confident this story ends with me waking up some day in February and realizing I forgot to delete the volumes, but that&rsquo;s for another day.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>I argued with Gemini and ChatGPT about this for about an hour. ChatGPT told me with a straight face that despite the asterisk, that was just an in-group joke that trips up newbies all the time and that the TFLOPs reported in the above table were dense. After I found a much longer 100 page PDF that showed the dense/sparse values explicitly, it relented. I propose an exciting new benchmark would be testing LLMs against NVIDIA&rsquo;s marketing.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/llms/>Llms</a></li><li><a href=http://localhost:1313/tags/training/>Training</a></li><li><a href=http://localhost:1313/tags/research/>Research</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/writing/20b-tokens-of-what/><span class=title>« Prev</span><br><span>Twenty Billion Tokens of What, Exactly?</span>
</a><a class=next href=http://localhost:1313/writing/offsec-evals-dark-forest/><span class=title>Next »</span><br><span>Offsec Evals: Growing Up In The Dark Forest</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Shane Caldwell</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><style>.copy-code{display:inline-flex;align-items:center;justify-content:center;width:32px;height:32px;background:var(--tertiary);border:1px solid var(--border);border-radius:6px;color:var(--secondary);cursor:pointer;transition:all .2s ease;position:absolute;top:8px;right:8px;z-index:10}.copy-code:hover{background:var(--secondary);color:var(--theme)}.copy-code svg{width:16px;height:16px}.copy,.highlight .copy{display:none!important}pre{position:relative}</style><script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll('.copy, [class*="copy"]').forEach(e=>{e.classList.contains("copy-code")||e.remove()});const e=document.querySelectorAll("pre");e.forEach(e=>{const t=e.cloneNode(!0);e.parentNode.replaceChild(t,e)}),document.querySelectorAll("pre code").forEach(e=>{const n=e.parentElement;if(n.querySelector(".copy-code"))return;const t=document.createElement("button");t.classList.add("copy-code"),t.setAttribute("aria-label","Copy code"),t.setAttribute("type","button");const s=`<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"/><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"/></svg>`,a=`<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6L9 17l-5-5"/></svg>`;t.innerHTML=s;function o(){t.innerHTML=a,t.style.color="#10b981",setTimeout(()=>{t.innerHTML=s,t.style.color=""},2e3)}t.addEventListener("click",function(t){t.preventDefault(),t.stopPropagation();const n=e.textContent||e.innerText;navigator.clipboard?navigator.clipboard.writeText(n).then(()=>{o()}).catch(()=>{i(n)}):i(n)});function i(e){const t=document.createElement("textarea");t.value=e,t.style.position="fixed",t.style.opacity="0",document.body.appendChild(t),t.select();try{document.execCommand("copy"),o()}catch(e){console.error("Copy failed:",e)}document.body.removeChild(t)}n.appendChild(t)})})</script><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},chtml:{scale:1,mtextInheritFont:!1,matchFontHeight:!1},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]},startup:{pageReady:()=>MathJax.startup.defaultPageReady().then(()=>{document.querySelectorAll("mjx-container").forEach(e=>{e.style.overflow="visible"})})}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js></script><style>.footnote-popup{position:absolute;background:var(--theme);border:1px solid var(--border);border-radius:6px;padding:12px 16px;max-width:300px;font-size:.85em;line-height:1.4;z-index:1000;box-shadow:0 4px 12px rgba(0,0,0,.15);font-family:var(--font-mono);color:var(--primary);display:none;pointer-events:auto;word-wrap:break-word}.dark .footnote-popup{background:#2d2d2d;box-shadow:0 4px 12px rgba(0,0,0,.3)}.footnote-popup::before{content:'';position:absolute;top:-6px;left:50%;transform:translateX(-50%);width:12px;height:12px;background:var(--theme);border:1px solid var(--border);border-bottom:none;border-right:none;rotate:45deg}.dark .footnote-popup::before{background:#2d2d2d}.footnote-ref{text-decoration:none!important;font-weight:700;padding:2px 6px;border-radius:4px;background:var(--primary);color:var(--theme)!important;transition:all .2s ease;position:relative;border:1px solid var(--border);font-size:.8em;line-height:1.2;display:inline-block;min-width:18px;text-align:center;margin:0 1px;vertical-align:baseline}.footnote-ref:hover{background:var(--secondary);color:var(--theme)!important;box-shadow:0 2px 4px rgba(0,0,0,.2)}.dark .footnote-ref{background:#fff;color:#000!important;border:1px solid #666}.dark .footnote-ref:hover{background:#e5e5e5;color:#000!important;box-shadow:0 2px 6px rgba(0,0,0,.4)}</style><script>document.addEventListener("DOMContentLoaded",function(){let e=null,t=null;function s(e,t){const n=document.createElement("div");return n.className="footnote-popup",n.innerHTML=t,document.body.appendChild(n),n}function o(n,s){t&&(clearTimeout(t),t=null);const i=n.getBoundingClientRect(),r=s.getBoundingClientRect();let o=i.left+i.width/2-s.offsetWidth/2,a=i.top-s.offsetHeight-10;o<10&&(o=10),o+s.offsetWidth>window.innerWidth-10&&(o=window.innerWidth-s.offsetWidth-10),a<10&&(a=i.bottom+10),s.style.left=o+window.scrollX+"px",s.style.top=a+window.scrollY+"px",s.style.display="block",e=s}function n(){t=setTimeout(()=>{e&&(e.style.display="none",e=null)},150)}document.querySelectorAll("a.footnote-ref").forEach(e=>{const i=e.getAttribute("href");if(!i)return;const r=document.querySelector(i.replace(/:/g,"\\:"));if(!r)return;const c=r.innerHTML.replace(/<a[^>]*href="#fnref[^"]*"[^>]*>.*?<\/a>/g,"").trim();if(!c)return;const a=s(i,c);e.addEventListener("mouseenter",()=>{o(e,a)}),e.addEventListener("mouseleave",n),a.addEventListener("mouseenter",()=>{t&&(clearTimeout(t),t=null)}),a.addEventListener("mouseleave",n)}),window.addEventListener("scroll",()=>{e&&(e.style.display="none",e=null)})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>