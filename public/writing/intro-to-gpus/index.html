<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Intro to GPUs For the Research Oriented | Shane Caldwell</title>
<meta name=keywords content="llms,systems"><meta name=description content="Getting comfortable with the hardware on a quest for more MFU."><meta name=author content="Shane Caldwell"><link rel=canonical href=http://localhost:1313/writing/intro-to-gpus/><link crossorigin=anonymous href=/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon-180x180.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/writing/intro-to-gpus/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=icon type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon-180x180.png><link rel=apple-touch-icon href=/apple-touch-icon.png><link rel="shortcut icon" href=/favicon.ico><meta name=msapplication-TileColor content="#000000"><meta name=theme-color content="#000000"><script data-goatcounter=https://sjcaldwell.goatcounter.com/count async src=//gc.zgo.at/count.js></script><style>:root{--font-mono:'SF Mono', 'Monaco', 'Inconsolata', 'Roboto Mono', 'Source Code Pro', 'Menlo', 'Consolas', monospace}body,html{font-family:var(--font-mono)!important}*:not(mjx-container):not(mjx-container *),*:not(mjx-container):not(mjx-container *)::before,*:not(mjx-container):not(mjx-container *)::after{font-family:var(--font-mono)!important}mjx-container{overflow-x:visible!important;overflow-y:visible!important;overflow:visible!important}mjx-container[display=true]{display:block!important;text-align:center!important;margin:1em auto!important;max-width:100%!important}mjx-container[display=true] mjx-math{display:inline-block!important;text-align:left!important}mjx-container mjx-mtable{display:table!important;margin:0 auto!important;width:auto!important}mjx-container mjx-mtr{display:table-row!important;height:auto!important}mjx-container mjx-mtd{display:table-cell!important;padding:.3em .8em!important;vertical-align:middle!important}mjx-container[display=true] mjx-mtable mjx-mtr{display:table-row!important;white-space:nowrap!important}mjx-container[display=true] mjx-mtable{display:table!important;border-collapse:separate!important;border-spacing:0 .3em!important}mjx-container::-webkit-scrollbar{display:none!important}mjx-container{-ms-overflow-style:none!important;scrollbar-width:none!important}.dark{--primary:#ffffff;--secondary:#e5e5e5;--tertiary:#cccccc}body:not(.dark){--primary:#000000;--secondary:#333333;--tertiary:#666666}.dark a{color:#fff!important;text-decoration:underline}.dark a:hover{color:#e5e5e5!important}code,pre{font-family:var(--font-mono)!important;font-size:.9em}h1,h2,h3,h4,h5,h6{font-weight:700!important;color:var(--primary)!important}.post-meta{color:var(--secondary)!important}button,.button{font-family:var(--font-mono)!important;font-weight:500}.paper-card,.talk-card{background:var(--theme);border:1px solid var(--border);border-radius:8px;padding:24px;margin-bottom:24px;transition:all .2s ease;box-shadow:0 2px 4px rgba(0,0,0,5%)}.paper-card:hover,.talk-card:hover{border-color:var(--secondary);box-shadow:0 4px 8px rgba(0,0,0,.1);transform:translateY(-1px)}.dark .paper-card,.dark .talk-card{background:#1a1a1a;border-color:#333;box-shadow:0 2px 4px rgba(0,0,0,.2)}.dark .paper-card:hover,.dark .talk-card:hover{border-color:#555;box-shadow:0 4px 8px rgba(0,0,0,.3)}.paper-title{margin:0 0 12px!important;font-size:1.25em;line-height:1.3}.paper-title a{color:var(--primary)!important;text-decoration:none;border-bottom:2px solid transparent;transition:border-color .2s ease}.paper-title a:hover{border-bottom-color:var(--primary)}.paper-meta{margin-bottom:16px;font-size:.9em}.paper-authors{color:var(--secondary);margin-bottom:4px;font-weight:500}.paper-date{color:var(--tertiary);font-size:.85em}.paper-abstract{color:var(--primary);line-height:1.5}.paper-abstract p{margin:0}.talk-title{margin:0 0 12px!important;font-size:1.25em;line-height:1.3}.talk-collaborators{margin-bottom:16px;font-size:.9em;color:var(--secondary)}.talk-collaborators p{margin:0}.talk-details{display:flex;flex-direction:column;gap:8px}.talk-event,.talk-recording{font-size:.9em}.talk-recording a{color:var(--primary)!important;text-decoration:underline}.talk-recording a:hover{color:var(--secondary)!important}@media(max-width:768px){.paper-card,.talk-card{padding:16px;margin-bottom:16px}}.twitter-tweet{margin:24px auto!important;max-width:550px!important}.post-content blockquote.twitter-tweet,.post-content div:has(.twitter-tweet){display:flex;justify-content:center;margin:24px 0}.post-content .twitter-tweet iframe{margin:0 auto;display:block}.post-content figure{margin:24px 0;text-align:center}.post-content figure img{margin-bottom:12px;border-radius:6px;box-shadow:0 4px 8px rgba(0,0,0,.1);max-width:100%;height:auto}.dark .post-content figure img{box-shadow:0 4px 8px rgba(0,0,0,.3)}.post-content figcaption{font-family:var(--font-mono)!important;font-size:.85em;color:var(--secondary);font-style:italic;line-height:1.4;margin-top:8px;padding:0 16px}.post-content figcaption p{margin:0;text-align:center}@media(max-width:768px){.post-content figcaption{font-size:.8em;padding:0 8px}.post-content ol{padding-left:24px!important;margin-left:4px!important}.post-content .footnote-ref{margin-left:2px!important;margin-right:3px!important}.post-content{padding-left:20px!important;padding-right:20px!important}.main{padding-left:8px!important;padding-right:8px!important}}</style><meta property="og:url" content="http://localhost:1313/writing/intro-to-gpus/"><meta property="og:site_name" content="Shane Caldwell"><meta property="og:title" content="Intro to GPUs For the Research Oriented"><meta property="og:description" content="Getting comfortable with the hardware on a quest for more MFU."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="writing"><meta property="article:published_time" content="2026-01-06T00:00:00+00:00"><meta property="article:modified_time" content="2026-01-06T00:00:00+00:00"><meta property="article:tag" content="Llms"><meta property="article:tag" content="Systems"><meta property="og:image" content="http://localhost:1313/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/"><meta name=twitter:title content="Intro to GPUs For the Research Oriented"><meta name=twitter:description content="Getting comfortable with the hardware on a quest for more MFU."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Writing","item":"http://localhost:1313/writing/"},{"@type":"ListItem","position":2,"name":"Intro to GPUs For the Research Oriented","item":"http://localhost:1313/writing/intro-to-gpus/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Intro to GPUs For the Research Oriented","name":"Intro to GPUs For the Research Oriented","description":"Getting comfortable with the hardware on a quest for more MFU.","keywords":["llms","systems"],"articleBody":"Over the back half of last year, I’ve started touching on a lot of what I would call systems-y ML work. That is, a lot of the things I’ve wanted to do with LLMs have involved going below comfortable levels of abstraction into what the physical hardware is doing. In particular, this showed up when working on increasing Model FLOPs Utilization (MFU) for a training job I was working on. I pulled a few knobs available to me from torchland and got to see the number go up. Stuff like changing the batch size, the tensor quantization, pretokenizing my data, they were all available at the top level model code and I saw a significant increase in MFU.\nThe thing that took me the furthest though, by a wide margin, was swapping out my naive implementation for FlashAttention.\nhttps://x.com/shncldwll/status/1992312563749806512\nI’ve been finding myself bothered by not understanding the why of that. I know on some level that the implementation is more memory efficient, but I don’t have a good first principles understanding of why that’s true, nor could I write it myself. I had a similar feeling last year using Unsloth for the first time, which let me train a bigger model on longer contexts. There were systems tricks being done that I didn’t understand. This felt very limiting, since I was then subject to working on projects where my preferred software stack supported the techniques I was interested in using for research.\nI feel LLMs are getting good enough at writing systems code that soon that won’t be a real limitation and it will be relatively “cheap” to support a fork of training code that does what you want it to do. At the same time, it’s clear to me that if you don’t understand what you’re asking an LLM to do, it won’t be able to perform nearly as well as if you did understand the problem. In that sense, even autonomous coding agents feel like more augmentation than autonomy. If you can jump a metaphorical five feet in a particular area of programming problems, you can get the LLM to jump fifty feet. If you can only jump one foot, well…1\nSo, it behooves researchers and research engineers in particular to have a deeper understanding of systems ML programming, both for our own ability to engage with the literature and to be a better steward to any coding agents we’re working alongside. That’s easier said than done, since GPU programming is considered notoriously difficult, black-magick-y kind of work. I’ve spoken to plenty of big lab researchers who talk about legends of ML like they’re regular people who happen to be good at their jobs. They talk about GPU people quite a bit differently:\nthere was ~1 guy at openai responsible for inference CUDA kernels\nlet’s call him Bob, people would refer to his attention kernel as “the Bob kernel”\nit executed probably trillions of times a day on hundreds of thousands of GPUs\none singular guy https://t.co/hARlCebKux\n— Rohan Pandey (@khoomeik) September 16, 2025 this guy designs kernels with spreadsheets to hit shared memory banks evenly, open sourced assembly kernels that basically taught nvidia how to do gpu matmuls and register allocation properly, and authored most prod chatgpt kernels. genuinely incredible https://t.co/hoK8ureOAE\n— Clive Chan (@itsclivetime) September 17, 2025 But as Silvanus P. Thompson said,\nWhat one fool can do, another can.\nIn this post, I’ll provide a gentle introduction to learning about GPU kernels in the context of writing more efficient ML training jobs, and share a roadmap I’ve found useful for those looking to do the same.\nI find tackling similar concepts at different levels of abstractions really reinforces concepts, so we’ll be moving up and down the stack as required to make the points necessary. By the end, you should understand elements of the physical design of the GPU that informs how you would design an efficient kernel at whatever level of the software stack you choose to do that.\nWhy go GPU mode? Not all performance engineering is GPU-related. We’ve discussed in past posts, for example, optimizations aimed at overcoming network-bandwidth bottlenecks. In general, you should first strive to know what your bottleneck is before you try to find techniques to tackle it. Briefly, we can break down each bottleneck type in the Pentagram of Performance2 as follows:\nCompute: You’re bound by doing GEMM (General Matrix Multiply) operations. This one is hard to do anything about. You have a GPU that does a certain amount of teraflops, and you’re bound by the flops you’ve got to do for your operation. If you buy a more expensive GPU, you can get more TFLOPs.\nOverhead: The grab bag for stuff that isn’t the other stuff on this list. In particular, this might look like eager execution mode in PyTorch. Your GPU is waiting for python to dispatch the next CUDA kernel, and while it’s waiting on that it’s not doing any matrix multiplication.\nTo illustrate this, take a look at this old blog on CUDA Graphs which has the following illustration:\nJust gotta keep those things busy.\nIn eager PyTorch world, there’s no assumed knowledge of the graph and each kernel will launch separately. There’s a latency incurred, but more importantly you don’t start queuing the next kernel until you’ve got the result of the last one back. The GPU has to wait while the CPU prepares more work for it, and sits idle. If you’ve got the entire graph of operations, it takes a little bit of time to create that graph and dispatch it, but there’s very little latency between the kernels. That’s an example of reducing overhead.\nData-Loading Bandwidth: Say you have to load data through the cloud, or reading from disk takes a long time because you don’t have an SSD or something. Reducing this is an example of why long training jobs have usually tokenized the data before they read it in. If you’re streaming samples in (which you have to be) you don’t want to incur the additional cost of waiting for them to be properly processed before they can get on the GPU. You want this data in the GPU’s DRAM before it’s needed for a kernel.\nNetwork-Bandwidth: All distributed training is going to incur some sort of cost. Activations might need to move between nodes, or data parallel nodes may need to run an all-reduce. All of that time being taken up is time your GPUs can’t do matrix multiplications. DiLoCo, for example, reduces the frequency with which data parallel workers need to communicate. The Intellect-1 report also mentions int 8 quantization being performed on the DiLoCo psuedo-gradients in order to reduce the payload size. This is another example of tackling network-bandwidth bottlenecks, though an extreme one.\nMemory-Bandwidth: This is moving data from one place in memory to another. Writing from the host to the GPU? Memory bandwidth. DRAM on your GPU to SRAM? Memory bandwidth. This is the big one that justifies writing custom kernels, so it’s worth backing up to explain GPUs a bit so you can better spot these kind of problems.\nBriefly, you’ve probably heard of operator fusion or kernel fusion associated with memory bandwidth issues. One of the goals of this blog post is for you to understand that more intuitively.\nIntro to GPUs Most people I read about in the optimization space recommend writing kernels for ML in Triton. That said, I think for pedagogical reasons, you should start with CUDA.\nTriton introduces a lot of abstractions to make your life easier, but I think you appreciate why those abstractions are there and design your kernels differently if you understand them better. Let’s take a look at the “Hello World” of GPU programming, the vector addition.\nA kernel is just the function you write that is designed to be run on the GPU. They won’t return anything. Rather, they’re passed pointers to global memory and will mutate that. Literature will refer to “host” and “device” code. The host is the CPU (it’s the one dispatching work to the GPU!), and the device is the GPU (it does the work!).\n__global__ void vecAddKernel(float* A, float* B, float* C, int n){ int i = threadIdx.x + blockDim.x * blockIdx.x; //indexing if (i \u003c n){ C[i] = A[i] + B[i]; } } The actual calculation isn’t interesting, but there are two things that are.\nFirst, the calculation of the index. Why do we need one? This is because the execution of kernels is concurrent and parallel. One copy of this program is executing per-thread. We have no idea which thread is going to execute first, and multiple copies are almost certainly running on different pieces of hardware simultaneously. As the programmer, we want to make sure we’re taking care of each element, and the index is the way of finding what a given thread is “responsible” for.\nNext is the boundary condition. i \u003c n implies that we’re going to launch more threads than our arrays have elements, so we want to avoid reading from or writing to memory that would be out of bounds of that. To see why that is, let’s look at the “host” function that would launch that kernel.\nvoid vecAdd(float* A, float* B, float* C, int n){ float *A_d, *B_d, *C_d; int size = n * sizeof(float); cudaMalloc((void **) \u0026A_d, size); cudaMalloc((void **) \u0026B_d, size); cudaMalloc((void **) \u0026C_d, size); cudaMemcpy(A_d, A, size, cudaMemcpyHostToDevice); cudaMemcpy(B_d, B, size, cudaMemcpyHostToDevice); vecAddKernel\u003c\u003c\u003cceil(n, 256.0), 256\u003e\u003e\u003e(A_d, B_d, C_d, n); cudaMemcpy(C, C_d, size, cudaMemcpyDeviceToHost); cudaFree(A_d) cudaFree(B_d) cudaFree(C_d) } Note that we’re copying arrays from the host to the device. This is copied into the GPU’s DRAM, which is what shows up when you query nvidia-smi. You do this all the time in PyTorch, though less explicitly, when you call .to(device)\nThen, after the kernel is run, you need to access the result of your computation and pull it back to the host memory.\nThe interesting line is: vecAddKernel\u003c\u003c\u003e\u003e(A_d, B_d, C_d, n);, which looks like a templated function call. You can interpret that syntax like this:\nfoo_kernel\u003c\u003c\u003c(num blocks, threads per block)\u003e\u003e\u003e\nThe threads you’re running will be organized into blocks. All the blocks and all the threads running within them are known as the grid.\nThe idiom here ceil(n, 256.0) is a common one. The number of threads you need is determined by the size of your input data. For vecAddKernel we’re launching in sets of blocks of size 256. So say $N$ is 20,000. ceil is going to round up the result of 20,000 divided by 256 (which is 78.125) , and get 79. That gives us 20,224 threads to work on our input of size 20,000. That’s why we need the boundary condition.\nDimensions In the vecAddKernel, our grid and block size was one-dimensional. That is, we had 79 blocks and each of them had 256 threads and they were flat.\nSo when we calculated the index:\nint i = threadIdx.x + blockDim.x * blockIdx.x;\nthreadIdx.x: What thread am I in this block?\nblockIdx.x: What block am I running in?\nblockDim.x: How many threads are in each block?\nAs you might suspect based on the x property, you can actually lay out threads in two additional dimensions. The grid and the block can both be separately defined as dim3 objects.\nThis is an abstraction that’s most useful based on the kind of data you’re reading. If you were processing an image, for example, you might choose to organize the blocks as being on a 2D grid to more naturally line up the thread organization with the problem structure. For example, in a matrix-multiply. Assume we are multiplying $M$ by $N$. We could use two dimensional blocks to take care of it.\n__global__ void MatrixMulKernel(float *M, float* N, float* P, int Width){ int row = blockIdx.y * blockDim.y + threadIdx.y; int col = blockIdx.x * blockDim.x + threadIdx.x; } For the sake of understanding, assume we did:\ndim3 grid(2, 2); // 2x2 or four blocks in the grid dim3 block(2, 2); // 2x2 or four threads per block MatrixMulKernel\u003c\u003c\u003cgrid, block\u003e\u003e\u003e(M, N, P, W) Our grid would look like\nBut after we applied the indexing, we would get to our desired global value for the output.\nThat’s the basic model of CUDA programming. What’s missing from this is an understanding of the hardware that can guide you towards better design of kernels. The most fundamental physical-hardware understanding I can think of is compute intensity and the mismatch between memory-bandwidth and FLOPs.\nMemory Bandwidth In general, every kernel is going to involve:\nReading in one or more pieces of data.\nPerforming one or more operations on that data.\nWriting that data back.\nGPUs are incredibly specialized pieces of equipment that are really good at matrix multiplication, and their flops are well-advertised. Looking at the H100 spec sheet, we see:\nWe’ve discussed the sparsity feature in a previous post. We’re most frequently in training going to be looking at our Peak BF16 Tensor Core TFLOPs, which for this H100 SXM5 is 989.4 TFLOPs.\nThat’s a lot of FLOPs. In fact, that may be so many FLOPs that we actually struggle to use them all effectively. Because you can’t run floating point operations on data you don’t have yet.\nIncluded in the spec sheet of the H100 (though less advertised) is the 3.35 terabytes/second of global memory bandwidth. That is, not including latency, you can get 3.35 terabytes from DRAM a second. If we assume we’re using 16 bit values (two bytes per number), that means we can load in 1.1 trillion numbers in the same time that the GPU can perform 133.9 trillion floating point operations.\nThe implication is that we can compute 100x faster than we can actually get access to the data. And how many FLOPs are actually in common operations?\nConsider our previous addition example. We were reading two vectors from memory, and writing one value out. That’s $3N$ reads and writes for $N$ FLOPs where $N$ is the size of the input vectors. Even if doing the calculation wasn’t one hundred times faster, it’s still obvious that we’re spending most of our time on reading and writing data. This is a memory-bound operation.\nIdeally, since actually doing the FLOPs is so much faster and we want to maximize them, we would like the number of FLOPs for each memory access to be higher. It turns out that this measure is formalized by the compute intensity of a kernel.\nThe compute intensity is equal to the number of FLOPs over the number of bytes accessed from global memory. If the FLOPs dominate, the compute intensity is higher and it’s a “compute-bound” algorithm. If the memory-accesses dominate, meaning the performance of the algorithm is limited by the speed of data transfer, it’s memory-bound.\nSo based on our understanding of the peak bandwidth of global memory from the H100, and our FLOPs, have the idea that we need to do around $100N$ FLOPs before we start to do enough compute for it to be the limiting factor.\nLet’s see if we can reproduce the spec sheet numbers of the H100.\nConsider the following function:\ndef f(x): for _ in range(iters): x = x * 2 return x Applied to the following data:\ninputs = [torch.randn(2**24, device='cuda', dtype=torch.float16)] This function takes a tensor on the GPU, and doubles it for a certain number of iterations. To do the calculation, we’re basically doing a single FLOP per element in the tensor every iteration. This is a contrived function, but it allows us to hold the amount of reads fixed while increasing the FLOPs in a predictable way.\nIt’s important to note that Triton acting as the jit has to fuse the resulting computations - it’s not going to load x up for each iteration of the loop or anything like that.\nLet’s look at the results:\nFor the first few values of repeat, you can see we’re using next to none of our FLOPs, but nearly all of our bandwidth. Our bandwidth is entirely saturated reading and writing data to and from global memory and it’s constraining our runtime. In-particular, you can see that until we get to a value of around 32-64, our runtime is basically flat. Because the bandwidth is the limiting factor, there’s essentially no difference in doubling the vector 16 times or 32 times. The calculation is entirely memory bound at this level of computational intensity.\nNow remember, the way the kernel is written, increasing values of repeat is changing the numerator in our computational intensity, that is, the FLOPs, while keeping the read/writes of data the same.\nNotably after 64 we start to see our bandwidth utilization fall off a cliff. The compute is now doing enough work that there’s a lag before it gives the data back to the memory. We are no longer being held up by our bandwidth, but really just the speed with which we can do the calculations. We have increased that numerator such that we are in a compute bound regime.\nThis tradeoff from where you’re bound based on bandwidth vs being bound on the FLOPs you can do is basically a roofline model.\nNote: An eagle-eyed reader would notice this peak value of 33.5 TFLOP/s is a lot smaller than what’s advertised in the spec sheet for BF16. Best I can tell, PyTorch is upcasting the BF16 to FP32 for the actual computation of the doubling, which for multiply operations is about ~33.5 TF/s on the H100. Since this is just a toy example, I didn’t beat it up too much to find the culprit.\nOn a basic level, you should now have an intuition for two things:\nBandwidth from global memory is a lot slower than FLOPs.\nBecause of (1), all else equal, it is best if I can increase the computational intensity of my kernels while reducing the amount of reads and writes to global memory.\nNow we can talk about more of the physical design of the GPU.\nLatency Hiding So far we’ve discussed the bandwidth of the memory. That is, the capacity for data that can move through the pipes from global memory to the GPUs SMs.\nOne thing we haven’t touched on yet is latency. That is, how long it takes for any data to show up to the GPU once requested.\nOur naive view of the Cuda programming model should suggest that in the operation\n__global__ void vecAddKernel(float* A, float* B, float* C, int n){ int i = threadIdx.x + blockDim.x * blockIdx.x; //indexing if (i \u003c n){ C[i] = A[i] + B[i]; } } All of our threads in the different blocks in the grid that we launched to do the vector add should be working essentially in lockstep. They’ll hit A[i] + B[i] and then all need to make requests for that data basically simultaneously, and our GPU will do just about nothing as it waits for that to occur, and we eat the latency.\nIt turns out this isn’t the case, and we can understand why if we learn a little more about the hardware!\nWarps Warps are yet another collection of threads that occurs within the block level. Currently the collection is grouped in a way that is hardware-specific. So right now, a warp has 32 threads in it, but that’s not guaranteed to be the case.\nGPUs execute their work on streaming multiprocessors (SMs), but those processors are physical pieces of hardware and can only operate so many threads at a time. So an SM will get many warps assigned to it. Multiple thread blocks can have warps that share an SM, but we won’t break up warps from the same thread block onto different SMs.\nCUDA is said to be an SIMT architecture - single instruction, multiple thread. This is how that happens! At a given time, an SM is running the instructions against a warp of 32 threads.\nOne convenient thing about this setup is how it breaks our naive view where we were just waiting around for data to show up. Because we’re actually doing the work concurrently within an SM and in parallel across them, we have the ability to hide latency. Nvidia GPUs have the ability to, with zero-cost, switch which warp they’re executing. They’re able to do this because they have really large registers, allowing them to keep all the threads data available very local to the SM. So if they’re running a warp that issues a call to load data, no problem, the scheduler can load a new warp until it hits some kind of high latency instruction, and onto the next warp.\nTo get a sense of the numbers, H100s can have 2048 threads in flight per SM, meaning they can have 64 concurrent warps running on them, meaning it’s easy to hide latency by doing different work3.\nOne thing that might bother you about this is the way the vector addition kernel is written, it doesn’t seem like the latency hiding buys us a lot. Each warp is going to need to read its own data from global memory. The scheduler will recognize this as a high latency, operation, swap to a new warp, which will hit the exact same instructions for a different piece of memory in DRAM, and so on. So why bother?\nMemory Locality Many kernels are more complicated than simple element-wise operations. Many of them require sharing information between threads. The way this information is shared results in interacting with a hierarchy of memory, each being successively closer to the SM with a lower latency. This means that as you start to write more complicated kernels, the latency-hiding of the warps can actually be effectively made use of.\nUntil now, we have only considered DRAM, the “global” memory that all threads in a grid have access to. We’ve also determine reading from it is very slow.\nThere is also a much nearer memory within a block, called shared memory. All threads in a block have read-write access to this memory. Shared memory is on chip meaning it can be accessed very quickly. So if you’re working on an algorithm where your thread blocks can efficiently make use of their intermediate calculations, you can use shared memory to reduce the amount of strain you’re putting on global memory bandwidth.\nConsider the following example, a dot product from CUDA By Example:\n__global__ void dot(float* a, float* b, float* c) { __shared__ float cache[threadsPerBlock]; int tid = threadIdx.x + blockIdx.x * blockDim.x; int cacheIndex = threadIdx.x; float temp = 0; while (tid \u003c N){ temp += a[tid] * b[tid]; tid += blockDim.x * gridDim.x; } // set the cache values cache[cacheIndex] = temp; // synchronize threads in this block __syncthreads(); // for reductions, threadsPerBlock must be a power of 2 // because of the following code int i = blockDim.x/2; while (i != 0){ if (cacheIndex \u003c i) cache[cacheIndex] += cache[cacheIndex + i]; __syncthreads(); i /= 2; } if (cacheIndex == 0) c[blockIdx.x] = cache[0]; } Note: This algorithm will not finish the reduction of the dot product, leaving one element per block launched in C. The final reduction is then performed by a simple sum on the CPU.\nUnderstanding the specifics of the grid-stride loop is less important than seeing the use of the shared memory cache. Each thread is writing to its own element in cache (note that they only need to know their local index, since the shared memory is block specific). In order to perform the reduction, other threads will need that intermediate data, so it’s shared, and is ultimately read from multiple times. You want to limit the amount of times you reach for a specific element from global memory. If you can make it one time, you should, since the cost of going to global memory is so high.\nThe use of __syncthreads just ensures that all threads reach that line before continuing, so all writes are completed before a thread reads from the data again. This is helpful because only collections of warps run at the same time. So if you’ve got a thread block that’s greater than 32, you’ll need to make sure all warps have caught up to that position before continuing.\nThe other thing that’s worth noting is that this is the first algorithm we’ve observed where the block size choice is actually crucial to the correctness of the code.\n// for reductions, threadsPerBlock must be a power of 2 // because of the following code int i = blockDim.x/2; while (i != 0){ if (cacheIndex \u003c i) cache[cacheIndex] += cache[cacheIndex + i]; __syncthreads(); i /= 2; } If you use that indexing trick when the threadsPerBlock is not a power of 2, you’ll skip elements. The takeaway here is that the block and grid design choice as the programmer writing the kernel is not arbitrary. If I got into matrix multiplication tiling here the post would get too long, but it’s worth looking at if you want to better grasp the connection between the hardware understanding (reading from global memory is slow and painful) and how that leads to code design decisions (design blocks that can make use of shared memory to make as few of those reads and writes as possible).\nOnce you start getting into more complex kernels, which are the ones that matter, the lack of real C++ knowledge starts to hurt.\nWhy Triton? In Triton, instead of writing code from the perspective of a single thread, you’ve got a program working on an entire block of data. You can then write NumPy-style vectorized operations on it. The Triton compiler will figure out the thread-level implementation. I assume most ML practitioners reading this, like myself, have a greater familiarity with Python than a systems programming language like C++. As a reminder, though, learn enough CUDA to be dangerous to make sure you understand the basics of the hardware.\nThe reasons one might want to use it are similar to any level of abstraction you might introduce into a software stack: it’s easier to write, and it’s more portable. Hand-tuned CUDA is generally faster, but Triton is generally “fast enough”. If you need evidence that Triton is “enough” for most use cases, look no further than torch using it by default to create kernels with torch.compile.\ntorch.compile When you use torch.compile, that’s acting as the frontend for the compilation process. TorchDynamo is used to capture the computational graph that defines your function. Then it’s passed to TorchInductor, which is the backend compiler that generates your accelerator specific code. For most GPUs you might be using, like NVIDIA, AMD, and Intel, this will generate Triton code.\nWhen learning Triton, it’s often useful to see what the compiler will output. Consider the following example. We might naively expect these kernels to be compiled separately.\nimport torch import os # Enable Triton code generation output os.environ[\"TORCH_LOGS\"] = \"output_code\" os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = \"./my_kernels\" os.environ[\"TORCH_COMPILE_DEBUG\"] = \"1\" # Two separate operations (unfused) def unfused(x): x1 = x.cos() x2 = x1.cos() return x2 # Single chained operation (will fuse) def fused(x): return x.cos().cos() # Compile both unfused_compiled = torch.compile(unfused) fused_compiled = torch.compile(fused) # Trigger compilation x = torch.randn(1024, 1024, device=\"cuda\") _ = unfused_compiled(x) _ = fused_compiled(x) Both operations are effectively the same. In eager pytorch, the unfused one would have 2N reads and 2N writes along with the 2N FLOPs. The fused operation would have the same amount of FLOPs, but only N reads and N writes. So is this the sort of thing you break out a custom Triton kernel for?\n$TORCHINDUCTOR_CACHE_DIR will create a my_kernels directory. In a child directory nc you’ll see an actual .py file that contains the Triton kernel.\nimport triton import triton.language as tl from torch._inductor.runtime import triton_helpers, triton_heuristics from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties triton_helpers.set_driver_to_gpu() @triton_heuristics.pointwise( size_hints={'x': 1048576}, filename=__file__, triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=108, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]}, inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cos_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '8B5C5AF9338C1AAB256A9A579415704A502E73E986488BEC47BA9903BFF4F8C2', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}, min_elem_per_thread=0 ) @triton.jit def triton_poi_fused_cos_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr): xnumel = 1048576 xoffset = tl.program_id(0) * XBLOCK xindex = xoffset + tl.arange(0, XBLOCK)[:] xmask = tl.full([XBLOCK], True, tl.int1) x0 = xindex tmp0 = tl.load(in_ptr0 + (x0), None) tmp1 = tl_math.cos(tmp0) tmp2 = tl_math.cos(tmp1) tl.store(out_ptr0 + (x0), tmp2, None) Separately, turning on the TORCH_COMPILE_DEBUG gives you a bunch of interesting stuff, including Triton code before and after fusion. The code ends up being the same, but it’s interesting to look at nonetheless.\nIn this case, we can see our naive example of illustrating kernel fusion was educational but incorrect. The compiler is able to recognize both our fused and unfused example have the same computation graph, and fuse them into literally the same function, with one global read and one global write. The compiler is pretty smart, and will fuse ‘obvious’ things where it sees an advantage!\nSo if the compiler is smart enough to fuse some operations for better performance, why bother learning to write Triton by hand? What kind of thing is the compiler not smart enough to handle? And since Triton is a higher level language, what can you do from Python that the compiler can’t?\nOnline Softmax Let’s take a look at writing Softmax. You’ve probably called it from torch a hundred times.\n$$\\text{Softmax}(z_{i}) = \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)}$$Just looking at the mathematical description, it looks like we’re only going to need one piece of global information, which is the sum of the exponents. However, we’ll actually want the numerically stable version of softmax. If we take $e^{x_i}$ for large logits, we can easily end up with inf in the numerator or denominator, which ruins the calculation.\n$Softmax(x) = Softmax(x-c)$, meaning we can subtract an arbitrary constant from each $x_i$ without effecting the final calculation. If we choose $c = max(x)$, then the largest exponent will be $e^0$, which is 1. For the same reason, one term in the denominator will be 1, meaning the sum is at smallest one. The computations then work out cleanly.\nSo for this to be correct, we actually need two shared pieces of information. We need the max of $x$, and we need the summed exponent. Let’s look at the most naive Triton kernel we can think of for that. In this case, we’ll assume that the axis of parallelization is that each program handles one row of the input, assuming this is a standard case of softmax being run on logits.\n@triton.jit def softmax_kernel( input_ptr, output_ptr, n_cols, input_row_stride, output_row_stride, BLOCK_SIZE: tl.constexpr ): # Each program instance handles one row row_idx = tl.program_id(0) # Pointer to the start of this row row_input_ptr = input_ptr + row_idx * input_row_stride row_output_ptr = output_ptr + row_idx * output_row_stride # First pass: find max max_val = -float('inf') for block_start in tl.range(0, n_cols, BLOCK_SIZE): offsets = block_start + tl.arange(0, BLOCK_SIZE) mask = offsets \u003c n_cols x = tl.load(row_input_ptr + offsets, mask=mask, other=-float('inf')) block_max = tl.max(x, axis=0) max_val = tl.maximum(max_val, block_max) # Second pass: compute sum of exp(x - max) sum_exp = 0.0 for block_start in tl.range(0, n_cols, BLOCK_SIZE): offsets = block_start + tl.arange(0, BLOCK_SIZE) mask = offsets \u003c n_cols x = tl.load(row_input_ptr + offsets, mask=mask, other=-float('inf')) sum_exp += tl.sum(tl.exp(x - max_val), axis=0) # Third pass: write output for block_start in tl.range(0, n_cols, BLOCK_SIZE): offsets = block_start + tl.arange(0, BLOCK_SIZE) mask = offsets \u003c n_cols x = tl.load(row_input_ptr + offsets, mask=mask, other=-float('inf')) output = tl.exp(x - max_val) / sum_exp tl.store(row_output_ptr + offsets, output, mask=mask) The most obvious pain point here is that we’re calling tl.load three times. We’re reading in the data first to find the max of each row, second to collect the sum of exponents, and then third to compute the final value and write the output. Can we reduce the number of reads?\nIt turns out we can, by building up the max and the sum of the exponents at the same time. We can keep the running sum of exponents going, scaling the result based on our current understanding of the max value, like so.\nimport torch import triton import triton.language as tl @triton.jit def softmax_kernel( input_ptr, output_ptr, n_cols, input_row_stride, output_row_stride, BLOCK_SIZE: tl.constexpr ): # Each program instance handles one row row_idx = tl.program_id(0) # Pointer to the start of this row row_input_ptr = input_ptr + row_idx * input_row_stride row_output_ptr = output_ptr + row_idx * output_row_stride # First pass: find max AND sum (online algorithm) max_val = -float('inf') sum_exp = 0.0 for block_start in tl.range(0, n_cols, BLOCK_SIZE): offsets = block_start + tl.arange(0, BLOCK_SIZE) mask = offsets \u003c n_cols x = tl.load(row_input_ptr + offsets, mask=mask, other=-float('inf')) block_max = tl.max(x, axis=0) new_max = tl.maximum(max_val, block_max) # Rescale previous sum to new max, then add new terms sum_exp = sum_exp * tl.exp(max_val - new_max) + tl.sum(tl.exp(x - new_max), axis=0) max_val = new_max # Second pass: write output for block_start in tl.range(0, n_cols, BLOCK_SIZE): offsets = block_start + tl.arange(0, BLOCK_SIZE) mask = offsets \u003c n_cols x = tl.load(row_input_ptr + offsets, mask=mask, other=-float('inf')) output = tl.exp(x - max_val) / sum_exp tl.store(row_output_ptr + offsets, output, mask=mask) This is equivalent in terms of output, but lets us get rid of one of our passes. This is exactly the kind of multi-pass reduction the Triton compiler can’t handle for us. It requires us to see how we could get to the same result for less.\nWe’ll bake it off in a really basic way, profiling it on H100 for a fixed column-size that we can reasonably expect a single program to do while varying the amount of rows it has to be performed on.\nIt’s nice when intuition lines up with profiling.\nThe online version is in fact around 30% more efficient than it’s three-pass counterpart., despite writing this in high-level Python. I’m a little surprised to see it doing better than PyTorch for some sizes (about 20ms), particularly since I haven’t gone through the trouble of auto-tuning it. At any rate, we won’t go through the whole tuning process with this because it’s not a kernel we intend to place directly into a training and inference job. We’ll save that for something meatier like FlashAttention.\nWhat we wanted to show is that our knowledge of the hardware and a desire to reduce global memory reads from CUDA could result in faster implementations in Triton while writing code that is recognizably Python.\nA Roadmap This stuff is a lot easier to learn than it used to be. Part of that is just the sheer quantity of resources that didn’t exist a few years ago. Below I’ve listed the resources I’ve used to get this far.\nMaking Deep Learning Go Brrrr From First Principles: Generally explains the motivations of “why” in performance engineering, and lays out the basic concepts you should keep in mind when you’re learning. You should leave this fairly comfortable with the pentagram of compute, memory-bandwidth, overhead, data-loading bandwidth, and network bandwidth, and when each factors in. When looking at any optimization technique, a basic sanity check for whether you’re learning anything useful will be whether you can identify what kind of bottleneck you’re tackling.\nThe Ultra-Scale Playbook: Training LLMs on GPU Clusters: Mostly handles the distributed side of the house. There’s a lot here, including some notes on distributed profiling and bits on loading custom kernels. In general, this will get you comfortable with some of the important numbers around tracking training efficiency. If you want your learning to be in the context of training models then this is a reasonable second place to spend your time.\nProgramming Massively Parallel Processors: This is the textbook you’ll be told to read when you want to “learn CUDA”. It isn’t particularly focused on model training, but it’s good to understand the basics even if you ultimately plan on using Triton instead. My recommendation is to read the first five chapters and do the exercises. You may feel you’ve got the gist of the programming after chapter three. However, chapters four and five are more foundational. Chapter four teaches you the architecture of a GPU, and how blocks and warps work with the SMs of the GPU. Chapter five is all about memory bandwidth. You will find it extremely difficult to reason about the efficiency of your kernels without this knowledge. Having bounced off the book after chapter three several times, I found chapters four and five were sort of a skeleton key to a lot of stuff I was reading online.\nsrush/GPU-Puzzles is a nice series of GPU puzzles that can be run with numba, which maps Python to CUDA kernels. Many of the exercises are quite easy and will only take you a few minutes, though the harder ones can take a few hours. In general, solving the exercises to be “correct” and then working through what you need to do to get to the optimal amount of global reads/writes will teach you a lot of the mental model of optimization. If you’ve read chapter five of PMPP, your motivation for doing so will be more clear.\nTriton Tutorials the official docs here are strong and go in order. You’ll learn how to simulate kernel performance without access to a GPU, and how to profile your performance when you’ve got it. I recommend going through each of them, getting the code to run, and then trying to implement it yourself. Try different things and see where they help or hurt performance, taking particular note if where you’re reading and writing to memory and how. You can take these in order - once you get to fused attention, well, you probably know as much as me!\nGPU Mode has close to 100 lectures covering various topics and a great Discord. If you’re interested in performance, there’s a community for you here.\nLeetGPU is like Leetcode but for GPU problems. It has CUDA, Triton, PyTorch, CuTeDSL, Mojo, and recent Jax support. Personally I used it to get exposed to a bunch of different Triton patterns and idioms4\nPersonally, I don’t believe researchers/research engineers can afford to ignore how their hardware works anymore. With LLMs and the amount of communities and projects all based around high performance training and inference, I don’t think there’s ever been a better time to get started with this kind of work. Good luck!\nThis based on my experience of getting LLMs to write frontend code, where I have tremendously little to offer them besides how I would “like” it to look and feel, rather than specific engineering approaches I prefer for a principled reason. It certainly gets the job done, but i can tell I’m just scratching the surface because I don’t know how to ask for what I want. ↩︎\nT-shirts to be released at a later date. ↩︎\nThis doesn’t really matter for the latency conversation, but the SXM5 GPUs have 132 SMs enabled. So ~270k threads going at the same time. ↩︎\nI also hope to bring more users to the platform. I’m currently ranked 30th in Triton and I should be lower. ↩︎\n","wordCount":"6553","inLanguage":"en","image":"http://localhost:1313/","datePublished":"2026-01-06T00:00:00Z","dateModified":"2026-01-06T00:00:00Z","author":{"@type":"Person","name":"Shane Caldwell"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/writing/intro-to-gpus/"},"publisher":{"@type":"Organization","name":"Shane Caldwell","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Shane Caldwell (Alt + H)">Shane Caldwell</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Main><span>Main</span></a></li><li><a href=http://localhost:1313/papers/ title=Papers><span>Papers</span></a></li><li><a href=http://localhost:1313/talks/ title=Talks><span>Talks</span></a></li><li><a href=http://localhost:1313/writing/ title=Writing><span>Writing</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Intro to GPUs For the Research Oriented</h1><div class=post-description>Getting comfortable with the hardware on a quest for more MFU.</div><div class=post-meta><span title='2026-01-06 00:00:00 +0000 UTC'>January 6, 2026</span>&nbsp;·&nbsp;31 min&nbsp;·&nbsp;6553 words&nbsp;·&nbsp;Shane Caldwell</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#dimensions>Dimensions</a></li><li><a href=#memory-bandwidth>Memory Bandwidth</a></li><li><a href=#latency-hiding>Latency Hiding</a><ul><li><a href=#warps>Warps</a></li></ul></li><li><a href=#memory-locality>Memory Locality</a></li><li><a href=#why-triton>Why Triton?</a></li><li><a href=#torchcompile><code>torch.compile</code></a></li><li><a href=#online-softmax>Online Softmax</a></li><li><a href=#a-roadmap>A Roadmap</a></li></ul></nav></div></details></div><div class=post-content><p>Over the back half of last year, I&rsquo;ve started touching on a lot of what I would call <em>systems-y</em> ML work. That is, a lot of the things I&rsquo;ve wanted to do with LLMs have involved going below comfortable levels of abstraction into what the physical hardware is doing. In particular, this showed up when working on increasing Model FLOPs Utilization (MFU) for a <a href=https://x.com/shncldwll/status/1992312563749806512>training job I was working on</a>. I pulled a few knobs available to me from torchland and got to see the number go up. Stuff like changing the batch size, the tensor quantization, pretokenizing my data, they were all available at the top level model code and I saw a significant increase in MFU.</p><p>The thing that took me the furthest though, by a wide margin, was swapping out my naive implementation for FlashAttention.</p><p><a href=https://x.com/shncldwll/status/1992312563749806512>https://x.com/shncldwll/status/1992312563749806512</a></p><p>I&rsquo;ve been finding myself bothered by not understanding the <em>why</em> of that. I know on some level that the implementation is more memory efficient, but I don&rsquo;t have a good first principles understanding of why that&rsquo;s true, nor could I write it myself. I had a similar feeling last year using Unsloth for the first time, which let me train a bigger model on longer contexts. There were systems tricks being done that I didn&rsquo;t understand. This felt very limiting, since I was then subject to working on projects where my preferred software stack supported the techniques I was interested in using for research.</p><p>I feel LLMs are getting good enough at writing systems code that soon that won&rsquo;t be a real limitation and it will be relatively &ldquo;cheap&rdquo; to support a fork of training code that does what you want it to do. At the same time, it&rsquo;s clear to me that if you don&rsquo;t understand what you&rsquo;re asking an LLM to do, it won&rsquo;t be able to perform nearly as well as if you <em>did</em> understand the problem. In that sense, even autonomous coding agents feel like more augmentation than autonomy. If you can jump a metaphorical five feet in a particular area of programming problems, you can get the LLM to jump fifty feet. If you can only jump one foot, well&mldr;<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p>So, it behooves researchers and research engineers in particular to have a deeper understanding of systems ML programming, both for our own ability to engage with the literature and to be a better steward to any coding agents we&rsquo;re working alongside. That&rsquo;s easier said than done, since GPU programming is considered notoriously difficult, black-magick-y kind of work. I&rsquo;ve spoken to plenty of big lab researchers who talk about legends of ML like they&rsquo;re regular people who happen to be good at their jobs. They talk about GPU people quite a bit differently:</p><blockquote class=twitter-tweet><p lang=en dir=ltr>there was ~1 guy at openai responsible for inference CUDA kernels<br><br>let’s call him Bob, people would refer to his attention kernel as “the Bob kernel”<br><br>it executed probably trillions of times a day on hundreds of thousands of GPUs<br><br>one singular guy <a href=https://t.co/hARlCebKux>https://t.co/hARlCebKux</a></p>&mdash; Rohan Pandey (@khoomeik) <a href="https://twitter.com/khoomeik/status/1968027724913709071?ref_src=twsrc%5Etfw">September 16, 2025</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><blockquote class=twitter-tweet><p lang=en dir=ltr>this guy designs kernels with spreadsheets to hit shared memory banks evenly, open sourced assembly kernels that basically taught nvidia how to do gpu matmuls and register allocation properly, and authored most prod chatgpt kernels. genuinely incredible <a href=https://t.co/hoK8ureOAE>https://t.co/hoK8ureOAE</a></p>&mdash; Clive Chan (@itsclivetime) <a href="https://twitter.com/itsclivetime/status/1968140448062746651?ref_src=twsrc%5Etfw">September 17, 2025</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><p>But as Silvanus P. Thompson said,</p><blockquote><p>What one fool can do, another can.</p></blockquote><p>In this post, I&rsquo;ll provide a gentle introduction to learning about GPU kernels in the context of writing more efficient ML training jobs, and share a roadmap I&rsquo;ve found useful for those looking to do the same.</p><p>I find tackling similar concepts at different levels of abstractions really reinforces concepts, so we&rsquo;ll be moving up and down the stack as required to make the points necessary. By the end, you should understand elements of the physical design of the GPU that informs how you would design an efficient kernel at whatever level of the software stack you choose to do that.</p><h1 id=why-go-gpu-mode>Why go GPU mode?<a hidden class=anchor aria-hidden=true href=#why-go-gpu-mode>#</a></h1><p>Not all performance engineering is GPU-related. We&rsquo;ve discussed in past posts, for example, optimizations aimed at overcoming <a href=https://hackbot.dad/writing/data-parallelism-for-the-poor/>network-bandwidth bottlenecks</a>. In general, you should first strive to know what your bottleneck is before you try to find techniques to tackle it. Briefly, we can break down each bottleneck type in the Pentagram of Performance<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> as follows:</p><p><strong>Compute</strong>: You&rsquo;re bound by doing GEMM (<strong>Ge</strong>neral <strong>M</strong>atrix <strong>M</strong>ultiply) operations. This one is hard to do anything about. You have a GPU that does a certain amount of teraflops, and you&rsquo;re bound by the flops you&rsquo;ve got to do for your operation. If you buy a more expensive GPU, you can get more TFLOPs.</p><p><strong>Overhead</strong>: The grab bag for stuff that isn&rsquo;t the other stuff on this list. In particular, this might look like eager execution mode in PyTorch. Your GPU is waiting for python to dispatch the next CUDA kernel, and while it&rsquo;s waiting on that it&rsquo;s not doing any matrix multiplication.</p><p>To illustrate this, take a look at this <a href=https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/>old blog on CUDA Graphs</a> which has the following illustration:</p><figure><img loading=lazy src=cuda_graphs.png alt="Just gotta keep those things busy."><figcaption><p>Just gotta keep those things busy.</p></figcaption></figure><p>In eager PyTorch world, there&rsquo;s no assumed knowledge of the graph and each kernel will launch separately. There&rsquo;s a latency incurred, but more importantly you don&rsquo;t start queuing the next kernel until you&rsquo;ve got the result of the last one back. The GPU has to wait while the CPU prepares more work for it, and sits idle. If you&rsquo;ve got the entire graph of operations, it takes a little bit of time to create that graph and dispatch it, but there&rsquo;s very little latency between the kernels. That&rsquo;s an example of reducing overhead.</p><p><strong>Data-Loading Bandwidth</strong>: Say you have to load data through the cloud, or reading from disk takes a long time because you don&rsquo;t have an SSD or something. Reducing this is an example of why long training jobs have usually tokenized the data before they read it in. If you&rsquo;re streaming samples in (which you have to be) you don&rsquo;t want to incur the additional cost of waiting for them to be properly processed before they can get on the GPU. You want this data in the GPU&rsquo;s DRAM before it&rsquo;s needed for a kernel.</p><p><strong>Network-Bandwidth</strong>: All distributed training is going to incur some sort of cost. Activations might need to move between nodes, or data parallel nodes may need to run an all-reduce. All of that time being taken up is time your GPUs can&rsquo;t do matrix multiplications. DiLoCo, for example, reduces the frequency with which data parallel workers need to communicate. The <a href=https://www.primeintellect.ai/blog/intellect-1>Intellect-1</a> report also mentions int 8 quantization being performed on the DiLoCo psuedo-gradients in order to reduce the payload size. This is another example of tackling network-bandwidth bottlenecks, though an extreme one.</p><p><strong>Memory-Bandwidth</strong>: This is moving data from one place in memory to another. Writing from the host to the GPU? Memory bandwidth. DRAM on your GPU to SRAM? Memory bandwidth. This is the big one that justifies writing custom kernels, so it&rsquo;s worth backing up to explain GPUs a bit so you can better spot these kind of problems.</p><p>Briefly, you&rsquo;ve probably heard of <strong>operator fusion</strong> or <strong>kernel fusion</strong> associated with memory bandwidth issues. One of the goals of this blog post is for you to understand that more intuitively.</p><h1 id=intro-to-gpus>Intro to GPUs<a hidden class=anchor aria-hidden=true href=#intro-to-gpus>#</a></h1><p>Most people I read about in the optimization space recommend writing kernels for ML in Triton. That said, I think for pedagogical reasons, you should start with CUDA.</p><p>Triton introduces a lot of abstractions to make your life easier, but I think you appreciate why those abstractions are there and design your kernels differently if you understand them better. Let&rsquo;s take a look at the &ldquo;Hello World&rdquo; of GPU programming, the vector addition.</p><p>A <strong>kernel</strong> is just the function you write that is designed to be run on the GPU. They won&rsquo;t return anything. Rather, they&rsquo;re passed pointers to global memory and will mutate that. Literature will refer to &ldquo;host&rdquo; and &ldquo;device&rdquo; code. The <strong>host</strong> is the CPU (it&rsquo;s the one dispatching work to the GPU!), and the <strong>device</strong> is the GPU (it does the work!).</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>__global__</span>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=nf>vecAddKernel</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>A</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>B</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>C</span><span class=p>,</span> <span class=kt>int</span> <span class=n>n</span><span class=p>){</span>
</span></span><span class=line><span class=cl>	<span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span> <span class=c1>//indexing
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=k>if</span> <span class=p>(</span><span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>){</span>
</span></span><span class=line><span class=cl>		<span class=n>C</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=n>B</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>The actual calculation isn&rsquo;t interesting, but there are two things that are.</p><p>First, the calculation of the index. Why do we need one? This is because the execution of kernels is concurrent and parallel. One copy of this program is executing <em>per-thread</em>. We have no idea which thread is going to execute first, and multiple copies are almost certainly running on different pieces of hardware simultaneously. As the programmer, we want to make sure we&rsquo;re taking care of each element, and the index is the way of finding what a given thread is &ldquo;responsible&rdquo; for.</p><p>Next is the boundary condition. <code>i &lt; n</code> implies that we&rsquo;re going to launch more threads than our arrays have elements, so we want to avoid reading from or writing to memory that would be out of bounds of that. To see why that is, let&rsquo;s look at the &ldquo;host&rdquo; function that would launch that kernel.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=kt>void</span> <span class=nf>vecAdd</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>A</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>B</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>C</span><span class=p>,</span> <span class=kt>int</span> <span class=n>n</span><span class=p>){</span>
</span></span><span class=line><span class=cl>	<span class=kt>float</span> <span class=o>*</span><span class=n>A_d</span><span class=p>,</span> <span class=o>*</span><span class=n>B_d</span><span class=p>,</span> <span class=o>*</span><span class=n>C_d</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=kt>int</span> <span class=n>size</span> <span class=o>=</span> <span class=n>n</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>cudaMalloc</span><span class=p>((</span><span class=kt>void</span> <span class=o>**</span><span class=p>)</span> <span class=o>&amp;</span><span class=n>A_d</span><span class=p>,</span> <span class=n>size</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=n>cudaMalloc</span><span class=p>((</span><span class=kt>void</span> <span class=o>**</span><span class=p>)</span> <span class=o>&amp;</span><span class=n>B_d</span><span class=p>,</span> <span class=n>size</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=n>cudaMalloc</span><span class=p>((</span><span class=kt>void</span> <span class=o>**</span><span class=p>)</span> <span class=o>&amp;</span><span class=n>C_d</span><span class=p>,</span> <span class=n>size</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>A_d</span><span class=p>,</span> <span class=n>A</span><span class=p>,</span> <span class=n>size</span><span class=p>,</span> <span class=n>cudaMemcpyHostToDevice</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>B_d</span><span class=p>,</span> <span class=n>B</span><span class=p>,</span> <span class=n>size</span><span class=p>,</span> <span class=n>cudaMemcpyHostToDevice</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>vecAddKernel</span><span class=o>&lt;&lt;&lt;</span><span class=n>ceil</span><span class=p>(</span><span class=n>n</span><span class=p>,</span> <span class=mf>256.0</span><span class=p>),</span> <span class=mi>256</span><span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>A_d</span><span class=p>,</span> <span class=n>B_d</span><span class=p>,</span> <span class=n>C_d</span><span class=p>,</span> <span class=n>n</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>C</span><span class=p>,</span> <span class=n>C_d</span><span class=p>,</span> <span class=n>size</span><span class=p>,</span> <span class=n>cudaMemcpyDeviceToHost</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>cudaFree</span><span class=p>(</span><span class=n>A_d</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>cudaFree</span><span class=p>(</span><span class=n>B_d</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>cudaFree</span><span class=p>(</span><span class=n>C_d</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Note that we&rsquo;re copying arrays from the host to the device. This is copied into the GPU&rsquo;s DRAM, which is what shows up when you query <code>nvidia-smi</code>. You do this all the time in PyTorch, though less explicitly, when you call <code>.to(device)</code></p><p>Then, after the kernel is run, you need to access the result of your computation and pull it back to the host memory.</p><p>The interesting line is: <code>vecAddKernel&lt;&lt;&lt;ceil(n, 256.0), 256>>>(A_d, B_d, C_d, n);</code>, which looks like a templated function call. You can interpret that syntax like this:</p><p><code>foo_kernel&lt;&lt;&lt;(num blocks, threads per block)>>></code></p><p>The threads you&rsquo;re running will be organized into <strong>blocks</strong>. All the blocks and all the threads running within them are known as the <strong>grid</strong>.</p><p>The idiom here <code>ceil(n, 256.0)</code> is a common one. The number of threads you need is determined by the size of your input data. For <code>vecAddKernel</code> we&rsquo;re launching in sets of blocks of size 256. So say $N$ is 20,000. <code>ceil</code> is going to round up the result of 20,000 divided by 256 (which is 78.125) , and get 79. That gives us 20,224 threads to work on our input of size 20,000. That&rsquo;s why we need the boundary condition.</p><h2 id=dimensions>Dimensions<a hidden class=anchor aria-hidden=true href=#dimensions>#</a></h2><p>In the <code>vecAddKernel</code>, our grid and block size was one-dimensional. That is, we had 79 blocks and each of them had 256 threads and they were flat.</p><p>So when we calculated the index:</p><p><code>int i = threadIdx.x + blockDim.x * blockIdx.x;</code></p><p><code>threadIdx.x</code>: What thread am I in this block?</p><p><code>blockIdx.x</code>: What block am I running in?</p><p><code>blockDim.x</code>: How many threads are in each block?</p><p>As you might suspect based on the <code>x</code> property, you can actually lay out threads in two additional dimensions. The grid and the block can both be separately defined as <code>dim3</code> objects.</p><p>This is an abstraction that&rsquo;s most useful based on the kind of data you&rsquo;re reading. If you were processing an image, for example, you might choose to organize the blocks as being on a 2D grid to more naturally line up the thread organization with the problem structure. For example, in a matrix-multiply. Assume we are multiplying $M$ by $N$. We could use two dimensional blocks to take care of it.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>MatrixMulKernel</span><span class=p>(</span><span class=kt>float</span> <span class=o>*</span><span class=n>M</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>N</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>P</span><span class=p>,</span> <span class=kt>int</span> <span class=n>Width</span><span class=p>){</span>
</span></span><span class=line><span class=cl>	<span class=kt>int</span> <span class=n>row</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>y</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>y</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=kt>int</span> <span class=n>col</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span></code></pre></div><p>For the sake of understanding, assume we did:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>dim3</span> <span class=nf>grid</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>);</span> <span class=c1>// 2x2 or four blocks in the grid
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>dim3</span> <span class=nf>block</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>);</span> <span class=c1>// 2x2 or four threads per block
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>MatrixMulKernel</span><span class=o>&lt;&lt;&lt;</span><span class=n>grid</span><span class=p>,</span> <span class=n>block</span><span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>M</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=n>P</span><span class=p>,</span> <span class=n>W</span><span class=p>)</span>
</span></span></code></pre></div><p>Our grid would look like</p><figure><img loading=lazy src=thread_block_index.png></figure><p>But after we applied the indexing, we would get to our desired global value for the output.</p><figure><img loading=lazy src=global_thread_math.png></figure><p>That&rsquo;s the basic model of CUDA programming. What&rsquo;s missing from this is an understanding of the hardware that can guide you towards better design of kernels. The most fundamental physical-hardware understanding I can think of is <em>compute intensity</em> and the mismatch between memory-bandwidth and FLOPs.</p><h2 id=memory-bandwidth>Memory Bandwidth<a hidden class=anchor aria-hidden=true href=#memory-bandwidth>#</a></h2><p>In general, every kernel is going to involve:</p><ol><li><p>Reading in one or more pieces of data.</p></li><li><p>Performing one or more operations on that data.</p></li><li><p>Writing that data back.</p></li></ol><p>GPUs are incredibly specialized pieces of equipment that are really good at matrix multiplication, and their flops are well-advertised. Looking at the H100 spec sheet, we see:</p><figure><img loading=lazy src=h100_spec_sheet.png></figure><p>We&rsquo;ve discussed the sparsity feature in <a href=https://hackbot.dad/writing/pretraining-at-home/>a previous post</a>. We&rsquo;re most frequently in training going to be looking at our Peak BF16 Tensor Core TFLOPs, which for this H100 SXM5 is 989.4 TFLOPs.</p><p>That&rsquo;s a lot of FLOPs. In fact, that may be so many FLOPs that we actually struggle to use them all effectively. Because you can&rsquo;t run floating point operations on data you don&rsquo;t <em>have yet</em>.</p><p>Included in the spec sheet of the H100 (though less advertised) is the 3.35 terabytes/second of global memory bandwidth. That is, not including latency, you can get 3.35 terabytes from DRAM a second. If we assume we&rsquo;re using 16 bit values (two bytes per number), that means we can load in 1.1 trillion numbers in the same time that the GPU can perform 133.9 trillion floating point operations.</p><p>The implication is that we can compute 100x faster than we can actually get access to the data. And how many FLOPs are actually in common operations?</p><p>Consider our previous addition example. We were reading two vectors from memory, and writing one value out. That&rsquo;s $3N$ reads and writes for $N$ FLOPs where $N$ is the size of the input vectors. Even if doing the calculation wasn&rsquo;t one hundred times faster, it&rsquo;s still obvious that we&rsquo;re spending most of our time on reading and writing data. This is a memory-bound operation.</p><p>Ideally, since actually doing the FLOPs is so much faster and we want to maximize them, we would like the number of FLOPs for each memory access to be higher. It turns out that this measure is formalized by the <strong>compute intensity</strong> of a kernel.</p><p>The compute intensity is equal to the number of FLOPs over the number of bytes accessed from global memory. If the FLOPs dominate, the compute intensity is higher and it&rsquo;s a &ldquo;compute-bound&rdquo; algorithm. If the memory-accesses dominate, meaning the performance of the algorithm is limited by the speed of data transfer, it&rsquo;s memory-bound.</p><p>So based on our understanding of the peak bandwidth of global memory from the H100, and our FLOPs, have the idea that we need to do around $100N$ FLOPs before we start to do enough compute for it to be the limiting factor.</p><p>Let&rsquo;s see if we can reproduce the spec sheet numbers of the H100.</p><p>Consider the following function:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>f</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>iters</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>*</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><p>Applied to the following data:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=o>**</span><span class=mi>24</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cuda&#39;</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float16</span><span class=p>)]</span>
</span></span></code></pre></div><p>This function takes a tensor on the GPU, and doubles it for a certain number of iterations. To do the calculation, we&rsquo;re basically doing a single FLOP per element in the tensor every iteration. This is a contrived function, but it allows us to hold the amount of reads fixed while increasing the FLOPs in a predictable way.</p><p>It&rsquo;s important to note that Triton acting as the <code>jit</code> has to fuse the resulting computations - it&rsquo;s not going to load x up for each iteration of the loop or anything like that.</p><p>Let&rsquo;s look at the results:</p><figure><img loading=lazy src=roofline.png></figure><p>For the first few values of <code>repeat</code>, you can see we&rsquo;re using next to none of our FLOPs, but nearly all of our bandwidth. Our bandwidth is entirely saturated reading and writing data to and from global memory and it&rsquo;s constraining our runtime. In-particular, you can see that until we get to a value of around 32-64, our runtime is basically flat. Because the bandwidth is the limiting factor, there&rsquo;s essentially no difference in doubling the vector 16 times or 32 times. The calculation is entirely <em>memory bound</em> at this level of computational intensity.</p><p>Now remember, the way the kernel is written, increasing values of repeat is changing the numerator in our computational intensity, that is, the FLOPs, while keeping the read/writes of data the same.</p><p>Notably after <code>64</code> we start to see our bandwidth utilization fall off a cliff. The compute is now doing enough work that there&rsquo;s a lag before it gives the data back to the memory. We are no longer being held up by our bandwidth, but really just the speed with which we can do the calculations. We have increased that numerator such that we are in a <strong>compute bound</strong> regime.</p><p>This tradeoff from where you&rsquo;re bound based on bandwidth vs being bound on the FLOPs you can do is basically a roofline model.</p><p><strong>Note</strong>: An eagle-eyed reader would notice this <strong>peak</strong> value of 33.5 TFLOP/s is a lot smaller than what&rsquo;s advertised in the spec sheet for BF16. Best I can tell, PyTorch is upcasting the BF16 to FP32 for the actual computation of the doubling, which for multiply operations is about ~33.5 TF/s on the H100. Since this is just a toy example, I didn&rsquo;t beat it up too much to find the culprit.</p><p>On a basic level, you should now have an intuition for two things:</p><ol><li><p>Bandwidth from global memory is a lot slower than FLOPs.</p></li><li><p>Because of (1), all else equal, it is best if I can <strong>increase</strong> the computational intensity of my kernels while <strong>reducing</strong> the amount of reads and writes to global memory.</p></li></ol><p>Now we can talk about more of the physical design of the GPU.</p><h2 id=latency-hiding>Latency Hiding<a hidden class=anchor aria-hidden=true href=#latency-hiding>#</a></h2><p>So far we&rsquo;ve discussed the <em>bandwidth</em> of the memory. That is, the capacity for data that can move through the pipes from global memory to the GPUs SMs.</p><p>One thing we haven&rsquo;t touched on yet is <em>latency</em>. That is, how long it takes for any data to show up to the GPU once requested.</p><p>Our naive view of the Cuda programming model should suggest that in the operation</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__global__</span>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=nf>vecAddKernel</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>A</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>B</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>C</span><span class=p>,</span> <span class=kt>int</span> <span class=n>n</span><span class=p>){</span>
</span></span><span class=line><span class=cl>	<span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span> <span class=c1>//indexing
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=k>if</span> <span class=p>(</span><span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>){</span>
</span></span><span class=line><span class=cl>		<span class=n>C</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=n>B</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>All of our threads in the different blocks in the grid that we launched to do the vector add should be working essentially in lockstep. They&rsquo;ll hit <code>A[i] + B[i]</code> and then all need to make requests for that data basically simultaneously, and our GPU will do just about nothing as it waits for that to occur, and we eat the latency.</p><p>It turns out this isn&rsquo;t the case, and we can understand why if we learn a little more about the hardware!</p><h3 id=warps>Warps<a hidden class=anchor aria-hidden=true href=#warps>#</a></h3><p><strong>Warps</strong> are yet another collection of threads that occurs within the block level. Currently the collection is grouped in a way that is <strong>hardware-specific</strong>. So right now, a warp has 32 threads in it, but that&rsquo;s not guaranteed to be the case.</p><p>GPUs execute their work on <strong>streaming multiprocessors</strong> (SMs), but those processors are physical pieces of hardware and can only operate so many threads at a time. So an SM will get many warps assigned to it. Multiple thread blocks can have warps that share an SM, but we won&rsquo;t break up warps from the same thread block onto different SMs.</p><p>CUDA is said to be an SIMT architecture - single instruction, multiple thread. This is how that happens! At a given time, an SM is running the instructions against a warp of 32 threads.</p><p>One convenient thing about this setup is how it breaks our naive view where we were just waiting around for data to show up. Because we&rsquo;re actually doing the work concurrently within an SM and in parallel across them, we have the ability to hide latency. Nvidia GPUs have the ability to, with zero-cost, switch which warp they&rsquo;re executing. They&rsquo;re able to do this because they have really large registers, allowing them to keep all the threads data available very local to the SM. So if they&rsquo;re running a warp that issues a call to load data, no problem, the scheduler can load a new warp until it hits some kind of high latency instruction, and onto the next warp.</p><p>To get a sense of the numbers, H100s can have 2048 threads in flight per SM, meaning they can have <strong>64 concurrent warps</strong> running on them, meaning it&rsquo;s easy to hide latency by doing different work<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p><p>One thing that might bother you about this is the way the vector addition kernel is written, it doesn&rsquo;t seem like the latency hiding buys us a lot. Each warp is going to need to read its own data from global memory. The scheduler will recognize this as a high latency, operation, swap to a new warp, which will hit the exact same instructions for a different piece of memory in DRAM, and so on. So why bother?</p><h2 id=memory-locality>Memory Locality<a hidden class=anchor aria-hidden=true href=#memory-locality>#</a></h2><p>Many kernels are more complicated than simple element-wise operations. Many of them require sharing information between threads. The way this information is shared results in interacting with a hierarchy of memory, each being successively closer to the SM with a lower latency. This means that as you start to write more complicated kernels, the latency-hiding of the warps can actually be effectively made use of.</p><p>Until now, we have only considered DRAM, the &ldquo;global&rdquo; memory that all threads in a grid have access to. We&rsquo;ve also determine reading from it is very slow.</p><p>There is also a much nearer memory within a block, called <em>shared memory</em>. All threads in a block have read-write access to this memory. Shared memory is <em>on chip</em> meaning it can be accessed very quickly. So if you&rsquo;re working on an algorithm where your thread blocks can efficiently make use of their intermediate calculations, you can use shared memory to reduce the amount of strain you&rsquo;re putting on global memory bandwidth.</p><p>Consider the following example, a dot product from CUDA By Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>dot</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>a</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>b</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>c</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=n>__shared__</span> <span class=kt>float</span> <span class=n>cache</span><span class=p>[</span><span class=n>threadsPerBlock</span><span class=p>];</span>
</span></span><span class=line><span class=cl>	<span class=kt>int</span> <span class=n>tid</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=kt>int</span> <span class=n>cacheIndex</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=kt>float</span> <span class=n>temp</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=k>while</span> <span class=p>(</span><span class=n>tid</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>){</span>
</span></span><span class=line><span class=cl>		<span class=n>temp</span> <span class=o>+=</span> <span class=n>a</span><span class=p>[</span><span class=n>tid</span><span class=p>]</span> <span class=o>*</span> <span class=n>b</span><span class=p>[</span><span class=n>tid</span><span class=p>];</span>
</span></span><span class=line><span class=cl>		<span class=n>tid</span> <span class=o>+=</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>gridDim</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1>// set the cache values
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=n>cache</span><span class=p>[</span><span class=n>cacheIndex</span><span class=p>]</span> <span class=o>=</span> <span class=n>temp</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1>// synchronize threads in this block
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1>// for reductions, threadsPerBlock must be a power of 2
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=c1>// because of the following code
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span><span class=o>/</span><span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=k>while</span> <span class=p>(</span><span class=n>i</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>){</span>
</span></span><span class=line><span class=cl>		<span class=k>if</span> <span class=p>(</span><span class=n>cacheIndex</span> <span class=o>&lt;</span> <span class=n>i</span><span class=p>)</span>
</span></span><span class=line><span class=cl>			<span class=n>cache</span><span class=p>[</span><span class=n>cacheIndex</span><span class=p>]</span> <span class=o>+=</span> <span class=n>cache</span><span class=p>[</span><span class=n>cacheIndex</span> <span class=o>+</span> <span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>		<span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>		<span class=n>i</span> <span class=o>/=</span> <span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=k>if</span> <span class=p>(</span><span class=n>cacheIndex</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>c</span><span class=p>[</span><span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span><span class=p>]</span> <span class=o>=</span> <span class=n>cache</span><span class=p>[</span><span class=mi>0</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p><strong>Note: This algorithm will not finish the reduction of the dot product, leaving one element per block launched in C. The final reduction is then performed by a simple sum on the CPU.</strong></p><p>Understanding the specifics of the grid-stride loop is less important than seeing the use of the shared memory <code>cache</code>. Each thread is writing to its own element in <code>cache</code> (note that they only need to know their local index, since the shared memory is block specific). In order to perform the reduction, other threads will need that intermediate data, so it&rsquo;s shared, and is ultimately read from multiple times. You want to limit the amount of times you reach for a specific element from global memory. If you can make it one time, you should, since the cost of going to global memory is so high.</p><p>The use of <code>__syncthreads</code> just ensures that all threads reach that line before continuing, so all writes are completed before a thread reads from the data again. This is helpful because only collections of <em>warps</em> run at the same time. So if you&rsquo;ve got a thread block that&rsquo;s greater than 32, you&rsquo;ll need to make sure all warps have caught up to that position before continuing.</p><p>The other thing that&rsquo;s worth noting is that this is the first algorithm we&rsquo;ve observed where the block size choice is actually crucial to the correctness of the code.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>	<span class=c1>// for reductions, threadsPerBlock must be a power of 2
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=c1>// because of the following code
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span><span class=o>/</span><span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=k>while</span> <span class=p>(</span><span class=n>i</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>){</span>
</span></span><span class=line><span class=cl>		<span class=k>if</span> <span class=p>(</span><span class=n>cacheIndex</span> <span class=o>&lt;</span> <span class=n>i</span><span class=p>)</span>
</span></span><span class=line><span class=cl>			<span class=n>cache</span><span class=p>[</span><span class=n>cacheIndex</span><span class=p>]</span> <span class=o>+=</span> <span class=n>cache</span><span class=p>[</span><span class=n>cacheIndex</span> <span class=o>+</span> <span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>		<span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>		<span class=n>i</span> <span class=o>/=</span> <span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span></code></pre></div><p>If you use that indexing trick when the <code>threadsPerBlock</code> is not a power of 2, you&rsquo;ll skip elements. The takeaway here is that the block and grid design choice as the programmer writing the kernel is not arbitrary. If I got into matrix multiplication tiling here the post would get too long, but it&rsquo;s worth looking at if you want to better grasp the connection between the hardware understanding (reading from global memory is slow and painful) and how that leads to code design decisions (design blocks that can make use of shared memory to make as few of those reads and writes as possible).</p><p>Once you start getting into more complex kernels, which are the ones that matter, the lack of real C++ knowledge starts to hurt.</p><h2 id=why-triton>Why Triton?<a hidden class=anchor aria-hidden=true href=#why-triton>#</a></h2><p>In <a href=https://triton-lang.org/main/getting-started/tutorials/>Triton</a>, instead of writing code from the perspective of a single thread, you&rsquo;ve got a program working on an entire block of data. You can then write NumPy-style vectorized operations on it. The Triton compiler will figure out the thread-level implementation. I assume most ML practitioners reading this, like myself, have a greater familiarity with Python than a systems programming language like C++. As a reminder, though, learn enough CUDA to be dangerous to make sure you understand the basics of the hardware.</p><p>The reasons one might want to use it are similar to any level of abstraction you might introduce into a software stack: it&rsquo;s easier to write, and it&rsquo;s more portable. Hand-tuned CUDA is generally faster, but Triton is generally &ldquo;fast enough&rdquo;. If you need evidence that Triton is &ldquo;enough&rdquo; for most use cases, look no further than torch using it by default to create kernels with <code>torch.compile</code>.</p><h2 id=torchcompile><code>torch.compile</code><a hidden class=anchor aria-hidden=true href=#torchcompile>#</a></h2><p>When you use <code>torch.compile</code>, that&rsquo;s acting as the frontend for the compilation process. TorchDynamo is used to capture the computational graph that defines your function. Then it&rsquo;s passed to TorchInductor, which is the backend compiler that generates your accelerator specific code. For most GPUs you might be using, like NVIDIA, AMD, and Intel, this will generate Triton code.</p><p>When learning Triton, it&rsquo;s often useful to see what the compiler will output. Consider the following example. We might naively expect these kernels to be compiled separately.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Enable Triton code generation output</span>
</span></span><span class=line><span class=cl><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;TORCH_LOGS&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=s2>&#34;output_code&#34;</span>
</span></span><span class=line><span class=cl><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;TORCHINDUCTOR_CACHE_DIR&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=s2>&#34;./my_kernels&#34;</span>
</span></span><span class=line><span class=cl><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;TORCH_COMPILE_DEBUG&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=s2>&#34;1&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Two separate operations (unfused)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>unfused</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x1</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>cos</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>x2</span> <span class=o>=</span> <span class=n>x1</span><span class=o>.</span><span class=n>cos</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>x2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Single chained operation (will fuse)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>fused</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>x</span><span class=o>.</span><span class=n>cos</span><span class=p>()</span><span class=o>.</span><span class=n>cos</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Compile both</span>
</span></span><span class=line><span class=cl><span class=n>unfused_compiled</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>unfused</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>fused_compiled</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>fused</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Trigger compilation</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1024</span><span class=p>,</span> <span class=mi>1024</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s2>&#34;cuda&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>_</span> <span class=o>=</span> <span class=n>unfused_compiled</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>_</span> <span class=o>=</span> <span class=n>fused_compiled</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></div><p>Both operations are effectively the same. In eager pytorch, the <code>unfused</code> one would have 2N reads and 2N writes along with the 2N FLOPs. The <code>fused</code> operation would have the same amount of FLOPs, but only N reads and N writes. So is this the sort of thing you break out a custom Triton kernel for?</p><p><code>$TORCHINDUCTOR_CACHE_DIR</code> will create a <code>my_kernels</code> directory. In a child directory <code>nc</code> you&rsquo;ll see an actual <code>.py</code> file that contains the Triton kernel.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>triton</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>triton.language</span> <span class=k>as</span> <span class=nn>tl</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch._inductor.runtime</span> <span class=kn>import</span> <span class=n>triton_helpers</span><span class=p>,</span> <span class=n>triton_heuristics</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch._inductor.runtime.triton_helpers</span> <span class=kn>import</span> <span class=n>libdevice</span><span class=p>,</span> <span class=n>math</span> <span class=k>as</span> <span class=n>tl_math</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch._inductor.runtime.hints</span> <span class=kn>import</span> <span class=n>AutotuneHint</span><span class=p>,</span> <span class=n>ReductionHint</span><span class=p>,</span> <span class=n>TileHint</span><span class=p>,</span> <span class=n>DeviceProperties</span>
</span></span><span class=line><span class=cl><span class=n>triton_helpers</span><span class=o>.</span><span class=n>set_driver_to_gpu</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@triton_heuristics.pointwise</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>size_hints</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;x&#39;</span><span class=p>:</span> <span class=mi>1048576</span><span class=p>},</span> 
</span></span><span class=line><span class=cl>    <span class=n>filename</span><span class=o>=</span><span class=vm>__file__</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>triton_meta</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;signature&#39;</span><span class=p>:</span> <span class=p>{</span><span class=s1>&#39;in_ptr0&#39;</span><span class=p>:</span> <span class=s1>&#39;*fp32&#39;</span><span class=p>,</span> <span class=s1>&#39;out_ptr0&#39;</span><span class=p>:</span> <span class=s1>&#39;*fp32&#39;</span><span class=p>,</span> <span class=s1>&#39;xnumel&#39;</span><span class=p>:</span> <span class=s1>&#39;i32&#39;</span><span class=p>,</span> <span class=s1>&#39;XBLOCK&#39;</span><span class=p>:</span> <span class=s1>&#39;constexpr&#39;</span><span class=p>},</span> <span class=s1>&#39;device&#39;</span><span class=p>:</span> <span class=n>DeviceProperties</span><span class=p>(</span><span class=nb>type</span><span class=o>=</span><span class=s1>&#39;cuda&#39;</span><span class=p>,</span> <span class=n>index</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>multi_processor_count</span><span class=o>=</span><span class=mi>108</span><span class=p>,</span> <span class=n>cc</span><span class=o>=</span><span class=mi>80</span><span class=p>,</span> <span class=n>major</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>regs_per_multiprocessor</span><span class=o>=</span><span class=mi>65536</span><span class=p>,</span> <span class=n>max_threads_per_multi_processor</span><span class=o>=</span><span class=mi>2048</span><span class=p>,</span> <span class=n>warp_size</span><span class=o>=</span><span class=mi>32</span><span class=p>),</span> <span class=s1>&#39;constants&#39;</span><span class=p>:</span> <span class=p>{},</span> <span class=s1>&#39;configs&#39;</span><span class=p>:</span> <span class=p>[{(</span><span class=mi>0</span><span class=p>,):</span> <span class=p>[[</span><span class=s1>&#39;tt.divisibility&#39;</span><span class=p>,</span> <span class=mi>16</span><span class=p>]],</span> <span class=p>(</span><span class=mi>1</span><span class=p>,):</span> <span class=p>[[</span><span class=s1>&#39;tt.divisibility&#39;</span><span class=p>,</span> <span class=mi>16</span><span class=p>]],</span> <span class=p>(</span><span class=mi>2</span><span class=p>,):</span> <span class=p>[[</span><span class=s1>&#39;tt.divisibility&#39;</span><span class=p>,</span> <span class=mi>16</span><span class=p>]]}]},</span>
</span></span><span class=line><span class=cl>    <span class=n>inductor_meta</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;grid_type&#39;</span><span class=p>:</span> <span class=s1>&#39;Grid1D&#39;</span><span class=p>,</span> <span class=s1>&#39;autotune_hints&#39;</span><span class=p>:</span> <span class=nb>set</span><span class=p>(),</span> <span class=s1>&#39;kernel_name&#39;</span><span class=p>:</span> <span class=s1>&#39;triton_poi_fused_cos_0&#39;</span><span class=p>,</span> <span class=s1>&#39;mutated_arg_names&#39;</span><span class=p>:</span> <span class=p>[],</span> <span class=s1>&#39;optimize_mem&#39;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span> <span class=s1>&#39;no_x_dim&#39;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span> <span class=s1>&#39;num_load&#39;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s1>&#39;num_reduction&#39;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span> <span class=s1>&#39;backend_hash&#39;</span><span class=p>:</span> <span class=s1>&#39;8B5C5AF9338C1AAB256A9A579415704A502E73E986488BEC47BA9903BFF4F8C2&#39;</span><span class=p>,</span> <span class=s1>&#39;are_deterministic_algorithms_enabled&#39;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span> <span class=s1>&#39;assert_indirect_indexing&#39;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span> <span class=s1>&#39;autotune_local_cache&#39;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span> <span class=s1>&#39;autotune_pointwise&#39;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span> <span class=s1>&#39;autotune_remote_cache&#39;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span> <span class=s1>&#39;force_disable_caches&#39;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span> <span class=s1>&#39;dynamic_scale_rblock&#39;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span> <span class=s1>&#39;max_autotune&#39;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span> <span class=s1>&#39;max_autotune_pointwise&#39;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span> <span class=s1>&#39;min_split_scan_rblock&#39;</span><span class=p>:</span> <span class=mi>256</span><span class=p>,</span> <span class=s1>&#39;spill_threshold&#39;</span><span class=p>:</span> <span class=mi>16</span><span class=p>,</span> <span class=s1>&#39;store_cubin&#39;</span><span class=p>:</span> <span class=kc>False</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=n>min_elem_per_thread</span><span class=o>=</span><span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nd>@triton.jit</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>triton_poi_fused_cos_0</span><span class=p>(</span><span class=n>in_ptr0</span><span class=p>,</span> <span class=n>out_ptr0</span><span class=p>,</span> <span class=n>xnumel</span><span class=p>,</span> <span class=n>XBLOCK</span> <span class=p>:</span> <span class=n>tl</span><span class=o>.</span><span class=n>constexpr</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>xnumel</span> <span class=o>=</span> <span class=mi>1048576</span>
</span></span><span class=line><span class=cl>    <span class=n>xoffset</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>program_id</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=o>*</span> <span class=n>XBLOCK</span>
</span></span><span class=line><span class=cl>    <span class=n>xindex</span> <span class=o>=</span> <span class=n>xoffset</span> <span class=o>+</span> <span class=n>tl</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>XBLOCK</span><span class=p>)[:]</span>
</span></span><span class=line><span class=cl>    <span class=n>xmask</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>full</span><span class=p>([</span><span class=n>XBLOCK</span><span class=p>],</span> <span class=kc>True</span><span class=p>,</span> <span class=n>tl</span><span class=o>.</span><span class=n>int1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x0</span> <span class=o>=</span> <span class=n>xindex</span>
</span></span><span class=line><span class=cl>    <span class=n>tmp0</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>in_ptr0</span> <span class=o>+</span> <span class=p>(</span><span class=n>x0</span><span class=p>),</span> <span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>tmp1</span> <span class=o>=</span> <span class=n>tl_math</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>tmp0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>tmp2</span> <span class=o>=</span> <span class=n>tl_math</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>tmp1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>tl</span><span class=o>.</span><span class=n>store</span><span class=p>(</span><span class=n>out_ptr0</span> <span class=o>+</span> <span class=p>(</span><span class=n>x0</span><span class=p>),</span> <span class=n>tmp2</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span></span></code></pre></div><p>Separately, turning on the <code>TORCH_COMPILE_DEBUG</code> gives you a bunch of interesting stuff, including Triton code before and after fusion. The code ends up being the same, but it&rsquo;s interesting to look at nonetheless.</p><p>In this case, we can see our naive example of illustrating kernel fusion was educational but incorrect. The compiler is able to recognize both our <code>fused</code> and <code>unfused</code> example have the same computation graph, and fuse them into <em>literally</em> the same function, with one global read and one global write. The compiler is pretty smart, and will fuse &lsquo;obvious&rsquo; things where it sees an advantage!</p><p>So if the compiler is smart enough to fuse some operations for better performance, why bother learning to write Triton by hand? What kind of thing is the compiler not smart enough to handle? And since Triton is a higher level language, what can you do from Python that the compiler can&rsquo;t?</p><h2 id=online-softmax>Online Softmax<a hidden class=anchor aria-hidden=true href=#online-softmax>#</a></h2><p>Let&rsquo;s take a look at writing Softmax. You&rsquo;ve probably called it from torch a hundred times.</p>$$\text{Softmax}(z_{i}) = \frac{\exp(z_i)}{\sum_j \exp(z_j)}$$<p>Just looking at the mathematical description, it looks like we&rsquo;re only going to need <em>one</em> piece of global information, which is the sum of the exponents. However, we&rsquo;ll actually want the numerically stable version of softmax. If we take $e^{x_i}$ for large logits, we can easily end up with <code>inf</code> in the numerator or denominator, which ruins the calculation.</p><p>$Softmax(x) = Softmax(x-c)$, meaning we can subtract an arbitrary constant from each $x_i$ without effecting the final calculation. If we choose $c = max(x)$, then the largest exponent will be $e^0$, which is 1. For the same reason, one term in the denominator will be 1, meaning the sum is at smallest one. The computations then work out cleanly.</p><p>So for this to be correct, we actually need <em>two</em> shared pieces of information. We need the max of $x$, and we need the summed exponent. Let&rsquo;s look at the most naive Triton kernel we can think of for that. In this case, we&rsquo;ll assume that the axis of parallelization is that each <em>program</em> handles one row of the input, assuming this is a standard case of softmax being run on logits.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@triton.jit</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>softmax_kernel</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>input_ptr</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>output_ptr</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>n_cols</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>input_row_stride</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>output_row_stride</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>BLOCK_SIZE</span><span class=p>:</span> <span class=n>tl</span><span class=o>.</span><span class=n>constexpr</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Each program instance handles one row</span>
</span></span><span class=line><span class=cl>    <span class=n>row_idx</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>program_id</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># Pointer to the start of this row</span>
</span></span><span class=line><span class=cl>    <span class=n>row_input_ptr</span> <span class=o>=</span> <span class=n>input_ptr</span> <span class=o>+</span> <span class=n>row_idx</span> <span class=o>*</span> <span class=n>input_row_stride</span>
</span></span><span class=line><span class=cl>    <span class=n>row_output_ptr</span> <span class=o>=</span> <span class=n>output_ptr</span> <span class=o>+</span> <span class=n>row_idx</span> <span class=o>*</span> <span class=n>output_row_stride</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># First pass: find max</span>
</span></span><span class=line><span class=cl>    <span class=n>max_val</span> <span class=o>=</span> <span class=o>-</span><span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>block_start</span> <span class=ow>in</span> <span class=n>tl</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>n_cols</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>offsets</span> <span class=o>=</span> <span class=n>block_start</span> <span class=o>+</span> <span class=n>tl</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>mask</span> <span class=o>=</span> <span class=n>offsets</span> <span class=o>&lt;</span> <span class=n>n_cols</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>row_input_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>,</span> <span class=n>other</span><span class=o>=-</span><span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>block_max</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>max_val</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>maximum</span><span class=p>(</span><span class=n>max_val</span><span class=p>,</span> <span class=n>block_max</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># Second pass: compute sum of exp(x - max)</span>
</span></span><span class=line><span class=cl>    <span class=n>sum_exp</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>block_start</span> <span class=ow>in</span> <span class=n>tl</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>n_cols</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>offsets</span> <span class=o>=</span> <span class=n>block_start</span> <span class=o>+</span> <span class=n>tl</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>mask</span> <span class=o>=</span> <span class=n>offsets</span> <span class=o>&lt;</span> <span class=n>n_cols</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>row_input_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>,</span> <span class=n>other</span><span class=o>=-</span><span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>sum_exp</span> <span class=o>+=</span> <span class=n>tl</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>tl</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>max_val</span><span class=p>),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># Third pass: write output</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>block_start</span> <span class=ow>in</span> <span class=n>tl</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>n_cols</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>offsets</span> <span class=o>=</span> <span class=n>block_start</span> <span class=o>+</span> <span class=n>tl</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>mask</span> <span class=o>=</span> <span class=n>offsets</span> <span class=o>&lt;</span> <span class=n>n_cols</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>row_input_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>,</span> <span class=n>other</span><span class=o>=-</span><span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>max_val</span><span class=p>)</span> <span class=o>/</span> <span class=n>sum_exp</span>
</span></span><span class=line><span class=cl>        <span class=n>tl</span><span class=o>.</span><span class=n>store</span><span class=p>(</span><span class=n>row_output_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>output</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span></code></pre></div><p>The most obvious pain point here is that we&rsquo;re calling <code>tl.load</code> three times. We&rsquo;re reading in the data first to find the max of each row, second to collect the sum of exponents, and then third to compute the final value and write the output. Can we reduce the number of reads?</p><p>It turns out we can, by building up the max and the sum of the exponents at the same time. We can keep the running sum of exponents going, scaling the result based on our current understanding of the max value, like so.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>triton</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>triton.language</span> <span class=k>as</span> <span class=nn>tl</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@triton.jit</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>softmax_kernel</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>input_ptr</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>output_ptr</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>n_cols</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>input_row_stride</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>output_row_stride</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>BLOCK_SIZE</span><span class=p>:</span> <span class=n>tl</span><span class=o>.</span><span class=n>constexpr</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Each program instance handles one row</span>
</span></span><span class=line><span class=cl>    <span class=n>row_idx</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>program_id</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># Pointer to the start of this row</span>
</span></span><span class=line><span class=cl>    <span class=n>row_input_ptr</span> <span class=o>=</span> <span class=n>input_ptr</span> <span class=o>+</span> <span class=n>row_idx</span> <span class=o>*</span> <span class=n>input_row_stride</span>
</span></span><span class=line><span class=cl>    <span class=n>row_output_ptr</span> <span class=o>=</span> <span class=n>output_ptr</span> <span class=o>+</span> <span class=n>row_idx</span> <span class=o>*</span> <span class=n>output_row_stride</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># First pass: find max AND sum (online algorithm)</span>
</span></span><span class=line><span class=cl>    <span class=n>max_val</span> <span class=o>=</span> <span class=o>-</span><span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>sum_exp</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>block_start</span> <span class=ow>in</span> <span class=n>tl</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>n_cols</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>offsets</span> <span class=o>=</span> <span class=n>block_start</span> <span class=o>+</span> <span class=n>tl</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>mask</span> <span class=o>=</span> <span class=n>offsets</span> <span class=o>&lt;</span> <span class=n>n_cols</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>row_input_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>,</span> <span class=n>other</span><span class=o>=-</span><span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>block_max</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>new_max</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>maximum</span><span class=p>(</span><span class=n>max_val</span><span class=p>,</span> <span class=n>block_max</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Rescale previous sum to new max, then add new terms</span>
</span></span><span class=line><span class=cl>        <span class=n>sum_exp</span> <span class=o>=</span> <span class=n>sum_exp</span> <span class=o>*</span> <span class=n>tl</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>max_val</span> <span class=o>-</span> <span class=n>new_max</span><span class=p>)</span> <span class=o>+</span> <span class=n>tl</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>tl</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>new_max</span><span class=p>),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>max_val</span> <span class=o>=</span> <span class=n>new_max</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># Second pass: write output</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>block_start</span> <span class=ow>in</span> <span class=n>tl</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>n_cols</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>offsets</span> <span class=o>=</span> <span class=n>block_start</span> <span class=o>+</span> <span class=n>tl</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>mask</span> <span class=o>=</span> <span class=n>offsets</span> <span class=o>&lt;</span> <span class=n>n_cols</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>row_input_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>,</span> <span class=n>other</span><span class=o>=-</span><span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>max_val</span><span class=p>)</span> <span class=o>/</span> <span class=n>sum_exp</span>
</span></span><span class=line><span class=cl>        <span class=n>tl</span><span class=o>.</span><span class=n>store</span><span class=p>(</span><span class=n>row_output_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>output</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span></code></pre></div><p>This is equivalent in terms of output, but lets us get rid of one of our passes. This is <em>exactly</em> the kind of multi-pass reduction the Triton compiler can&rsquo;t handle for us. It requires us to see how we could get to the same result for less.</p><p>We&rsquo;ll bake it off in a really basic way, profiling it on H100 for a fixed column-size that we can reasonably expect a single program to do while varying the amount of rows it has to be performed on.</p><figure><img loading=lazy src=online_softmax_benchmark.png alt="It&rsquo;s nice when intuition lines up with profiling."><figcaption><p>It&rsquo;s nice when intuition lines up with profiling.</p></figcaption></figure><p>The online version is in fact around <strong>30%</strong> more efficient than it&rsquo;s three-pass counterpart., despite writing this in high-level Python. I&rsquo;m a little surprised to see it doing better than PyTorch for some sizes (about 20ms), particularly since I haven&rsquo;t gone through the trouble of auto-tuning it. At any rate, we won&rsquo;t go through the whole tuning process with this because it&rsquo;s not a kernel we intend to place directly into a training and inference job. We&rsquo;ll save that for something meatier like FlashAttention.</p><p>What we wanted to show is that our knowledge of the hardware and a desire to reduce global memory reads from CUDA could result in faster implementations in Triton while writing code that is recognizably Python.</p><h2 id=a-roadmap>A Roadmap<a hidden class=anchor aria-hidden=true href=#a-roadmap>#</a></h2><p>This stuff is a lot easier to learn than it used to be. Part of that is just the sheer quantity of resources that didn&rsquo;t exist a few years ago. Below I&rsquo;ve listed the resources I&rsquo;ve used to get this far.</p><ul><li><p><a href=https://horace.io/brrr_intro.html>Making Deep Learning Go Brrrr From First Principles</a>: Generally explains the motivations of &ldquo;why&rdquo; in performance engineering, and lays out the basic concepts you should keep in mind when you&rsquo;re learning. You should leave this fairly comfortable with the pentagram of compute, memory-bandwidth, overhead, data-loading bandwidth, and network bandwidth, and when each factors in. When looking at any optimization technique, a basic sanity check for whether you&rsquo;re learning anything useful will be whether you can identify what kind of bottleneck you&rsquo;re tackling.</p></li><li><p><a href=https://huggingface.co/spaces/nanotron/ultrascale-playbook>The Ultra-Scale Playbook: Training LLMs on GPU Clusters</a>: Mostly handles the distributed side of the house. There&rsquo;s a lot here, including some notes on distributed profiling and bits on loading custom kernels. In general, this will get you comfortable with some of the important numbers around tracking training efficiency. If you want your learning to be in the context of <em>training models</em> then this is a reasonable second place to spend your time.</p></li><li><p><a href=https://www.amazon.com/Programming-Massively-Parallel-Processors-Hands/dp/0323912311>Programming Massively Parallel Processors</a>: This is the textbook you&rsquo;ll be told to read when you want to &ldquo;learn CUDA&rdquo;. It isn&rsquo;t particularly focused on model training, but it&rsquo;s good to understand the basics even if you ultimately plan on using Triton instead. My recommendation is to read the first five chapters and do the exercises. You may feel you&rsquo;ve got the gist of the programming after chapter three. However, chapters four and five are more foundational. Chapter four teaches you the architecture of a GPU, and how blocks and warps work with the SMs of the GPU. Chapter five is all about memory bandwidth. You will find it extremely difficult to reason about the efficiency of your kernels without this knowledge. Having bounced off the book after chapter three several times, I found chapters four and five were sort of a skeleton key to a lot of stuff I was reading online.</p></li><li><p><a href=https://github.com/srush/GPU-Puzzles><code>srush/GPU-Puzzles</code></a> is a nice series of GPU puzzles that can be run with numba, which maps Python to CUDA kernels. Many of the exercises are quite easy and will only take you a few minutes, though the harder ones can take a few hours. In general, solving the exercises to be &ldquo;correct&rdquo; and then working through what you need to do to get to the optimal amount of global reads/writes will teach you a lot of the mental model of optimization. If you&rsquo;ve read chapter five of PMPP, your motivation for doing so will be more clear.</p></li><li><p><a href=https://triton-lang.org/main/getting-started/tutorials/>Triton Tutorials</a> the official docs here are strong and go in order. You&rsquo;ll learn how to simulate kernel performance without access to a GPU, and how to profile your performance when you&rsquo;ve got it. I recommend going through each of them, getting the code to run, and then trying to implement it yourself. Try different things and see where they help or hurt performance, taking particular note if where you&rsquo;re reading and writing to memory and how. You can take these in order - once you get to fused attention, well, you probably know as much as me!</p></li><li><p><a href=https://www.youtube.com/channel/UCJgIbYl6C5no72a0NUAPcTA>GPU Mode</a> has close to 100 lectures covering various topics and a great Discord. If you&rsquo;re interested in performance, there&rsquo;s a community for you here.</p></li><li><p><a href=https://leetgpu.com/>LeetGPU</a> is like Leetcode but for GPU problems. It has CUDA, Triton, PyTorch, CuTeDSL, Mojo, and recent Jax support. Personally I used it to get exposed to a bunch of different Triton patterns and idioms<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></p></li></ul><p>Personally, I don&rsquo;t believe researchers/research engineers can afford to ignore how their hardware works anymore. With LLMs and the amount of communities and projects all based around high performance training and inference, I don&rsquo;t think there&rsquo;s ever been a better time to get started with this kind of work. Good luck!</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>This based on my experience of getting LLMs to write frontend code, where I have tremendously little to offer them besides how I would &ldquo;like&rdquo; it to look and feel, rather than specific engineering approaches I prefer for a principled reason. It certainly gets the job done, but i can tell I&rsquo;m just scratching the surface because I don&rsquo;t know how to ask for what I want.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>T-shirts to be released at a later date.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>This doesn&rsquo;t really matter for the latency conversation, but the SXM5 GPUs have 132 SMs enabled. So ~270k threads going at the same time.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>I also hope to bring more users to the platform. I&rsquo;m currently ranked 30th in Triton and I should be lower.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/llms/>Llms</a></li><li><a href=http://localhost:1313/tags/systems/>Systems</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/writing/all-reduce-across-atlantic/><span class=title>Next »</span><br><span>All Reduce Across the Atlantic: Bandwidth in Decentralized Training</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=http://localhost:1313/>Shane Caldwell</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><style>.copy-code{display:inline-flex;align-items:center;justify-content:center;width:32px;height:32px;background:var(--tertiary);border:1px solid var(--border);border-radius:6px;color:var(--secondary);cursor:pointer;transition:all .2s ease;position:absolute;top:8px;right:8px;z-index:10}.copy-code:hover{background:var(--secondary);color:var(--theme)}.copy-code svg{width:16px;height:16px}.copy,.highlight .copy{display:none!important}pre{position:relative}</style><script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll('.copy, [class*="copy"]').forEach(e=>{e.classList.contains("copy-code")||e.remove()});const e=document.querySelectorAll("pre");e.forEach(e=>{const t=e.cloneNode(!0);e.parentNode.replaceChild(t,e)}),document.querySelectorAll("pre code").forEach(e=>{const n=e.parentElement;if(n.querySelector(".copy-code"))return;const t=document.createElement("button");t.classList.add("copy-code"),t.setAttribute("aria-label","Copy code"),t.setAttribute("type","button");const s=`<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"/><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"/></svg>`,a=`<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6L9 17l-5-5"/></svg>`;t.innerHTML=s;function o(){t.innerHTML=a,t.style.color="#10b981",setTimeout(()=>{t.innerHTML=s,t.style.color=""},2e3)}t.addEventListener("click",function(t){t.preventDefault(),t.stopPropagation();const n=e.textContent||e.innerText;navigator.clipboard?navigator.clipboard.writeText(n).then(()=>{o()}).catch(()=>{i(n)}):i(n)});function i(e){const t=document.createElement("textarea");t.value=e,t.style.position="fixed",t.style.opacity="0",document.body.appendChild(t),t.select();try{document.execCommand("copy"),o()}catch(e){console.error("Copy failed:",e)}document.body.removeChild(t)}n.appendChild(t)})})</script><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},chtml:{scale:1,mtextInheritFont:!1,matchFontHeight:!1},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]},startup:{pageReady:()=>MathJax.startup.defaultPageReady().then(()=>{document.querySelectorAll("mjx-container").forEach(e=>{e.style.overflow="visible"})})}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js></script><style>.footnote-popup{position:absolute;background:var(--theme);border:1px solid var(--border);border-radius:6px;padding:12px 16px;max-width:300px;font-size:.85em;line-height:1.4;z-index:1000;box-shadow:0 4px 12px rgba(0,0,0,.15);font-family:var(--font-mono);color:var(--primary);display:none;pointer-events:auto;word-wrap:break-word}.dark .footnote-popup{background:#2d2d2d;box-shadow:0 4px 12px rgba(0,0,0,.3)}.footnote-popup::before{content:'';position:absolute;top:-6px;left:50%;transform:translateX(-50%);width:12px;height:12px;background:var(--theme);border:1px solid var(--border);border-bottom:none;border-right:none;rotate:45deg}.dark .footnote-popup::before{background:#2d2d2d}.footnote-ref{text-decoration:none!important;font-weight:700;padding:2px 6px;border-radius:4px;background:var(--primary);color:var(--theme)!important;transition:all .2s ease;position:relative;border:1px solid var(--border);font-size:.8em;line-height:1.2;display:inline-block;min-width:18px;text-align:center;margin:0 1px;vertical-align:baseline}.footnote-ref:hover{background:var(--secondary);color:var(--theme)!important;box-shadow:0 2px 4px rgba(0,0,0,.2)}.dark .footnote-ref{background:#fff;color:#000!important;border:1px solid #666}.dark .footnote-ref:hover{background:#e5e5e5;color:#000!important;box-shadow:0 2px 6px rgba(0,0,0,.4)}</style><script>document.addEventListener("DOMContentLoaded",function(){let e=null,t=null;function s(e,t){const n=document.createElement("div");return n.className="footnote-popup",n.innerHTML=t,document.body.appendChild(n),n}function o(n,s){t&&(clearTimeout(t),t=null);const i=n.getBoundingClientRect(),r=s.getBoundingClientRect();let o=i.left+i.width/2-s.offsetWidth/2,a=i.top-s.offsetHeight-10;o<10&&(o=10),o+s.offsetWidth>window.innerWidth-10&&(o=window.innerWidth-s.offsetWidth-10),a<10&&(a=i.bottom+10),s.style.left=o+window.scrollX+"px",s.style.top=a+window.scrollY+"px",s.style.display="block",e=s}function n(){t=setTimeout(()=>{e&&(e.style.display="none",e=null)},150)}document.querySelectorAll("a.footnote-ref").forEach(e=>{const i=e.getAttribute("href");if(!i)return;const r=document.querySelector(i.replace(/:/g,"\\:"));if(!r)return;const c=r.innerHTML.replace(/<a[^>]*href="#fnref[^"]*"[^>]*>.*?<\/a>/g,"").trim();if(!c)return;const a=s(i,c);e.addEventListener("mouseenter",()=>{o(e,a)}),e.addEventListener("mouseleave",n),a.addEventListener("mouseenter",()=>{t&&(clearTimeout(t),t=null)}),a.addEventListener("mouseleave",n)}),window.addEventListener("scroll",()=>{e&&(e.style.display="none",e=null)})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>