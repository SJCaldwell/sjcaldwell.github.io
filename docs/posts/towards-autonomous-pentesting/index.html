<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.52">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Shane Caldwell">
<meta name="dcterms.date" content="2020-04-28">

<title>Deep Reinforcement Learning for Security: Toward an Autonomous Pentesting Agent – Shane Caldwell</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Shane Caldwell</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../talks.html"> 
<span class="menu-text">Papers and Talks</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/SJCaldwell"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/shncldwll"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Deep Reinforcement Learning for Security: Toward an Autonomous Pentesting Agent</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">AI</div>
                <div class="quarto-category">security</div>
                <div class="quarto-category">RL</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Shane Caldwell </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 28, 2020</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#the-state-of-ml-in-defense" id="toc-the-state-of-ml-in-defense" class="nav-link" data-scroll-target="#the-state-of-ml-in-defense">The state of ML in Defense</a></li>
  <li><a href="#the-state-of-ml-in-offense" id="toc-the-state-of-ml-in-offense" class="nav-link" data-scroll-target="#the-state-of-ml-in-offense">The state of ML in Offense</a></li>
  <li><a href="#reinforcement-learning" id="toc-reinforcement-learning" class="nav-link" data-scroll-target="#reinforcement-learning">Reinforcement Learning</a>
  <ul class="collapse">
  <li><a href="#multi-armed-bandits" id="toc-multi-armed-bandits" class="nav-link" data-scroll-target="#multi-armed-bandits">Multi-Armed Bandits</a></li>
  </ul></li>
  <li><a href="#the-successes-and-caveats-of-deep-reinforcement-learning" id="toc-the-successes-and-caveats-of-deep-reinforcement-learning" class="nav-link" data-scroll-target="#the-successes-and-caveats-of-deep-reinforcement-learning">The Successes (and Caveats) of Deep Reinforcement Learning</a>
  <ul class="collapse">
  <li><a href="#successes" id="toc-successes" class="nav-link" data-scroll-target="#successes">Successes</a></li>
  <li><a href="#caveats" id="toc-caveats" class="nav-link" data-scroll-target="#caveats">Caveats</a></li>
  </ul></li>
  <li><a href="#the-environment-challenge" id="toc-the-environment-challenge" class="nav-link" data-scroll-target="#the-environment-challenge">The Environment Challenge</a>
  <ul class="collapse">
  <li><a href="#state-space" id="toc-state-space" class="nav-link" data-scroll-target="#state-space">State Space</a></li>
  <li><a href="#action-space" id="toc-action-space" class="nav-link" data-scroll-target="#action-space">Action Space</a></li>
  <li><a href="#metasploit-api-the-game-engine-of-pentesting" id="toc-metasploit-api-the-game-engine-of-pentesting" class="nav-link" data-scroll-target="#metasploit-api-the-game-engine-of-pentesting">Metasploit API: The ‘game engine’ of pentesting</a></li>
  </ul></li>
  <li><a href="#related-safety-problems" id="toc-related-safety-problems" class="nav-link" data-scroll-target="#related-safety-problems">Related Safety Problems</a></li>
  <li><a href="#simulation-as-a-path-forward" id="toc-simulation-as-a-path-forward" class="nav-link" data-scroll-target="#simulation-as-a-path-forward">Simulation as a path forward</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>I’ve found myself very interested in reinforcement learning recently. As you do deep learning work, you can sometimes feel limited in the problems you can solve by the paradigms you have available. To paraphrase <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Andrej Karpathy</a>, the APIs to deep learning can seem constraining, despite their power. We start with a fixed size input and fixed size output for problems like classification routinely solved by CNNs. To deal with text, we have RNNs and the more intricate LSTM models that can deal intelligently with long sequences with a kind of memory. There’s an incredible array of kinds of problems that can formulated to be solved by those approaches. We’ve seen generated artwork with <a href="https://www.artbreeder.com/">GANs</a>, object detectors used for medical diagnostics, and CNNs applied to <a href="https://medium.com/x8-the-ai-community/audio-classification-using-cnn-coding-example-f9cbd272269e">sound classification</a>. It will be a long time before we’re out of runway applying these techniques with novel variations to different fields with a lot of success. There are careers to be made for clever folks to use domain knowledge in a subject to reformulate their problem into one of these “solved problems”.</p>
<p>When I started studying machine learning, I actually had a specific domain in mind I wanted to apply it to. I’d been a penetration tester for almost two years and recently earned my <a href="https://sjcaldwell.github.io/2020/04/26/oscp-review.html">OSCP</a> when I was offered a position in a Masters in Data Science program. Pentesting was super fun, but I found myself daydreaming on the problem of whether it was possible to develop intelligent tools to aid in penetration testing. What would a tool like that be like? Specifically, I wanted to know whether it was possible to create an autonomous pentesting agent, like the kind of sentient hacking AI that make up the endlessly readable <a href="https://en.wikipedia.org/wiki/Neuromancer">William Gibson</a> novels.</p>
<p>It was also partially born out of a desire to make a useful tool in a competitive field. There are really wonderful tools out there for the would-be attacker. For web application pentesting, <a href="https://portswigger.net/burp">Burp Suite</a> is an incredibly comprehensive exploitation tool. It’s a proxy that sits inbetween your HTTP requests coming from your client browser heading to the server, allowing you to freely edit the content going to the server. Through this, all sorts of interesting attacks are possible. Using the tool is easy, as well! After browsing the site normally for awhile, it logs all the routes you can send requests to, and all the types of requests you’ve sent and recieved while interacting with the tool. From there, you can run a scan. The scan can reliably find everything from cross-site scripting to SQL injection mostly with the power of regular expressions and a handy list of strings that are usually used to exploit these sorts of attacks.</p>
<p>From the network side of things, Metasploit is even more compelling. It’s a tool and framework all in one. From within the metasploit tool you can keep track of almost everything you need to run a penetration test successfully. You can run scans, store information about target hosts, customize and launch exploits, and select payloads all from within that tool. Even more incredible - it’s open source! Once a proof of concept for an exploit has been discovered, there’s an easy to use API that allows you to write a little Ruby and produce your own exploit that you can share with others.</p>
<p>Those tools are remarkably solid and being produced by a community of talented security professionals. Better yet, they’re frameworks that allow a developer to add new functionality for anything they find lacking and share it with the world. Still, I couldn’t help but think it should be possible to perform the work automatically. I don’t mean ‘script recurring tasks’ automatic, I mean ‘set it, perform pentest, let me know how to patch the holes you found’ automatically. That’s not to say I want the work to go away. The most exciting aspects of the work are this rare 15% of it that requires an insane amount of creativity and knowledge. You can read writeups from folks who have found seemingly invisible bugs that you would think don’t have any impact at all, and used them to completely compromise applications and plunder their databases. If you don’t believe me, the popularization of bug bounties have made it incredibly easy to see what kind of hacks are out there in the wild. Bug bounties allow hackers to make money for security bugs found within their applications or networks, and many organizations running the programs allow for writeups to be published after the fact. <a href="https://pentester.land/list-of-bug-bounty-writeups.html">It’s humbling to read them</a>.</p>
<p>That other 85% or so can be a bit of a slog, though. There are several well known security issues that crop up time and time again. Finding them is always exciting in the way that all hacking is - you broke a thing that’s not supposed to break! You have access to stuff you’re not supposed to have! But it’s not challenging or engaging, really. Is it possible to build tools that make all of security the fun part? And of course, the holy grail - is it possible to make an agent even better at penetration testing than humans?</p>
<p>But before we plot the future, let’s see where we stand. How is ML being applied to security today?</p>
</section>
<section id="the-state-of-ml-in-defense" class="level2">
<h2 class="anchored" data-anchor-id="the-state-of-ml-in-defense">The state of ML in Defense</h2>
<p>Most machine learning naturally lends itself to defense, more than attack. There’s actually been a pretty good amount of defensive tooling developed. And why not? The paradigms fit like a glove. As a defender your biggest problem is probably that you have too much information. Networks are just <em>happening</em> all the time, generating all sorts of traffic on all sorts of services. You’re a human being with two eyes and a limited amount of caffeine to throw at the problem of perceiving incredibly granular logs. If you knew something bad was happening, you’re probably educated enough to take an action, but how can you know? Frequently some scripted logic and a regular expression list can alert you of some well described dangers - imagine your database administrator logged in from an IP belonging to a country they don’t live in and then changed their password - but not all dangerous situations are that well-described. What about stuff that’s just <em>weird</em>?</p>
<p>These fall under the general bucket of <em>anomoly detection</em> as a problem. First, you gather a lot of data and group it into some sort of <em>observation</em> at a fidelity a model can interpret. Then, you run the observation through the model and get a boolean output. Either it’s bad, and you alert a person, or it’s good, and nothing happens. Think about it as a “weird/not weird” classifier. The intuition behind the perceptual task is stored within the dataset, and the algorithm transforms it into something that’s augmenting a human’s capabilities by taking cognitive load off of them.</p>
<p>If you’re looking for something with a similar principle but more automated, all sorts of “smart firewalls” can be made this way. You learn what looks normal, train a network to recognize normal, and then if you’re not normal you’re an anomoly. The upside is big - if you detect an attack, you can take an action. The downside of a false alarm can be bad depending on the tooling, but as long as you’re not overwhelemed with anomalies to look at a false positive is fine. At least in theory whatever you’re looking at should be <em>anomalous</em> and therefore <em>interesting</em>.</p>
<p>In practice, this is challenging to pull off. What’s normal for a network is a living, breathing thing. New people come in, they leave. New servers come on site. If configured poorly, all of these things can be anomolous. Training a network in a custom way is also challenging - you want to learn a good distribution of <em>normal</em> but for that to be legitimate you would need to know within a shadow of a doubt that your network is currently not compromised as you’re training. Obviously, you have no idea whether that’s the case or not and there’s really no way to prove otherwise. So you have this sort of ontological problem for these types of detectors that’s challenging to solve, at least at the network level.</p>
<p><a href="https://www.cylance.com/en-us/index.html">Cylance</a> claims to do this on the endpoint level, using AI to find malware processes on desktops and phones. There’s not really a clear whitepaper that breaks down how, but it sounds <a href="https://s7d2.scene7.com/is/content/cylance/prod/cylance-web/en-us/resources/knowledge-center/resource-library/white-papers/Not_All_AI_Is_Created_Equal_White_Paper.pdf">pretty cool</a>. The approach for an endpoint anamoly detector seems equally sound to others in the anomoly detection paradigm - in each you find this distribution of process behavior that’s normal or acceptable, and if you fall outside of that you can flag it and allow a user to make the call to override detection if it’s a false positive.</p>
<p>You couldn’t really call any of these tools <em>autonomous defenders</em> though. You don’t have agents on the environment watching network traffic and taking actions in response to them. You might automatically put someone on a block list, or filter bad traffic (I too have scraped websites agressively enough that I was hit with a captcha) but none of those tools are giving the Security Operations Center the day off to play golf. We don’t have ourselves an “autonomous defender”, we have a fire alarm.</p>
</section>
<section id="the-state-of-ml-in-offense" class="level2">
<h2 class="anchored" data-anchor-id="the-state-of-ml-in-offense">The state of ML in Offense</h2>
<p>The state of things over on the offensive side is actually starting to catch up to defense, at least over the last couple of years. Attackers do a lot of enumerating resources, which is its own form of data collection (though it pales in comparison to the sheer volume of the defensive side).</p>
<p>They follow a very similar paradigm as well, actually. Except now <em>anamoly</em> means something different. On the offensive side it’s “Hey bud, that’s a whole lotta attack surface to look at there. Want me to check it out and see if any tires seem worth kicking”?</p>
<p>BishopFox’s <a href="https://github.com/BishopFox/eyeballer">eyeballer</a> is actually a really cool example of one of these. Many security tools sniff HTTP endpoints of a target and screenshot them for you to review. Eyeballer goes that extra step forward and lets you apply classification to the problem. Run them through the classifier to find out if they’re login pages, or they look like old custom code, whatever. It’s a great example of taking a domain specific pentesting problem and making it fit into the classification paradigm.</p>
<p>There’s been similar work done with text. I even found a language model used to do reconaissance on a target’s twitter and then use text models customize messages with phishing links catered to them. This is a BlackHat talk from <a href="https://www.blackhat.com/docs/us-16/materials/us-16-Seymour-Tully-Weaponizing-Data-Science-For-Social-Engineering-Automated-E2E-Spear-Phishing-On-Twitter.pdf">ZeroFox</a>. As you might’ve noticed, there are a lot of foxes in security consulting. But also, this is very much in line with what I was thinking of - an automated, intelligent tool to assist with security testing.</p>
<p>For the record, I think all of the tools I’ve listed above are insanely cool and I would’ve been proud to have worked on any of them. It is not a critique that none of them seem to fit the paradigm I’m looking for: how would you go about developing an agent that could act autonomously? To be specific, the ‘hello world’ of such an agent might look as follows:</p>
<p>How could you develop a system that had never seen <a href="https://information.rapid7.com/download-metasploitable-2017.html">Metasploitable</a> or similar vulnerable-by-design single hosts that could be placed on the same network as them, automatically enumerate information about, exploit, and extract data from them? If such a system was robust enough to handle many different intentionally vulnerable systems, it would be an <em>autonomous pentesting agent</em>.</p>
</section>
<section id="reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="reinforcement-learning">Reinforcement Learning</h2>
<p>If you’re interested in AI, you’ve probably heard of reinforcement learning. Even if you haven’t heard it by that name, it’s definitely been in the news. It’s the paradigm that made <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">AlphaGo</a> possible, and is the same paradigm that’s helped OpenAI crush Atari score for game after game. It’s also made a bot that can play Smash Bros pretty <a href="https://www.youtube.com/watch?v=dXJUlqBsZtE">dang well</a>. But what is it? And how might it help us develop a system that can hack autonomously?</p>
<p>Broadly, reinforcement learning is the study of agents that learn by trial and error. Agents learn policies that direct them to take <em>actions</em> and then observe the change in <em>environments</em> and the <em>reward</em> they recieve to inform their next action.</p>
<section id="multi-armed-bandits" class="level3">
<h3 class="anchored" data-anchor-id="multi-armed-bandits">Multi-Armed Bandits</h3>
<p>The classical non-deep example, the one a reader is most likely to have come across in the past, is the multi-armed bandit. The problem is a simple one: you find yourself in a casino. You stand in front of a slot machine with three arms. You’re told that each of the arms has a different probability of success - some are luckier than others. Your goal is to find the best strategy to achieve the highest reward you can in a given number of arm pulls.</p>
<p>A naive approach might be to play with each arm many times. In fact, play each arm so many rounds you can eventually estimate the true probability of reward on the machine when the law of large numbers kicks in. Once you’ve done this for each machine, you merely need to hang out on the machine that ended up with the highest reward probability, right? Easy peezy.</p>
<p>Those of you who have gone to a casino would surely retort that this is an inefficient and expensive strategy. Fine, then: let’s introduce some definitions and try to use math to be a little more than lucky.</p>
<p>We have <span class="math inline">\(n\)</span> arms on the machine, and <span class="math inline">\(t\)</span> number of time steps to play the game. Each arm represents an action <span class="math inline">\(a\)</span> we can take. Our goal is to approximate the true success probability of each of the arms or <span class="math inline">\(q(a)\)</span> and then exploit that knowledge reward.</p>
<p>We’ve established we can’t know the true reward, so we’ll call our approximation <span class="math inline">\(Q(a)\)</span>. Because this is an approximation based on our current understanding of the environment, and we’re an intelligent agent that updates our believes based on our observations, it makes most sense to think about <span class="math inline">\(Q_t(a)\)</span>, or our estimate valued of a given action at a given time step, <span class="math inline">\(t\)</span>.</p>
<p>First, we know nothing about the environment, so we pull an arm at random. Let’s say it gives us a reward! For one pull of the arm you’ve gotten exactly one reward. What do you think about that machines odd of success now?</p>
<p>Well, it makes the most sense to basically just keep a running list of how many times we’ve tried the action, and what our total reward has been with the action. That’s our estimated probability. Something like:</p>
<p><span class="math display">\[
Q_t(a) = \frac{R_1 + R_2 + ... + R_Nt(a)}{N_t(a)}
\]</span></p>
<p>With this, we could keep a running best guess of the reward for each action.</p>
<p>But that’s a lot of information to record. For a computer program, that means the memory needed for the program scales up linearly with the amount of time steps considered. In pratice, we use something called a <em>q table</em> to keep the memory constant. I won’t go into it too much here but you’ll see it below in my python implementation. The idea is the same, which is to update <span class="math inline">\(Q_t(a)\)</span> at each timestep allowing it to become slowly more accurate.</p>
<p>So what is our strategy? A <em>greedy</em> strategy is just to read the action from the Q table that maximizes your reward:</p>
<p><span class="math display">\[
A_t = argmax Q_t(a)
\]</span></p>
<p>Remember, we already pulled a lever once and it yielded an award. So that action is the only one in the Q table with a value over 0.0. So does that just mean we select that action over and over again, without ever trying the other arms? How do we know the other actions wouldn’t give us even greater rewards?</p>
<p>This is the essence of the multi-armed bandit problem. To <em>exploit</em> our current knowledge of the environment to the best of our ability or <em>explore</em> to learn more about an action we don’t currently understand very well.</p>
<p>To do this, we introduce <span class="math inline">\(\epsilon\)</span>. Every <span class="math inline">\(\epsilon%\)</span> of the time, we will choose a random action instead of the action we know will yield us the most gain, observe our success or failure, and update our <span class="math inline">\(Q_t(a)\)</span> for that action.</p>
<p>Given a reasonable choice of <span class="math inline">\(\epsilon\)</span> and enough time steps, this allows us to converge on the best solution, even if our initial solution is not optimal.</p>
<p>We can examine this in code, as below:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Environment:</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, p):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">        p is the probability of success for each </span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">        casino arm</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.p <span class="op">=</span> p</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, action):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">        The agent pulls an arm and selects an action.</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">        The reward is stochastic - you only get anything </span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">        with the probability given in self.p for a given arm.</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co">        action - the index of the arm you choose to pull</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        result_prob <span class="op">=</span> np.random.random() <span class="co"># Samples from continous uniform distribution</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> result_prob <span class="op">&lt;</span> <span class="va">self</span>.p[action]:</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Agent:</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, actions, eps):</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co">        actions - The number of actions (arms to pull)</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co">        eps - The frequency with which the agent will explore,</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co">              rather than selecting the highest reward action</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_acts <span class="op">=</span> actions</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.actions_count <span class="op">=</span> [<span class="dv">0</span> <span class="cf">for</span> action <span class="kw">in</span> <span class="bu">range</span>(actions)]</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q <span class="op">=</span> [<span class="dv">0</span> <span class="cf">for</span> action <span class="kw">in</span> <span class="bu">range</span>(actions)]</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> act(<span class="va">self</span>):</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.random.random() <span class="op">&lt;</span> <span class="va">self</span>.eps:</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>            <span class="co">#we explore</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> np.random.randint(<span class="va">self</span>.num_acts)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>            <span class="co">#we exploit</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> np.argmax(<span class="va">self</span>.Q)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> action</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_q_table(<span class="va">self</span>, action, reward):</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.actions_count[action] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        step_size <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> <span class="va">self</span>.actions_count[action]</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q[action] <span class="op">=</span> <span class="va">self</span>.Q[action] <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> step_size) <span class="op">*</span> <span class="va">self</span>.Q[action] <span class="op">+</span> step_size <span class="op">*</span> reward</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> experiment(p, time_steps, eps):</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="co">    p is probabilities of success for arms</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="co">    time_steps - number of time steps to run experiment for</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a><span class="co">    epsilon to choose for agent</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    env <span class="op">=</span> Environment(p)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    agent <span class="op">=</span> Agent(<span class="bu">len</span>(p), eps)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> time_step <span class="kw">in</span> <span class="bu">range</span>(time_steps):</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>        action <span class="op">=</span> agent.act() <span class="co"># get action from agent</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> env.step(action) <span class="co"># take action in env</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>        agent.update_q_table(action, reward) <span class="co">#update with reward</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> agent.Q</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>q_table <span class="op">=</span> experiment([<span class="fl">0.24</span>, <span class="fl">0.33</span>, <span class="fl">0.41</span>], <span class="dv">1_000_000</span>, <span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The final q_table appears as <code>[0.2397833283177857, 0.3332216502695646, 0.41020130865076515]</code>, indicating we pretty successful in estimating <span class="math inline">\(q(a)\)</span> with <span class="math inline">\(Q_(a)\)</span>.</p>
<p>So it’s a simplistic example, but illustrates the power of reinforcement learning. Unlike a supervised learning example, we never told the system what the right answer was the third level, with <span class="math inline">\(q(a_3) = 0.41\)</span>. We enabled the agent to observe the effects of its actions to update its policy, and change it’s behavior.</p>
<p>If you want to read more about classic reinforcement learning, I highly recommend the extremely pleasent to read and extremely free <a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">Reinforcement Learning: An Introduction</a>. Hopefully this gentle introduction has convinced you there’s an interesting power here, different from supervised or unsupervised learning methods you may have known in the past.</p>
</section>
</section>
<section id="the-successes-and-caveats-of-deep-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="the-successes-and-caveats-of-deep-reinforcement-learning">The Successes (and Caveats) of Deep Reinforcement Learning</h2>
<p>Reinforcement learning allows for self-directed optimization. Deep learning allows for function approximation. By combining the two we’re able to map environment state and action pairs into expected rewards.</p>
<section id="successes" class="level3">
<h3 class="anchored" data-anchor-id="successes">Successes</h3>
<p>I won’t go too long here, because there’s already plenty of hype. AlphaZero can play Go better than anyone has ever played Go, and through self-play eventually invented novel openings that human beings are now studying. Hard to overstate how mindblowing that is. I think this was a pretty epoch defining event for anyone interested in AI in any field.</p>
</section>
<section id="caveats" class="level3">
<h3 class="anchored" data-anchor-id="caveats">Caveats</h3>
<p>Before I get into the weeds of the challenges deep reinforcement learning faces as a field, I’d be remiss to not advise anyone interested to read Alex Irpan’s <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">Deep Reinforcement Learning Doesn’t Work Yet</a>. I’ll be summarizing some of these points below, but the whole article is a sobering but ultimately optimistic read for those looking to cut their teeth on deep RL.</p>
<p>I’ll be looking at each of these as challenges to be overcome for my own research: developing an autonomous pentesting agent.</p>
<section id="sample-inefficiency" class="level4">
<h4 class="anchored" data-anchor-id="sample-inefficiency">Sample Inefficiency</h4>
<p>One of the key problems in deep RL is sample ineffiency: that is, you need a <em>whole lot</em> of data to get good performance. The ratio of environment complexity to data required for strong performance can seem frighteningly high. For many environments, particularly real life ones, you’re almost out of luck.</p>
<p>Even in my multi-armed bandit scenario, I ran 1,000,000 episodes. This was a pretty simple environment to learn from. Imagine training an agent against Metasploitable. You allow the agent to take action until the completion of the episode. Then you restart the virtual machine in a clean state, and begin again. Parallelizing this requires multiple virtual machines, and the time between episodes is as long as it takes to load up a fresh disk image - and that’s for a single host! Full environments representing entire network would be even harder to generate adequate experience for. Think about how long it takes you to spin up a fleet of boxes in Amazon, much less configure all the network policies. Brutal. For a single host, resetting metasploitable to a clean state a million times would take, optimistically, two minutes a pop. Doing that one million times? That would take about <em>4 years</em>.</p>
<p>So even if the method <em>could</em> work in principle, experience generating the data to overcome sample inefficiency is going to be tough.</p>
</section>
<section id="reward-function-design-is-tough" class="level4">
<h4 class="anchored" data-anchor-id="reward-function-design-is-tough">Reward Function Design is Tough</h4>
<p>Designing reward for Go is kinda easy. Collecting territory and winning? These things are good. Giving up territory and losing the game? This is very bad. Atari is pretty straightforward as well. Each of these games provide a score - if you make the score go up, you’re doing well! If the score goes down, or you die, you’re doing poorly.</p>
<p>Expressing those sorts of reward functions in simple environments mathematically is not extraordinarily difficult.</p>
<p>How about more subtle goals though? Take our goal of pentesting:</p>
<p>How do you define good pentesting? To do that, you’d need to ask a good pentester what their goals are on an assesment. Since I don’t have any on hand, my personal experience will have to suffice: good pentesting is about <em>careful thoroughness</em>.</p>
<p>For a real life attacker, your only goal is to find a single exploitable hole good enough to weasel your way into the network, find high-value information, and take off with it. Ideally without letting anyone know you were there. Sort of a depth-first search kinda deal.</p>
<p>Pentesting needs to be wide <em>and</em> deep. You want to present the client with evidence you looked over their network to the best of your ability, found as many chinks in their armor as possible at all levels of access you were able to achieve. And while doing this, you’re under certain constraints. You can’t break their network to discover a high value target. Some things are off limits, also known as out-of-scope. Also you have a fixed amount of time, So you can’t explore <em>everything</em>. You have to provide breadth, and use your intuition to decide where to spend time going deep that will provide the biggest bang for the client’s buck. That’s good pentesting.</p>
<p>There are two kinds of rewards we might try: <strong>sparse rewards</strong> only provide reward at the end of the episode if the policy resulted in a ‘success’. The agent “won” the game. We’re having a hard time defining success for pentesting if we use the above definition, but even if the answer was just ‘got root access on a specific machine’ that likely wouldn’t be enough. With so little to go off of, you can imagine a pentesting agent firing off some random scans, maybe trying to some random exploits against random machines, and never recieving even a drop of reward for its trouble. The policy network has no valuable information to backprop on, and you’re essentially dead stuck unless by some miracle the network chooses random actions that lead to success. As a former pentester, I can attest that I have tried that strategy and been very disappointed in it.</p>
<p>In this case, we need something more complicated. <strong>Shaped reward</strong> provides increasing rewards for states as they become closer to the end goal, rewarding actions that are useful. This sound like a better fit for our problem. For example, scanning a potential target is not getting root on a high value target, but it’s a useful step on the way, so we should give some reward there.</p>
<p>How would you express that as a reward function? Exploits are good! Discovering hosts, and information about hosts is also good. But we want to ensure we’re not just brute-forcing throwing exploits at hosts to see if they work, so maybe we impose <em>noisiness cost</em> per action to encourage strategic exploits and scanning. How do we weigh the reward of exploit vs scanning? When it comes to information exfiltration, how do we teach an agent to understand what high-value vs low-value information is? We want the agent to understand <em>high-value</em> targets that deserve more intensive study, but how do we communicate that? In fact, we don’t want to do that at all - we want the agent to discover that. Now how do you say that with math? When you try to piece these ideas into a singular reward function it gets hard quick.</p>
</section>
<section id="reward-functions-like-to-blow-up-in-your-face" class="level4">
<h4 class="anchored" data-anchor-id="reward-functions-like-to-blow-up-in-your-face">Reward Functions like to Blow Up in Your Face</h4>
<p>Agents do not care about your problems. They only care about the reward their actions can give them. Despite the elegant expressiveness of mathematics and your best personal efforts, there will probably be a gap between these intentions. In these gaps, the agent will attempt to find whatever action in the environment gives them the quick fix of reward without all the challenge of discovering a really useful reward function.</p>
<p>OpenAI provides an infamous example in one of their experiments: in a boat racing game, they used a shaped reward. The agent got the most reward for winning, but they got partial reward for picking up powerups (useful for winning!) and passing checkpoints.</p>
The agent quickly discovers you can get the most reward by just collecting the powerups, since they regenerate quickly. It finds itself stuck in a really elegant bender as its opponents whiz by. The agent will never win the race this way, and still get an incredible amount of reward. This is called <em>reward hacking</em>.
<div style="text-align:center;">
<iframe width="560" height="315" src="https://www.youtube.com/embed/tlOIHko8ySg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
</iframe>
</div>
<p>Think about our previously proposed hodge-podge of actions that would give our hypothetical agent reward. It’s easy to imagine an agent that had not yet penetrated the network finding a successful exploit that got it access to another machine. Great place to farm! The agent would likely just fire off that again and again, and each success would give it more reward. The same could be said about a scan enumerating a host, or any number of activities. Without a carefully crafted reward, our proposed shaped reward could be easily “hacked”, with plenty of reward gained and our task un-done.</p>
</section>
</section>
</section>
<section id="the-environment-challenge" class="level2">
<h2 class="anchored" data-anchor-id="the-environment-challenge">The Environment Challenge</h2>
<section id="state-space" class="level3">
<h3 class="anchored" data-anchor-id="state-space">State Space</h3>
<p>Another thing deep reinforcement learning requires is an environment. For a game like chess or shogi, this is just the board. It’s pretty easy to gracefully represent as a matrix.</p>
<p>Defining a board for pentesting is kind of hard. You kind of start with a <em>fog of war</em> situation where you know about the perimiter of a network early on, but you really don’t know the full size of the environment in terms of number of hosts until you find one. So it’s an environment that starts small and gets bigger overtime, with each new host found having different properties.</p>
<p>Most game environments are pretty fixed, so that’s tough. It could be seen as a blessing, though. You’re encouraged to overfit like crazy in reinforcement learning when generating experience in the game, often these learned skills don’t transfer to a new environment. For penetration testing each “game” starts on a new network, or a new sized “board”. There’s a general pattern of penetration testing that should stay consistent, but the shape of the network and hosts on it will define what your optimal actions are. Hopefully that keeps overfitting to a minimum.</p>
</section>
<section id="action-space" class="level3">
<h3 class="anchored" data-anchor-id="action-space">Action Space</h3>
<p>Your action space, the actions available to an agent that can be taken, also need to be provided. Chess, for example, this might be the legal moves your agent can take for any input board state.</p>
<p>There are <em>continous</em> and <em>discrete</em> action spaces. Discrete action spaces basically just means a countable number of actions. The chess example applies here. Continous action spaces might be found when you’re using RL to set the specific value of a sensor, for example. Where the value of the sensor can take on any real-numbered value between a lower and upper bound. To be honest, I haven’t totally wrapped my head around methods for continous action spaces but I have seen a lot of clever problem formulation to make the action space <em>discrete</em> instead.</p>
<p>For example, take that sensor problem - pretty continous. But what if we assume there’s a minimum amount you can tune the sensor up or down that’s meaningful? Call it <span class="math inline">\(x\)</span>. Now, after taking an observation from our environment, let’s say we only have two options - up or down by <span class="math inline">\(x\)</span>. Well gollee gee, sir, up or down? I ain’t no mathematician but that’s a pretty discrete space if I do say so myself.</p>
<p>This sort of judo is on display whenever the problem allows for it. When OpenAI tackled Dota 2, they easily could have considered the action space continous - but they didn’t. <a href="https://cdn.openai.com/dota-2.pdf">They discretized the action space on a per-hero basis</a>, arriving at a model choosing among 8,000 to 80,000 discrete actions depending on their hero. A discrete action space will be pried from their cold, dead hands.</p>
<p>That’s a lot of moves. OpenAI had access to the game engine’s API, so these actions were probably <em>read</em> rather than hand-coded. For our pentesting problem, how do we handle that? You’re sitting in front of a terminal, where you can enter any text. A very miniscule part of the distribution of all text you can type into a terminal is going to be valuable for accessing your hacking tools. Within those tools, there’s very specific syntax that will be valuable. That’s a pretty big action space, and I’m not sure we can specify reward that will make that valuable, even shaped. So what’s the play?</p>
</section>
<section id="metasploit-api-the-game-engine-of-pentesting" class="level3">
<h3 class="anchored" data-anchor-id="metasploit-api-the-game-engine-of-pentesting">Metasploit API: The ‘game engine’ of pentesting</h3>
<p>I puzzled over this for a long-time before I did some literature review and found Jonathon Schwartz’s thesis <a href="https://jjschwartz.github.io/files/2018_CS_honours_thesis.pdf">Autonomous Penetration Testing using Reinforcement Learning</a>. In it, he creates a pretty convincing partially observable Markov decision process to form a model of penetration testing. It’s one of the few real attempts I’ve seen to tackle the formulation of the problem. One line inparticular really inspired me to take a serious look at the problem again. While justifying some simplifications to his network model, Jonathon says:</p>
<blockquote class="blockquote">
<p>The specific details of performing each action, for example which port to communicate with, are details that can be handled by application specific implementations when moving towards higher fidelity systems. Penetration testing is already moving in this direction with frameworks such as metasploit which abstract away exactly how an exploit is performed and simply provide a way to find if the exploit is applicable and launch it, taking care of all the lower level details of the exploit</p>
</blockquote>
<p>First, this struck me as an oversimplification. How many times had I loaded up an exploit in metasploit only to have it not work? Then I had to dig into the specifics of the Ruby code and twiddle with things. Many exploits also have a pretty large number of required arguments to set that require some domain/target specific knowledge. Then I decided this was totally genius. That insanely large action space of the open terminal now starts to more resemble a game board. Metasploit stores information about hosts it knows about, their open services and distribution information. Exploits apply to specific distributions and services. Metasploit even provides tools for information gathering once you’ve compromised your host. It’s not always enough - often you need to break out of their laundry list of commands and use an honest-to-god terminal. But there’s a <em>lot</em> you can do restricting the action space to the Metasploit level. I haven’t done the back of the envelope math, but that feels like Dota 2 size action space to me, maybe smaller.</p>
<p>The actions you can take with Metasploit, and the information it chooses to store reduces the complications in considering both the <em>action space</em> and the <em>state space</em> of penetration testing.</p>
</section>
</section>
<section id="related-safety-problems" class="level2">
<h2 class="anchored" data-anchor-id="related-safety-problems">Related Safety Problems</h2>
<p>Solving penetration would also involve, as a sub-problem, solving a variety of safety problems. Not safety as in “paper clip AGI destroys humanity” but several of the problems described in OpenAI’s <a href="https://arxiv.org/pdf/1606.06565.pdf">Concrete Problems in AI Safety</a>. It’s essentially a review of practical research problems that can be broadly categorized around AI safety. Some of them are practically necessary to solve before you can design an agent that could be truly an autonomous attacker or defender.</p>
<p>In the paper <em>safe exploration</em> is broadly defined as ensuring the “exploration” side of exploitation vs exploration is sensitive to how it explores such that it doesn’t take extremely risky ‘exploratory’ actions. This part of being subtle in penetration testing engagements. If you explore loudly (a super fast, full network scan) you’ll probably get lots of information quickly, but you’re also likely to to set off the SOC’s alarms and are liable to get your IP blocked. Every pentester has experienced the walk of shame, having tripped the alarms before the engagement has scarecly begun and writing a tepid email about how it would be oh-so kind of the SOC to unblock an IP range, yes, thank you, sorry.</p>
<p><em>Avoiding negative side effects</em> is defined as not disturbing the environment in negative ways while pursuing its goals. This is absolutely huge on an engagement. More than once I’ve been in a situation where I’m fairly certain I have an exploit that could gain me access to the server. Trouble is, while this exploit will give me a shell, it will also crash the service. Not only will this definitely trip some alarms, but if the service isn’t set to auto-start you could cause an outage. Disturbing the day in day out work of the client during a penetration test is a sure fire way to make sure you never work with them again. The question of whether we can teach an agent this sort of discretion without manually specifying all the things that it shouldn’t disturb is a challenging question with no immediate answers.</p>
<p>Both of these are generally problems that exist in more complex environments than the ones RL has succeeded in so far, and would certainly need to be solved before any of these agents with any influence over the real-world environment could be released in the wild.</p>
</section>
<section id="simulation-as-a-path-forward" class="level2">
<h2 class="anchored" data-anchor-id="simulation-as-a-path-forward">Simulation as a path forward</h2>
<p>If you’ve read this far, you might be under the impression I have a pretty negative view of the odds of solving penetration testing with RL. Nothing could be further from the truth! I’m just being honest about the many, potentially very thorny, sub-problems on the way to that problem.</p>
<p>To me, the immediate work to be done is in the simulation space. One has to choose a subset of Metasploit actions directly from their api and map them to actions an agent can take.</p>
<p>There’s still the problem of sample inefficiency - how do you generate enough experience?</p>
<p>The answer has to be simulation. Instead of interacting with a full virtual machine environment, you need a simulated environment that makes it easy for an agent to quickly test a policy against an environment. The way the network is composed needs to be, to my mind, similar to a rogue-like game. We want procedurally generated volunerable networks at a just realistic enough fidelity for policies learned to apply to a real network. These could be spun up and down quickly and easily parallelized to achieve the kind of massive experience generation achieved by OpenAI with Dota 2.</p>
<p>The aforementioned Jonathon Schwartz has already developed a simulator that I believe steps in that direction, and extending it would certainly make a good environment for the metasploit driven-agent I’m picturing.</p>
<p>For now, I need to consider the design of the subset of metasploit actions that would make an acceptable action space for solving non-trivial vulnerable networks. Achieving an acceptable fidelity for the simulation is also key - but to me it’s just the minimum viable environment that allows the metasploit action APIs to be meaningful.</p>
<p>In a future post, I’ll take my first steps using the OpenAI Gym framework to develop a simple environment I can train one of their prewritten models on. Whatever the final shape of the simulator, I believe making sure it fits within the OpenAI gym framework popularized by researchers at the forefront of RL is the best way to get new eyes onto the project. It’s also a good way for me to get some experience with DRL tooling.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>